--- metrics_old.py	2020-03-06 19:38:55.434809370 -0300
+++ metrics.py	2020-03-10 22:57:01.476823358 -0300
@@ -4,6 +4,7 @@
 from sklearn.metrics import matthews_corrcoef
 from sklearn.metrics import accuracy_score, f1_score
 from sklearn.metrics import roc_auc_score
+from sklearn.metrics import mean_squared_error
 from scipy.stats import pearsonr, spearmanr
 from seqeval.metrics import classification_report
 from data_utils.squad_eval import evaluate_func
@@ -14,6 +15,11 @@
 def compute_f1(predicts, labels):
     return 100.0 * f1_score(labels, predicts)
 
+#https://github.com/erickrf/assin/blob/master/assin-eval.py
+def compute_f1macro(predicts, labels):
+    label_set = set(labels)
+    return 100.0 * f1_score(labels, predicts, average='macro', labels=list(label_set))
+
 def compute_mcc(predicts, labels):
     return 100.0 * matthews_corrcoef(labels, predicts)
 
@@ -52,6 +58,9 @@
 def compute_emf1(predicts, labels):
     return evaluate_func(labels, predicts)
 
+def compute_mse(predicts, labels):
+    mse = mean_squared_error(labels, predicts)
+    return mse
 
 class Metric(Enum):
     ACC = 0
@@ -62,6 +71,8 @@
     AUC = 5
     SeqEval = 7
     EmF1 = 8
+    F1macro = 9
+    MSE = 10
 
 
 
@@ -73,7 +84,9 @@
  Metric.Spearman: compute_spearman,
  Metric.AUC: compute_auc,
  Metric.SeqEval: compute_seqacc,
- Metric.EmF1: compute_emf1
+ Metric.EmF1: compute_emf1,
+ Metric.F1macro: compute_f1macro,
+ Metric.MSE: compute_mse,
 }
 
 
@@ -85,7 +98,7 @@
     for mm in metric_meta:
         metric_name = mm.name
         metric_func = METRIC_FUNC[mm]
-        if mm in (Metric.ACC, Metric.F1, Metric.MCC):
+        if mm in (Metric.ACC, Metric.F1, Metric.MCC, Metric.F1macro, Metric.MSE):
             metric = metric_func(predictions, golds)
         elif mm == Metric.SeqEval:
             metric = metric_func(predictions, golds, label_mapper)
