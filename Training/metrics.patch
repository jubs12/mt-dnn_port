--- metrics.py	2020-03-28 13:34:38.496343274 -0300
+++ metrics_new.py	2020-03-28 18:29:04.480755359 -0300
@@ -2,7 +2,7 @@
 from enum import Enum
 
 from sklearn.metrics import matthews_corrcoef
-from sklearn.metrics import accuracy_score, f1_score
+from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
 from sklearn.metrics import roc_auc_score
 from scipy.stats import pearsonr, spearmanr
 from seqeval.metrics import classification_report
@@ -58,6 +58,11 @@
 def compute_emf1(predicts, labels):
     return evaluate_func(labels, predicts)
 
+def compute_mse(predicts, labels)
+    return mean_squared_error(labels, predicts)
+
+def compute_f1nomean(predicts, labels):
+    return f1_score(labels, predicts, average=None).tolist()
 
 class Metric(Enum):
     ACC = 0
@@ -70,7 +75,8 @@
     EmF1 = 8
     F1MAC = 9
     F1MIC = 10
-
+    MSE = 11
+    NoMeanF1 = 12
 
 
 METRIC_FUNC = {
@@ -84,6 +90,8 @@
     Metric.EmF1: compute_emf1,
     Metric.F1MAC: compute_f1mac,
     Metric.F1MIC: compute_f1mic,
+    Metric.MSE: compute_mse,
+    Metric.NoMeanF1: compute_f1nomean,
 }
 
 
@@ -95,7 +103,7 @@
     for mm in metric_meta:
         metric_name = mm.name
         metric_func = METRIC_FUNC[mm]
-        if mm in (Metric.ACC, Metric.F1, Metric.MCC, Metric.F1MAC, Metric.F1MIC):
+        if mm in (Metric.ACC, Metric.F1, Metric.MCC, Metric.F1MAC, Metric.F1MIC, Metric.MSE, Metric.NoMeanF1):
             metric = metric_func(predictions, golds)
         elif mm == Metric.SeqEval:
             metric = metric_func(predictions, golds, label_mapper)
