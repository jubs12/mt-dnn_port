Preparing train arguments
I0511 02:08:59.352468 140132815427328 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
I0511 02:09:00.268606 140132815427328 filelock.py:274] Lock 140132486755384 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:09:00.269269 140132815427328 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3zp7czda
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/232k [00:00<00:27, 8.46kB/s]Downloading:  15%|█▌        | 34.8k/232k [00:00<00:16, 11.9kB/s]Downloading:  38%|███▊      | 87.0k/232k [00:00<00:08, 16.8kB/s]Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 23.9kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 471kB/s] 
I0511 02:09:01.276336 140132815427328 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:09:01.276597 140132815427328 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:09:01.277514 140132815427328 filelock.py:318] Lock 140132486755384 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:09:01.277673 140132815427328 tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
05/11/2020 02:09:01 Task assin-ptbr-rte
05/11/2020 02:09:01 ../data/input/en/bert_base_uncased_lower/assin-ptbr-rte_train.json
05/11/2020 02:09:04 ../data/input/en/bert_base_uncased_lower/assin-ptbr-rte_dev.json
05/11/2020 02:09:05 ../data/input/en/bert_base_uncased_lower/assin-ptbr-rte_test.json
I0511 02:09:09.494557 140262169229056 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, bert_model_type='bert-base-uncased', cuda=True, data_dir='../data/input/en/bert_base_uncased_lower', data_sort_on=False, do_lower_case=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, embedding_opt=0, encode_mode=False, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=True, fp16_opt_level='O2', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='bert-base-uncased', init_ratio=1, learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, masked_lm_prob=0.15, max_answer_len=5, max_predictions_per_seq=128, max_seq_len=512, mem_cum_type='simple', mix_opt=0, mkd_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', num_hidden_layers=-1, optimizer='adamax', output_dir='../output/st-dnn/assin-ptbr-rte/bert_base/seed/2018', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, short_seq_prob=0.2, task_def='../data/task-def/assin-ptbr-rte.yaml', tensorboard=True, tensorboard_logdir='tensorboard_logdir', test_datasets=['assin-ptbr-rte'], train_datasets=['assin-ptbr-rte'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)
05/11/2020 02:09:10 0
05/11/2020 02:09:10 Launching the MT-DNN training
05/11/2020 02:09:10 Loading ../data/input/en/bert_base_uncased_lower/assin-ptbr-rte_train.json as task 0
Loaded 2500 samples out of 2500
Loaded 500 samples out of 500
Loaded 2000 samples out of 2000
05/11/2020 02:09:10 ####################
05/11/2020 02:09:10 {'log_file': 'mt-dnn-train.log', 'tensorboard': True, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': '../data/input/en/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../data/task-def/assin-ptbr-rte.yaml', 'train_datasets': ['assin-ptbr-rte'], 'test_datasets': ['assin-ptbr-rte'], 'glue_format_on': False, 'mkd_opt': 0, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 5, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 0, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': '../output/st-dnn/assin-ptbr-rte/bert_base/seed/2018', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': True, 'fp16_opt_level': 'O2', 'encode_mode': False, 'task_def_list': [{'kd_loss': '<LossCriterion.MseCriterion: 1>', 'loss': '<LossCriterion.CeCriterion: 0>', 'dropout_p': 'None', 'enable_san': 'True', 'split_names': "['train', 'dev', 'test']", 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'task_type': '<TaskType.Classification: 1>', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'n_class': '3', 'label_vocab': '<data_utils.vocab.Vocabulary object at 0x7f9114463d68>', 'self': '{}', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
05/11/2020 02:09:10 ####################
05/11/2020 02:09:10 ############# Gradient Accumulation Info #############
05/11/2020 02:09:10 number of step: 1565
05/11/2020 02:09:10 number of grad grad_accumulation step: 1
05/11/2020 02:09:10 adjusted number of step: 1565
05/11/2020 02:09:10 ############# Gradient Accumulation Info #############
I0511 02:09:11.217912 140262169229056 filelock.py:274] Lock 140261875776480 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:09:11.218606 140262169229056 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpp4kwupb2
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 343kB/s]
I0511 02:09:11.759888 140262169229056 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:09:11.760105 140262169229056 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:09:11.760878 140262169229056 filelock.py:318] Lock 140261875776480 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:09:11.761120 140262169229056 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:09:11.761719 140262169229056 configuration_utils.py:321] Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

I0511 02:09:11.763461 140262169229056 configuration_utils.py:321] Model config BertConfig {
  "adam_eps": 1e-06,
  "answer_att_hidden_size": 128,
  "answer_att_type": "bilinear",
  "answer_dropout_p": 0.1,
  "answer_mem_drop_p": 0.1,
  "answer_mem_type": 1,
  "answer_merge_opt": 1,
  "answer_num_turn": 5,
  "answer_opt": 0,
  "answer_rnn_type": "gru",
  "answer_sum_att_type": "bilinear",
  "answer_weight_norm_on": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "batch_size": 8,
  "batch_size_eval": 8,
  "bert_dropout_p": 0.1,
  "bert_l2norm": 0.0,
  "bert_model_type": "bert-base-uncased",
  "cuda": true,
  "data_dir": "../data/input/en/bert_base_uncased_lower",
  "data_sort_on": false,
  "do_lower_case": false,
  "dropout_p": 0.1,
  "dropout_w": 0.0,
  "dump_state_on": false,
  "embedding_opt": 0,
  "encode_mode": false,
  "encoder_type": 1,
  "epochs": 5,
  "fp16": true,
  "fp16_opt_level": "O2",
  "freeze_layers": -1,
  "global_grad_clipping": 1.0,
  "glue_format_on": false,
  "grad_accumulation_step": 1,
  "grad_clipping": 0,
  "have_lr_scheduler": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "init_checkpoint": "bert-base-uncased",
  "init_ratio": 1,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 5e-05,
  "log_file": "mt-dnn-train.log",
  "log_per_updates": 500,
  "lr_gamma": 0.5,
  "masked_lm_prob": 0.15,
  "max_answer_len": 5,
  "max_position_embeddings": 512,
  "max_predictions_per_seq": 128,
  "max_seq_len": 512,
  "mem_cum_type": "simple",
  "mix_opt": 0,
  "mkd_opt": 0,
  "model_ckpt": "checkpoints/model_0.pt",
  "model_type": "bert",
  "momentum": 0,
  "mtl_opt": 0,
  "multi_gpu_on": false,
  "multi_step_lr": "10,20,30",
  "name": "farmer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "optimizer": "adamax",
  "output_dir": "../output/st-dnn/assin-ptbr-rte/bert_base/seed/2018",
  "pad_token_id": 0,
  "ratio": 0,
  "resume": false,
  "save_per_updates": 10000,
  "save_per_updates_on": false,
  "scheduler_type": "ms",
  "seed": 2018,
  "short_seq_prob": 0.2,
  "task_def": "../data/task-def/assin-ptbr-rte.yaml",
  "task_def_list": [
    {
      "__class__": "<class 'experiments.exp_def.TaskDef'>",
      "data_type": "<DataFormat.PremiseAndOneHypothesis: 2>",
      "dropout_p": "None",
      "enable_san": "True",
      "kd_loss": "<LossCriterion.MseCriterion: 1>",
      "label_vocab": "<data_utils.vocab.Vocabulary object at 0x7f9114463d68>",
      "loss": "<LossCriterion.CeCriterion: 0>",
      "metric_meta": "(<Metric.F1MAC: 9>, <Metric.ACC: 0>)",
      "n_class": "3",
      "self": "{}",
      "split_names": "['train', 'dev', 'test']",
      "task_type": "<TaskType.Classification: 1>"
    }
  ],
  "tensorboard": true,
  "tensorboard_logdir": "tensorboard_logdir",
  "test_datasets": [
    "assin-ptbr-rte"
  ],
  "train_datasets": [
    "assin-ptbr-rte"
  ],
  "type_vocab_size": 2,
  "update_bert_opt": 0,
  "vb_dropout": true,
  "vocab_size": 30522,
  "warmup": 0.1,
  "warmup_schedule": "warmup_linear",
  "weight_decay": 0
}

I0511 02:09:16.200465 140262169229056 filelock.py:274] Lock 140260961395432 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:09:16.201307 140262169229056 file_utils.py:444] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpigl2cvhn
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 33.8k/440M [00:00<26:23, 278kB/s]Downloading:   0%|          | 115k/440M [00:00<21:42, 338kB/s] Downloading:   0%|          | 262k/440M [00:00<16:44, 438kB/s]Downloading:   0%|          | 557k/440M [00:00<12:28, 587kB/s]Downloading:   0%|          | 1.16M/440M [00:00<09:05, 805kB/s]Downloading:   1%|          | 2.36M/440M [00:00<06:31, 1.12MB/s]Downloading:   1%|          | 4.73M/440M [00:00<04:38, 1.57MB/s]Downloading:   2%|▏         | 7.23M/440M [00:00<03:18, 2.18MB/s]Downloading:   2%|▏         | 9.75M/440M [00:00<02:23, 3.00MB/s]Downloading:   3%|▎         | 11.6M/440M [00:01<01:53, 3.77MB/s]Downloading:   3%|▎         | 13.4M/440M [00:01<01:26, 4.94MB/s]Downloading:   4%|▎         | 15.5M/440M [00:01<01:06, 6.43MB/s]Downloading:   4%|▍         | 17.9M/440M [00:01<00:51, 8.24MB/s]Downloading:   5%|▍         | 19.9M/440M [00:01<00:42, 9.82MB/s]Downloading:   5%|▍         | 21.9M/440M [00:01<00:36, 11.6MB/s]Downloading:   5%|▌         | 24.0M/440M [00:01<00:31, 13.3MB/s]Downloading:   6%|▌         | 26.1M/440M [00:01<00:27, 14.9MB/s]Downloading:   6%|▋         | 28.1M/440M [00:01<00:26, 15.5MB/s]Downloading:   7%|▋         | 30.3M/440M [00:02<00:24, 16.8MB/s]Downloading:   7%|▋         | 32.2M/440M [00:02<00:26, 15.6MB/s]Downloading:   8%|▊         | 34.0M/440M [00:02<00:31, 13.0MB/s]Downloading:   8%|▊         | 37.1M/440M [00:02<00:25, 15.8MB/s]Downloading:   9%|▉         | 39.1M/440M [00:02<00:38, 10.4MB/s]Downloading:   9%|▉         | 41.6M/440M [00:02<00:31, 12.7MB/s]Downloading:  10%|█         | 44.2M/440M [00:03<00:26, 14.9MB/s]Downloading:  10%|█         | 46.2M/440M [00:03<00:24, 16.1MB/s]Downloading:  11%|█         | 49.1M/440M [00:03<00:21, 18.5MB/s]Downloading:  12%|█▏        | 51.9M/440M [00:03<00:18, 20.7MB/s]Downloading:  12%|█▏        | 54.4M/440M [00:03<00:24, 15.6MB/s]Downloading:  13%|█▎        | 57.9M/440M [00:03<00:20, 18.7MB/s]Downloading:  14%|█▍        | 60.6M/440M [00:03<00:18, 20.6MB/s]Downloading:  14%|█▍        | 63.6M/440M [00:03<00:16, 22.7MB/s]Downloading:  15%|█▌        | 66.6M/440M [00:04<00:15, 24.5MB/s]Downloading:  16%|█▌        | 69.6M/440M [00:04<00:14, 26.0MB/s]Downloading:  16%|█▋        | 72.5M/440M [00:04<00:20, 18.4MB/s]Downloading:  17%|█▋        | 75.8M/440M [00:04<00:17, 21.3MB/s]Downloading:  18%|█▊        | 78.5M/440M [00:04<00:16, 22.3MB/s]Downloading:  18%|█▊        | 81.1M/440M [00:04<00:16, 22.2MB/s]Downloading:  19%|█▉        | 83.5M/440M [00:04<00:16, 21.4MB/s]Downloading:  20%|█▉        | 86.0M/440M [00:04<00:15, 22.2MB/s]Downloading:  20%|██        | 88.3M/440M [00:05<00:22, 15.8MB/s]Downloading:  21%|██        | 91.7M/440M [00:05<00:18, 18.8MB/s]Downloading:  21%|██▏       | 94.2M/440M [00:05<00:17, 20.3MB/s]Downloading:  22%|██▏       | 97.3M/440M [00:05<00:15, 22.5MB/s]Downloading:  23%|██▎       | 99.9M/440M [00:05<00:14, 23.3MB/s]Downloading:  23%|██▎       | 102M/440M [00:05<00:14, 23.5MB/s] Downloading:  24%|██▍       | 105M/440M [00:05<00:21, 15.8MB/s]Downloading:  25%|██▍       | 109M/440M [00:06<00:17, 19.1MB/s]Downloading:  25%|██▌       | 111M/440M [00:06<00:16, 20.2MB/s]Downloading:  26%|██▌       | 114M/440M [00:06<00:17, 19.1MB/s]Downloading:  26%|██▋       | 116M/440M [00:06<00:16, 19.5MB/s]Downloading:  27%|██▋       | 118M/440M [00:06<00:17, 17.9MB/s]Downloading:  27%|██▋       | 120M/440M [00:06<00:17, 18.3MB/s]Downloading:  28%|██▊       | 122M/440M [00:06<00:23, 13.8MB/s]Downloading:  29%|██▊       | 126M/440M [00:07<00:18, 16.9MB/s]Downloading:  29%|██▉       | 128M/440M [00:07<00:20, 15.0MB/s]Downloading:  30%|██▉       | 131M/440M [00:07<00:17, 17.6MB/s]Downloading:  30%|███       | 133M/440M [00:07<00:16, 18.8MB/s]Downloading:  31%|███       | 136M/440M [00:07<00:14, 20.9MB/s]Downloading:  31%|███▏      | 138M/440M [00:07<00:13, 21.7MB/s]Downloading:  32%|███▏      | 141M/440M [00:07<00:19, 15.5MB/s]Downloading:  33%|███▎      | 144M/440M [00:07<00:16, 17.6MB/s]Downloading:  33%|███▎      | 146M/440M [00:08<00:17, 17.2MB/s]Downloading:  34%|███▎      | 148M/440M [00:08<00:15, 18.3MB/s]Downloading:  34%|███▍      | 150M/440M [00:08<00:22, 13.0MB/s]Downloading:  35%|███▍      | 152M/440M [00:08<00:19, 14.5MB/s]Downloading:  35%|███▍      | 154M/440M [00:08<00:20, 14.1MB/s]Downloading:  35%|███▌      | 156M/440M [00:08<00:25, 11.3MB/s]Downloading:  36%|███▌      | 159M/440M [00:09<00:19, 14.2MB/s]Downloading:  37%|███▋      | 162M/440M [00:09<00:17, 16.3MB/s]Downloading:  37%|███▋      | 164M/440M [00:09<00:16, 17.1MB/s]Downloading:  38%|███▊      | 166M/440M [00:09<00:15, 17.7MB/s]Downloading:  38%|███▊      | 168M/440M [00:09<00:14, 18.2MB/s]Downloading:  39%|███▊      | 170M/440M [00:09<00:14, 18.0MB/s]Downloading:  39%|███▉      | 172M/440M [00:09<00:20, 13.0MB/s]Downloading:  40%|███▉      | 176M/440M [00:09<00:16, 16.0MB/s]Downloading:  40%|████      | 178M/440M [00:10<00:14, 17.5MB/s]Downloading:  41%|████      | 180M/440M [00:10<00:14, 17.8MB/s]Downloading:  41%|████▏     | 182M/440M [00:10<00:13, 19.0MB/s]Downloading:  42%|████▏     | 184M/440M [00:10<00:13, 19.6MB/s]Downloading:  42%|████▏     | 187M/440M [00:10<00:12, 20.7MB/s]Downloading:  43%|████▎     | 189M/440M [00:10<00:15, 15.8MB/s]Downloading:  44%|████▎     | 192M/440M [00:10<00:13, 17.9MB/s]Downloading:  44%|████▍     | 195M/440M [00:10<00:12, 20.1MB/s]Downloading:  45%|████▍     | 197M/440M [00:10<00:11, 20.6MB/s]Downloading:  45%|████▌     | 199M/440M [00:11<00:11, 20.3MB/s]Downloading:  46%|████▌     | 202M/440M [00:11<00:17, 13.4MB/s]Downloading:  46%|████▌     | 203M/440M [00:11<00:22, 10.4MB/s]Downloading:  46%|████▋     | 205M/440M [00:12<00:35, 6.67MB/s]Downloading:  47%|████▋     | 208M/440M [00:12<00:26, 8.65MB/s]Downloading:  48%|████▊     | 210M/440M [00:12<00:21, 10.9MB/s]Downloading:  48%|████▊     | 213M/440M [00:12<00:17, 12.8MB/s]Downloading:  49%|████▊     | 215M/440M [00:12<00:17, 13.0MB/s]Downloading:  49%|████▉     | 216M/440M [00:12<00:15, 14.3MB/s]Downloading:  50%|████▉     | 219M/440M [00:12<00:13, 16.0MB/s]Downloading:  50%|█████     | 221M/440M [00:12<00:13, 16.4MB/s]Downloading:  51%|█████     | 222M/440M [00:12<00:16, 13.1MB/s]Downloading:  51%|█████     | 226M/440M [00:13<00:13, 15.8MB/s]Downloading:  52%|█████▏    | 229M/440M [00:13<00:11, 18.5MB/s]Downloading:  52%|█████▏    | 231M/440M [00:13<00:10, 19.3MB/s]Downloading:  53%|█████▎    | 233M/440M [00:13<00:10, 19.6MB/s]Downloading:  53%|█████▎    | 235M/440M [00:13<00:10, 20.2MB/s]Downloading:  54%|█████▍    | 238M/440M [00:13<00:09, 21.3MB/s]Downloading:  55%|█████▍    | 240M/440M [00:13<00:09, 21.3MB/s]Downloading:  55%|█████▌    | 242M/440M [00:13<00:12, 15.4MB/s]Downloading:  56%|█████▌    | 245M/440M [00:14<00:10, 18.2MB/s]Downloading:  56%|█████▋    | 249M/440M [00:14<00:09, 20.8MB/s]Downloading:  57%|█████▋    | 251M/440M [00:14<00:08, 21.5MB/s]Downloading:  58%|█████▊    | 254M/440M [00:14<00:08, 21.9MB/s]Downloading:  58%|█████▊    | 256M/440M [00:14<00:08, 20.8MB/s]Downloading:  59%|█████▊    | 258M/440M [00:14<00:09, 20.0MB/s]Downloading:  59%|█████▉    | 260M/440M [00:14<00:12, 14.5MB/s]Downloading:  60%|█████▉    | 264M/440M [00:14<00:10, 17.2MB/s]Downloading:  60%|██████    | 266M/440M [00:15<00:09, 19.2MB/s]Downloading:  61%|██████    | 269M/440M [00:15<00:08, 19.7MB/s]Downloading:  61%|██████▏   | 271M/440M [00:15<00:09, 17.0MB/s]Downloading:  62%|██████▏   | 273M/440M [00:15<00:10, 15.6MB/s]Downloading:  62%|██████▏   | 275M/440M [00:15<00:14, 11.1MB/s]Downloading:  63%|██████▎   | 277M/440M [00:15<00:12, 13.3MB/s]Downloading:  63%|██████▎   | 279M/440M [00:15<00:10, 14.7MB/s]Downloading:  64%|██████▍   | 281M/440M [00:16<00:10, 15.9MB/s]Downloading:  64%|██████▍   | 283M/440M [00:16<00:09, 16.8MB/s]Downloading:  65%|██████▍   | 285M/440M [00:16<00:08, 18.0MB/s]Downloading:  65%|██████▌   | 287M/440M [00:16<00:08, 17.7MB/s]Downloading:  66%|██████▌   | 289M/440M [00:16<00:11, 12.7MB/s]Downloading:  66%|██████▋   | 293M/440M [00:16<00:09, 15.7MB/s]Downloading:  67%|██████▋   | 295M/440M [00:16<00:08, 17.1MB/s]Downloading:  68%|██████▊   | 297M/440M [00:16<00:07, 18.9MB/s]Downloading:  68%|██████▊   | 300M/440M [00:17<00:07, 19.7MB/s]Downloading:  69%|██████▊   | 302M/440M [00:17<00:06, 20.4MB/s]Downloading:  69%|██████▉   | 304M/440M [00:17<00:06, 20.1MB/s]Downloading:  70%|██████▉   | 306M/440M [00:17<00:09, 14.9MB/s]Downloading:  70%|███████   | 310M/440M [00:17<00:07, 17.8MB/s]Downloading:  71%|███████   | 312M/440M [00:17<00:06, 19.9MB/s]Downloading:  71%|███████▏  | 315M/440M [00:17<00:05, 21.0MB/s]Downloading:  72%|███████▏  | 317M/440M [00:17<00:05, 21.0MB/s]Downloading:  73%|███████▎  | 320M/440M [00:18<00:05, 20.8MB/s]Downloading:  73%|███████▎  | 322M/440M [00:18<00:05, 21.5MB/s]Downloading:  74%|███████▎  | 324M/440M [00:18<00:05, 22.0MB/s]Downloading:  74%|███████▍  | 327M/440M [00:18<00:07, 15.9MB/s]Downloading:  75%|███████▍  | 330M/440M [00:18<00:06, 18.5MB/s]Downloading:  75%|███████▌  | 332M/440M [00:18<00:05, 20.8MB/s]Downloading:  76%|███████▌  | 335M/440M [00:18<00:04, 21.7MB/s]Downloading:  77%|███████▋  | 337M/440M [00:18<00:05, 18.1MB/s]Downloading:  77%|███████▋  | 340M/440M [00:19<00:06, 16.7MB/s]Downloading:  78%|███████▊  | 341M/440M [00:19<00:06, 14.9MB/s]Downloading:  78%|███████▊  | 343M/440M [00:19<00:08, 11.9MB/s]Downloading:  79%|███████▊  | 346M/440M [00:19<00:06, 14.4MB/s]Downloading:  79%|███████▉  | 348M/440M [00:19<00:05, 15.9MB/s]Downloading:  79%|███████▉  | 350M/440M [00:19<00:06, 14.3MB/s]Downloading:  80%|████████  | 352M/440M [00:19<00:05, 15.7MB/s]Downloading:  80%|████████  | 354M/440M [00:20<00:05, 15.2MB/s]Downloading:  81%|████████  | 356M/440M [00:20<00:07, 12.0MB/s]Downloading:  81%|████████▏ | 359M/440M [00:20<00:05, 14.6MB/s]Downloading:  82%|████████▏ | 361M/440M [00:20<00:04, 15.9MB/s]Downloading:  82%|████████▏ | 363M/440M [00:20<00:04, 17.6MB/s]Downloading:  83%|████████▎ | 365M/440M [00:20<00:04, 18.4MB/s]Downloading:  83%|████████▎ | 368M/440M [00:20<00:03, 19.5MB/s]Downloading:  84%|████████▍ | 370M/440M [00:20<00:03, 18.7MB/s]Downloading:  84%|████████▍ | 372M/440M [00:21<00:03, 17.5MB/s]Downloading:  85%|████████▍ | 374M/440M [00:21<00:05, 12.9MB/s]Downloading:  86%|████████▌ | 377M/440M [00:21<00:03, 15.9MB/s]Downloading:  86%|████████▌ | 379M/440M [00:21<00:03, 15.8MB/s]Downloading:  87%|████████▋ | 381M/440M [00:21<00:03, 16.4MB/s]Downloading:  87%|████████▋ | 383M/440M [00:21<00:03, 17.0MB/s]Downloading:  87%|████████▋ | 385M/440M [00:21<00:03, 17.4MB/s]Downloading:  88%|████████▊ | 387M/440M [00:21<00:03, 17.7MB/s]Downloading:  88%|████████▊ | 389M/440M [00:22<00:03, 17.1MB/s]Downloading:  89%|████████▊ | 391M/440M [00:22<00:02, 18.1MB/s]Downloading:  89%|████████▉ | 393M/440M [00:22<00:03, 13.5MB/s]Downloading:  90%|████████▉ | 396M/440M [00:22<00:02, 16.4MB/s]Downloading:  90%|█████████ | 398M/440M [00:22<00:02, 18.2MB/s]Downloading:  91%|█████████ | 401M/440M [00:22<00:03, 12.5MB/s]Downloading:  91%|█████████▏| 402M/440M [00:23<00:04, 9.15MB/s]Downloading:  92%|█████████▏| 404M/440M [00:23<00:04, 7.46MB/s]Downloading:  92%|█████████▏| 406M/440M [00:23<00:03, 9.06MB/s]Downloading:  92%|█████████▏| 407M/440M [00:23<00:03, 9.98MB/s]Downloading:  93%|█████████▎| 408M/440M [00:23<00:03, 10.4MB/s]Downloading:  93%|█████████▎| 410M/440M [00:23<00:03, 9.34MB/s]Downloading:  93%|█████████▎| 411M/440M [00:24<00:03, 8.83MB/s]Downloading:  93%|█████████▎| 412M/440M [00:24<00:03, 9.10MB/s]Downloading:  94%|█████████▍| 413M/440M [00:24<00:02, 10.4MB/s]Downloading:  94%|█████████▍| 415M/440M [00:24<00:03, 8.10MB/s]Downloading:  95%|█████████▍| 418M/440M [00:24<00:02, 10.5MB/s]Downloading:  95%|█████████▌| 420M/440M [00:24<00:01, 11.1MB/s]Downloading:  96%|█████████▌| 421M/440M [00:24<00:01, 12.0MB/s]Downloading:  96%|█████████▌| 423M/440M [00:24<00:01, 13.0MB/s]Downloading:  96%|█████████▋| 424M/440M [00:25<00:01, 12.0MB/s]Downloading:  97%|█████████▋| 426M/440M [00:25<00:01, 13.3MB/s]Downloading:  97%|█████████▋| 428M/440M [00:25<00:00, 13.3MB/s]Downloading:  97%|█████████▋| 429M/440M [00:25<00:00, 14.1MB/s]Downloading:  98%|█████████▊| 431M/440M [00:25<00:00, 11.2MB/s]Downloading:  99%|█████████▊| 434M/440M [00:25<00:00, 14.0MB/s]Downloading:  99%|█████████▉| 436M/440M [00:25<00:00, 13.8MB/s]Downloading:  99%|█████████▉| 438M/440M [00:26<00:00, 6.89MB/s]Downloading: 100%|█████████▉| 440M/440M [00:26<00:00, 8.79MB/s]Downloading: 100%|██████████| 440M/440M [00:26<00:00, 16.6MB/s]
I0511 02:09:43.160417 140262169229056 file_utils.py:448] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:09:43.161072 140262169229056 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:09:43.161593 140262169229056 filelock.py:318] Lock 140260961395432 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:09:43.161830 140262169229056 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
05/11/2020 02:09:50 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=3, bias=True)
  )
)

05/11/2020 02:09:50 Total number of params: 109484547
05/11/2020 02:09:50 At epoch 0
05/11/2020 02:09:50 Task [ 0] updates[     1] train loss[1.53781] remaining[0:04:10]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
predicting 0
05/11/2020 02:10:58 Task assin-ptbr-rte -- epoch 0 -- Dev F1MAC: 50.686
05/11/2020 02:10:58 Task assin-ptbr-rte -- epoch 0 -- Dev ACC: 83.000
predicting 0
predicting 100
predicting 200
05/11/2020 02:11:04 [new test scores saved.]
/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
I0511 02:11:05.813842 140262169229056 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-rte/bert_base/seed/2018/model_0.pt
05/11/2020 02:11:05 At epoch 1
05/11/2020 02:11:44 Task [ 0] updates[   500] train loss[0.55324] remaining[0:00:25]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
predicting 0
05/11/2020 02:12:12 Task assin-ptbr-rte -- epoch 1 -- Dev F1MAC: 53.708
05/11/2020 02:12:12 Task assin-ptbr-rte -- epoch 1 -- Dev ACC: 86.200
predicting 0
predicting 100
predicting 200
05/11/2020 02:12:18 [new test scores saved.]
I0511 02:12:19.481080 140262169229056 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-rte/bert_base/seed/2018/model_1.pt
05/11/2020 02:12:19 At epoch 2
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
predicting 0
05/11/2020 02:13:27 Task assin-ptbr-rte -- epoch 2 -- Dev F1MAC: 51.061
05/11/2020 02:13:27 Task assin-ptbr-rte -- epoch 2 -- Dev ACC: 84.800
predicting 0
predicting 100
predicting 200
05/11/2020 02:13:33 [new test scores saved.]
I0511 02:13:35.043463 140262169229056 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-rte/bert_base/seed/2018/model_2.pt
05/11/2020 02:13:35 At epoch 3
05/11/2020 02:13:48 Task [ 0] updates[  1000] train loss[0.39800] remaining[0:00:53]
predicting 0
05/11/2020 02:14:42 Task assin-ptbr-rte -- epoch 3 -- Dev F1MAC: 52.423
05/11/2020 02:14:42 Task assin-ptbr-rte -- epoch 3 -- Dev ACC: 85.400
predicting 0
predicting 100
predicting 200
05/11/2020 02:14:49 [new test scores saved.]
I0511 02:14:50.562079 140262169229056 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-rte/bert_base/seed/2018/model_3.pt
05/11/2020 02:14:50 At epoch 4
05/11/2020 02:15:41 Task [ 0] updates[  1500] train loss[0.32764] remaining[0:00:13]
predicting 0
05/11/2020 02:15:56 Task assin-ptbr-rte -- epoch 4 -- Dev F1MAC: 54.006
05/11/2020 02:15:56 Task assin-ptbr-rte -- epoch 4 -- Dev ACC: 86.200
predicting 0
predicting 100
predicting 200
05/11/2020 02:16:02 [new test scores saved.]
I0511 02:16:04.289270 140262169229056 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-rte/bert_base/seed/2018/model_4.pt
Preparing train arguments
I0511 02:16:07.369014 139904091748096 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
I0511 02:16:08.265751 139904091748096 filelock.py:274] Lock 139903763600440 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:16:08.266728 139904091748096 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpjzy7ynp8
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/232k [00:00<00:27, 8.51kB/s]Downloading:  15%|█▌        | 34.8k/232k [00:00<00:16, 12.0kB/s]Downloading:  38%|███▊      | 87.0k/232k [00:00<00:08, 16.9kB/s]Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 24.0kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 473kB/s] 
I0511 02:16:09.321008 139904091748096 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:16:09.321232 139904091748096 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:16:09.322121 139904091748096 filelock.py:318] Lock 139903763600440 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:16:09.322268 139904091748096 tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
05/11/2020 02:16:09 Task assin-ptpt-rte
05/11/2020 02:16:09 ../data/input/en/bert_base_uncased_lower/assin-ptpt-rte_train.json
05/11/2020 02:16:13 ../data/input/en/bert_base_uncased_lower/assin-ptpt-rte_dev.json
05/11/2020 02:16:14 ../data/input/en/bert_base_uncased_lower/assin-ptpt-rte_test.json
I0511 02:16:18.132153 140260015290112 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, bert_model_type='bert-base-uncased', cuda=True, data_dir='../data/input/en/bert_base_uncased_lower', data_sort_on=False, do_lower_case=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, embedding_opt=0, encode_mode=False, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=True, fp16_opt_level='O2', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='bert-base-uncased', init_ratio=1, learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, masked_lm_prob=0.15, max_answer_len=5, max_predictions_per_seq=128, max_seq_len=512, mem_cum_type='simple', mix_opt=0, mkd_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', num_hidden_layers=-1, optimizer='adamax', output_dir='../output/st-dnn/assin-ptpt-rte/bert_base/seed/2018', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, short_seq_prob=0.2, task_def='../data/task-def/assin-ptpt-rte.yaml', tensorboard=True, tensorboard_logdir='tensorboard_logdir', test_datasets=['assin-ptpt-rte'], train_datasets=['assin-ptpt-rte'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)
05/11/2020 02:16:19 0
05/11/2020 02:16:19 Launching the MT-DNN training
05/11/2020 02:16:19 Loading ../data/input/en/bert_base_uncased_lower/assin-ptpt-rte_train.json as task 0
Loaded 2500 samples out of 2500
Loaded 500 samples out of 500
Loaded 2000 samples out of 2000
05/11/2020 02:16:19 ####################
05/11/2020 02:16:19 {'log_file': 'mt-dnn-train.log', 'tensorboard': True, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': '../data/input/en/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../data/task-def/assin-ptpt-rte.yaml', 'train_datasets': ['assin-ptpt-rte'], 'test_datasets': ['assin-ptpt-rte'], 'glue_format_on': False, 'mkd_opt': 0, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 5, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 0, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': '../output/st-dnn/assin-ptpt-rte/bert_base/seed/2018', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': True, 'fp16_opt_level': 'O2', 'encode_mode': False, 'task_def_list': [{'kd_loss': '<LossCriterion.MseCriterion: 1>', 'loss': '<LossCriterion.CeCriterion: 0>', 'dropout_p': 'None', 'enable_san': 'True', 'split_names': "['train', 'dev', 'test']", 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'task_type': '<TaskType.Classification: 1>', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'n_class': '3', 'label_vocab': '<data_utils.vocab.Vocabulary object at 0x7f909af0fe10>', 'self': '{}', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
05/11/2020 02:16:19 ####################
05/11/2020 02:16:19 ############# Gradient Accumulation Info #############
05/11/2020 02:16:19 number of step: 1565
05/11/2020 02:16:19 number of grad grad_accumulation step: 1
05/11/2020 02:16:19 adjusted number of step: 1565
05/11/2020 02:16:19 ############# Gradient Accumulation Info #############
I0511 02:16:19.803955 140260015290112 filelock.py:274] Lock 140259055294168 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:16:19.804648 140260015290112 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmplwevwnsc
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 258kB/s]
I0511 02:16:20.339537 140260015290112 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:16:20.339792 140260015290112 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:16:20.340887 140260015290112 filelock.py:318] Lock 140259055294168 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:16:20.341181 140260015290112 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:16:20.341840 140260015290112 configuration_utils.py:321] Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

I0511 02:16:20.343983 140260015290112 configuration_utils.py:321] Model config BertConfig {
  "adam_eps": 1e-06,
  "answer_att_hidden_size": 128,
  "answer_att_type": "bilinear",
  "answer_dropout_p": 0.1,
  "answer_mem_drop_p": 0.1,
  "answer_mem_type": 1,
  "answer_merge_opt": 1,
  "answer_num_turn": 5,
  "answer_opt": 0,
  "answer_rnn_type": "gru",
  "answer_sum_att_type": "bilinear",
  "answer_weight_norm_on": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "batch_size": 8,
  "batch_size_eval": 8,
  "bert_dropout_p": 0.1,
  "bert_l2norm": 0.0,
  "bert_model_type": "bert-base-uncased",
  "cuda": true,
  "data_dir": "../data/input/en/bert_base_uncased_lower",
  "data_sort_on": false,
  "do_lower_case": false,
  "dropout_p": 0.1,
  "dropout_w": 0.0,
  "dump_state_on": false,
  "embedding_opt": 0,
  "encode_mode": false,
  "encoder_type": 1,
  "epochs": 5,
  "fp16": true,
  "fp16_opt_level": "O2",
  "freeze_layers": -1,
  "global_grad_clipping": 1.0,
  "glue_format_on": false,
  "grad_accumulation_step": 1,
  "grad_clipping": 0,
  "have_lr_scheduler": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "init_checkpoint": "bert-base-uncased",
  "init_ratio": 1,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 5e-05,
  "log_file": "mt-dnn-train.log",
  "log_per_updates": 500,
  "lr_gamma": 0.5,
  "masked_lm_prob": 0.15,
  "max_answer_len": 5,
  "max_position_embeddings": 512,
  "max_predictions_per_seq": 128,
  "max_seq_len": 512,
  "mem_cum_type": "simple",
  "mix_opt": 0,
  "mkd_opt": 0,
  "model_ckpt": "checkpoints/model_0.pt",
  "model_type": "bert",
  "momentum": 0,
  "mtl_opt": 0,
  "multi_gpu_on": false,
  "multi_step_lr": "10,20,30",
  "name": "farmer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "optimizer": "adamax",
  "output_dir": "../output/st-dnn/assin-ptpt-rte/bert_base/seed/2018",
  "pad_token_id": 0,
  "ratio": 0,
  "resume": false,
  "save_per_updates": 10000,
  "save_per_updates_on": false,
  "scheduler_type": "ms",
  "seed": 2018,
  "short_seq_prob": 0.2,
  "task_def": "../data/task-def/assin-ptpt-rte.yaml",
  "task_def_list": [
    {
      "__class__": "<class 'experiments.exp_def.TaskDef'>",
      "data_type": "<DataFormat.PremiseAndOneHypothesis: 2>",
      "dropout_p": "None",
      "enable_san": "True",
      "kd_loss": "<LossCriterion.MseCriterion: 1>",
      "label_vocab": "<data_utils.vocab.Vocabulary object at 0x7f909af0fe10>",
      "loss": "<LossCriterion.CeCriterion: 0>",
      "metric_meta": "(<Metric.F1MAC: 9>, <Metric.ACC: 0>)",
      "n_class": "3",
      "self": "{}",
      "split_names": "['train', 'dev', 'test']",
      "task_type": "<TaskType.Classification: 1>"
    }
  ],
  "tensorboard": true,
  "tensorboard_logdir": "tensorboard_logdir",
  "test_datasets": [
    "assin-ptpt-rte"
  ],
  "train_datasets": [
    "assin-ptpt-rte"
  ],
  "type_vocab_size": 2,
  "update_bert_opt": 0,
  "vb_dropout": true,
  "vocab_size": 30522,
  "warmup": 0.1,
  "warmup_schedule": "warmup_linear",
  "weight_decay": 0
}

I0511 02:16:24.470380 140260015290112 filelock.py:274] Lock 140258840781656 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:16:24.471208 140260015290112 file_utils.py:444] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpjoc_2sjk
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 33.8k/440M [00:00<26:17, 279kB/s]Downloading:   0%|          | 98.3k/440M [00:00<21:58, 334kB/s]Downloading:   0%|          | 229k/440M [00:00<17:07, 429kB/s] Downloading:   0%|          | 490k/440M [00:00<12:49, 572kB/s]Downloading:   0%|          | 983k/440M [00:00<09:24, 778kB/s]Downloading:   0%|          | 2.03M/440M [00:00<06:46, 1.08MB/s]Downloading:   1%|          | 4.13M/440M [00:00<04:49, 1.51MB/s]Downloading:   2%|▏         | 7.26M/440M [00:00<03:25, 2.11MB/s]Downloading:   2%|▏         | 10.4M/440M [00:00<02:26, 2.93MB/s]Downloading:   3%|▎         | 12.5M/440M [00:01<01:48, 3.94MB/s]Downloading:   3%|▎         | 14.9M/440M [00:01<01:20, 5.27MB/s]Downloading:   4%|▍         | 17.4M/440M [00:01<01:01, 6.90MB/s]Downloading:   4%|▍         | 19.7M/440M [00:01<00:52, 7.96MB/s]Downloading:   5%|▌         | 22.6M/440M [00:01<00:40, 10.2MB/s]Downloading:   6%|▌         | 24.9M/440M [00:01<00:34, 12.1MB/s]Downloading:   6%|▋         | 27.6M/440M [00:01<00:28, 14.4MB/s]Downloading:   7%|▋         | 29.9M/440M [00:01<00:26, 15.8MB/s]Downloading:   7%|▋         | 32.2M/440M [00:01<00:26, 15.3MB/s]Downloading:   8%|▊         | 34.2M/440M [00:02<00:26, 15.5MB/s]Downloading:   8%|▊         | 36.6M/440M [00:02<00:23, 17.4MB/s]Downloading:   9%|▉         | 38.6M/440M [00:02<00:32, 12.5MB/s]Downloading:   9%|▉         | 40.3M/440M [00:02<00:30, 13.0MB/s]Downloading:  10%|▉         | 42.0M/440M [00:02<00:29, 13.5MB/s]Downloading:  10%|█         | 44.3M/440M [00:02<00:25, 15.3MB/s]Downloading:  10%|█         | 46.1M/440M [00:02<00:28, 13.8MB/s]Downloading:  11%|█         | 47.6M/440M [00:03<00:28, 13.6MB/s]Downloading:  11%|█▏        | 49.9M/440M [00:03<00:25, 15.5MB/s]Downloading:  12%|█▏        | 51.7M/440M [00:03<00:24, 16.1MB/s]Downloading:  12%|█▏        | 53.6M/440M [00:03<00:23, 16.8MB/s]Downloading:  13%|█▎        | 55.8M/440M [00:03<00:21, 18.1MB/s]Downloading:  13%|█▎        | 57.7M/440M [00:03<00:22, 17.4MB/s]Downloading:  14%|█▎        | 60.0M/440M [00:03<00:21, 18.1MB/s]Downloading:  14%|█▍        | 62.0M/440M [00:03<00:20, 18.6MB/s]Downloading:  15%|█▍        | 63.9M/440M [00:03<00:20, 18.7MB/s]Downloading:  15%|█▍        | 65.8M/440M [00:04<00:20, 18.6MB/s]Downloading:  15%|█▌        | 67.7M/440M [00:04<00:20, 18.4MB/s]Downloading:  16%|█▌        | 69.5M/440M [00:04<00:20, 18.3MB/s]Downloading:  16%|█▌        | 71.4M/440M [00:04<00:20, 17.8MB/s]Downloading:  17%|█▋        | 73.2M/440M [00:04<00:22, 16.3MB/s]Downloading:  17%|█▋        | 75.0M/440M [00:04<00:22, 16.5MB/s]Downloading:  18%|█▊        | 77.2M/440M [00:04<00:20, 17.9MB/s]Downloading:  18%|█▊        | 79.1M/440M [00:04<00:25, 14.1MB/s]Downloading:  18%|█▊        | 80.8M/440M [00:04<00:24, 15.0MB/s]Downloading:  19%|█▊        | 82.4M/440M [00:05<00:30, 11.6MB/s]Downloading:  19%|█▉        | 83.8M/440M [00:05<00:32, 11.1MB/s]Downloading:  19%|█▉        | 85.1M/440M [00:05<00:35, 9.88MB/s]Downloading:  20%|█▉        | 87.1M/440M [00:05<00:30, 11.7MB/s]Downloading:  20%|██        | 89.1M/440M [00:05<00:26, 13.3MB/s]Downloading:  21%|██        | 91.0M/440M [00:05<00:24, 14.5MB/s]Downloading:  21%|██        | 92.7M/440M [00:05<00:27, 12.9MB/s]Downloading:  21%|██▏       | 94.5M/440M [00:06<00:25, 13.8MB/s]Downloading:  22%|██▏       | 96.0M/440M [00:06<00:24, 14.1MB/s]Downloading:  22%|██▏       | 98.4M/440M [00:06<00:21, 16.0MB/s]Downloading:  23%|██▎       | 100M/440M [00:06<00:22, 15.1MB/s] Downloading:  23%|██▎       | 102M/440M [00:06<00:28, 11.9MB/s]Downloading:  24%|██▎       | 105M/440M [00:06<00:23, 14.4MB/s]Downloading:  24%|██▍       | 106M/440M [00:06<00:24, 13.7MB/s]Downloading:  25%|██▍       | 108M/440M [00:06<00:23, 14.1MB/s]Downloading:  25%|██▌       | 110M/440M [00:07<00:21, 15.7MB/s]Downloading:  25%|██▌       | 112M/440M [00:07<00:19, 16.5MB/s]Downloading:  26%|██▌       | 114M/440M [00:07<00:18, 17.7MB/s]Downloading:  26%|██▋       | 116M/440M [00:07<00:17, 18.9MB/s]Downloading:  27%|██▋       | 119M/440M [00:07<00:15, 20.4MB/s]Downloading:  28%|██▊       | 121M/440M [00:07<00:15, 21.2MB/s]Downloading:  28%|██▊       | 124M/440M [00:07<00:15, 20.5MB/s]Downloading:  29%|██▊       | 126M/440M [00:07<00:15, 20.3MB/s]Downloading:  29%|██▉       | 128M/440M [00:07<00:14, 21.2MB/s]Downloading:  30%|██▉       | 130M/440M [00:08<00:14, 20.8MB/s]Downloading:  30%|███       | 132M/440M [00:08<00:14, 20.8MB/s]Downloading:  31%|███       | 134M/440M [00:08<00:21, 14.4MB/s]Downloading:  31%|███▏      | 138M/440M [00:08<00:17, 17.5MB/s]Downloading:  32%|███▏      | 140M/440M [00:08<00:19, 15.4MB/s]Downloading:  32%|███▏      | 143M/440M [00:08<00:16, 17.8MB/s]Downloading:  33%|███▎      | 145M/440M [00:08<00:15, 19.2MB/s]Downloading:  34%|███▎      | 148M/440M [00:08<00:14, 20.7MB/s]Downloading:  34%|███▍      | 150M/440M [00:09<00:15, 18.9MB/s]Downloading:  35%|███▍      | 152M/440M [00:09<00:17, 16.3MB/s]Downloading:  35%|███▍      | 154M/440M [00:09<00:19, 15.0MB/s]Downloading:  35%|███▌      | 156M/440M [00:09<00:17, 15.9MB/s]Downloading:  36%|███▌      | 158M/440M [00:09<00:20, 13.6MB/s]Downloading:  36%|███▌      | 159M/440M [00:09<00:21, 13.2MB/s]Downloading:  37%|███▋      | 161M/440M [00:09<00:20, 13.5MB/s]Downloading:  37%|███▋      | 162M/440M [00:10<00:19, 14.0MB/s]Downloading:  37%|███▋      | 164M/440M [00:10<00:20, 13.2MB/s]Downloading:  38%|███▊      | 165M/440M [00:10<00:21, 12.6MB/s]Downloading:  38%|███▊      | 167M/440M [00:10<00:24, 11.2MB/s]Downloading:  38%|███▊      | 168M/440M [00:10<00:30, 8.90MB/s]Downloading:  38%|███▊      | 169M/440M [00:10<00:27, 9.90MB/s]Downloading:  39%|███▊      | 170M/440M [00:10<00:26, 10.2MB/s]Downloading:  39%|███▉      | 173M/440M [00:10<00:21, 12.3MB/s]Downloading:  40%|███▉      | 175M/440M [00:11<00:18, 14.2MB/s]Downloading:  40%|████      | 176M/440M [00:11<00:21, 12.5MB/s]Downloading:  40%|████      | 178M/440M [00:11<00:24, 10.6MB/s]Downloading:  41%|████      | 179M/440M [00:11<00:29, 8.85MB/s]Downloading:  41%|████      | 180M/440M [00:11<00:29, 8.83MB/s]Downloading:  41%|████      | 181M/440M [00:11<00:32, 7.95MB/s]Downloading:  41%|████▏     | 182M/440M [00:11<00:29, 8.89MB/s]Downloading:  42%|████▏     | 184M/440M [00:12<00:25, 10.2MB/s]Downloading:  42%|████▏     | 186M/440M [00:12<00:21, 11.7MB/s]Downloading:  43%|████▎     | 187M/440M [00:12<00:20, 12.3MB/s]Downloading:  43%|████▎     | 189M/440M [00:12<00:18, 13.7MB/s]Downloading:  43%|████▎     | 191M/440M [00:12<00:17, 14.4MB/s]Downloading:  44%|████▎     | 192M/440M [00:12<00:19, 12.4MB/s]Downloading:  44%|████▍     | 194M/440M [00:12<00:20, 11.9MB/s]Downloading:  44%|████▍     | 195M/440M [00:12<00:23, 10.5MB/s]Downloading:  45%|████▍     | 196M/440M [00:13<00:27, 8.97MB/s]Downloading:  45%|████▍     | 197M/440M [00:13<00:24, 9.94MB/s]Downloading:  45%|████▌     | 199M/440M [00:13<00:23, 10.4MB/s]Downloading:  45%|████▌     | 200M/440M [00:13<00:28, 8.40MB/s]Downloading:  46%|████▌     | 201M/440M [00:13<00:35, 6.74MB/s]Downloading:  46%|████▌     | 203M/440M [00:13<00:28, 8.24MB/s]Downloading:  46%|████▌     | 204M/440M [00:13<00:28, 8.44MB/s]Downloading:  46%|████▋     | 205M/440M [00:14<00:27, 8.69MB/s]Downloading:  47%|████▋     | 206M/440M [00:14<00:24, 9.61MB/s]Downloading:  47%|████▋     | 207M/440M [00:14<00:23, 10.1MB/s]Downloading:  47%|████▋     | 209M/440M [00:14<00:19, 11.6MB/s]Downloading:  48%|████▊     | 211M/440M [00:14<00:17, 13.0MB/s]Downloading:  48%|████▊     | 212M/440M [00:14<00:17, 12.7MB/s]Downloading:  49%|████▊     | 214M/440M [00:14<00:16, 13.4MB/s]Downloading:  49%|████▉     | 216M/440M [00:14<00:14, 15.5MB/s]Downloading:  50%|████▉     | 218M/440M [00:14<00:13, 16.2MB/s]Downloading:  50%|█████     | 220M/440M [00:14<00:12, 17.1MB/s]Downloading:  50%|█████     | 222M/440M [00:15<00:27, 8.07MB/s]Downloading:  51%|█████     | 225M/440M [00:15<00:20, 10.3MB/s]Downloading:  52%|█████▏    | 227M/440M [00:15<00:17, 12.2MB/s]Downloading:  52%|█████▏    | 229M/440M [00:15<00:15, 13.6MB/s]Downloading:  52%|█████▏    | 231M/440M [00:16<00:20, 10.3MB/s]Downloading:  53%|█████▎    | 233M/440M [00:16<00:21, 9.70MB/s]Downloading:  53%|█████▎    | 234M/440M [00:16<00:23, 8.61MB/s]Downloading:  54%|█████▍    | 238M/440M [00:16<00:18, 11.1MB/s]Downloading:  54%|█████▍    | 239M/440M [00:16<00:17, 11.2MB/s]Downloading:  55%|█████▍    | 241M/440M [00:16<00:16, 11.8MB/s]Downloading:  55%|█████▌    | 243M/440M [00:17<00:22, 8.90MB/s]Downloading:  55%|█████▌    | 244M/440M [00:17<00:19, 9.98MB/s]Downloading:  56%|█████▌    | 245M/440M [00:17<00:18, 10.7MB/s]Downloading:  56%|█████▌    | 247M/440M [00:17<00:18, 10.6MB/s]Downloading:  56%|█████▋    | 248M/440M [00:17<00:18, 10.4MB/s]Downloading:  57%|█████▋    | 250M/440M [00:17<00:15, 12.1MB/s]Downloading:  57%|█████▋    | 251M/440M [00:17<00:15, 12.4MB/s]Downloading:  58%|█████▊    | 253M/440M [00:17<00:13, 14.2MB/s]Downloading:  58%|█████▊    | 255M/440M [00:18<00:15, 12.1MB/s]Downloading:  58%|█████▊    | 256M/440M [00:18<00:18, 9.87MB/s]Downloading:  58%|█████▊    | 257M/440M [00:18<00:21, 8.68MB/s]Downloading:  59%|█████▉    | 260M/440M [00:18<00:17, 10.3MB/s]Downloading:  59%|█████▉    | 261M/440M [00:18<00:22, 8.05MB/s]Downloading:  60%|█████▉    | 262M/440M [00:18<00:20, 8.80MB/s]Downloading:  60%|█████▉    | 264M/440M [00:19<00:18, 9.42MB/s]Downloading:  60%|██████    | 265M/440M [00:19<00:17, 10.0MB/s]Downloading:  61%|██████    | 266M/440M [00:19<00:15, 11.3MB/s]Downloading:  61%|██████    | 268M/440M [00:19<00:23, 7.43MB/s]Downloading:  61%|██████    | 270M/440M [00:19<00:18, 9.14MB/s]Downloading:  62%|██████▏   | 271M/440M [00:19<00:17, 9.85MB/s]Downloading:  62%|██████▏   | 272M/440M [00:19<00:16, 10.1MB/s]Downloading:  62%|██████▏   | 273M/440M [00:19<00:15, 10.5MB/s]Downloading:  62%|██████▏   | 275M/440M [00:20<00:17, 9.61MB/s]Downloading:  63%|██████▎   | 276M/440M [00:20<00:15, 10.6MB/s]Downloading:  63%|██████▎   | 278M/440M [00:20<00:15, 10.7MB/s]Downloading:  63%|██████▎   | 279M/440M [00:20<00:20, 7.87MB/s]Downloading:  64%|██████▎   | 280M/440M [00:20<00:20, 7.71MB/s]Downloading:  64%|██████▍   | 281M/440M [00:20<00:17, 8.85MB/s]Downloading:  64%|██████▍   | 282M/440M [00:20<00:20, 7.83MB/s]Downloading:  64%|██████▍   | 284M/440M [00:21<00:17, 9.19MB/s]Downloading:  65%|██████▍   | 286M/440M [00:21<00:14, 10.9MB/s]Downloading:  65%|██████▌   | 288M/440M [00:21<00:12, 12.5MB/s]Downloading:  66%|██████▌   | 289M/440M [00:21<00:11, 12.7MB/s]Downloading:  66%|██████▌   | 291M/440M [00:21<00:14, 10.0MB/s]Downloading:  66%|██████▋   | 292M/440M [00:21<00:15, 9.86MB/s]Downloading:  67%|██████▋   | 293M/440M [00:21<00:15, 9.81MB/s]Downloading:  67%|██████▋   | 295M/440M [00:22<00:13, 10.6MB/s]Downloading:  67%|██████▋   | 296M/440M [00:22<00:13, 10.6MB/s]Downloading:  67%|██████▋   | 297M/440M [00:22<00:14, 9.82MB/s]Downloading:  68%|██████▊   | 298M/440M [00:22<00:13, 10.8MB/s]Downloading:  68%|██████▊   | 300M/440M [00:22<00:18, 7.68MB/s]Downloading:  68%|██████▊   | 301M/440M [00:22<00:27, 5.03MB/s]Downloading:  68%|██████▊   | 301M/440M [00:23<00:33, 4.13MB/s]Downloading:  69%|██████▉   | 303M/440M [00:23<00:25, 5.35MB/s]Downloading:  69%|██████▉   | 304M/440M [00:23<00:26, 5.11MB/s]Downloading:  69%|██████▉   | 305M/440M [00:23<00:25, 5.25MB/s]Downloading:  70%|██████▉   | 307M/440M [00:23<00:20, 6.67MB/s]Downloading:  70%|██████▉   | 308M/440M [00:23<00:16, 7.84MB/s]Downloading:  70%|███████   | 309M/440M [00:23<00:14, 8.84MB/s]Downloading:  70%|███████   | 310M/440M [00:24<00:14, 9.15MB/s]Downloading:  71%|███████   | 312M/440M [00:24<00:18, 6.88MB/s]Downloading:  71%|███████   | 313M/440M [00:24<00:14, 8.51MB/s]Downloading:  72%|███████▏  | 315M/440M [00:24<00:12, 9.78MB/s]Downloading:  72%|███████▏  | 316M/440M [00:24<00:12, 9.61MB/s]Downloading:  72%|███████▏  | 318M/440M [00:24<00:11, 10.5MB/s]Downloading:  72%|███████▏  | 319M/440M [00:24<00:13, 8.89MB/s]Downloading:  73%|███████▎  | 320M/440M [00:25<00:14, 8.44MB/s]Downloading:  73%|███████▎  | 321M/440M [00:25<00:12, 9.45MB/s]Downloading:  73%|███████▎  | 323M/440M [00:25<00:11, 10.2MB/s]Downloading:  74%|███████▎  | 324M/440M [00:25<00:10, 11.3MB/s]Downloading:  74%|███████▍  | 325M/440M [00:25<00:11, 9.70MB/s]Downloading:  74%|███████▍  | 328M/440M [00:25<00:09, 11.7MB/s]Downloading:  75%|███████▍  | 329M/440M [00:25<00:08, 13.2MB/s]Downloading:  75%|███████▌  | 331M/440M [00:25<00:07, 13.8MB/s]Downloading:  76%|███████▌  | 333M/440M [00:26<00:08, 12.5MB/s]Downloading:  76%|███████▌  | 334M/440M [00:26<00:10, 10.2MB/s]Downloading:  76%|███████▌  | 335M/440M [00:26<00:15, 6.68MB/s]Downloading:  77%|███████▋  | 338M/440M [00:26<00:11, 8.72MB/s]Downloading:  77%|███████▋  | 340M/440M [00:27<00:17, 5.77MB/s]Downloading:  77%|███████▋  | 341M/440M [00:27<00:24, 4.00MB/s]Downloading:  78%|███████▊  | 342M/440M [00:27<00:23, 4.25MB/s]Downloading:  78%|███████▊  | 344M/440M [00:27<00:17, 5.52MB/s]Downloading:  79%|███████▊  | 346M/440M [00:28<00:13, 7.18MB/s]Downloading:  79%|███████▉  | 348M/440M [00:28<00:12, 7.65MB/s]Downloading:  79%|███████▉  | 350M/440M [00:28<00:09, 9.34MB/s]Downloading:  80%|███████▉  | 351M/440M [00:28<00:09, 9.62MB/s]Downloading:  80%|████████  | 352M/440M [00:28<00:09, 9.66MB/s]Downloading:  80%|████████  | 354M/440M [00:28<00:08, 10.5MB/s]Downloading:  81%|████████  | 356M/440M [00:28<00:06, 12.1MB/s]Downloading:  81%|████████  | 357M/440M [00:28<00:06, 12.4MB/s]Downloading:  81%|████████▏ | 359M/440M [00:29<00:06, 13.0MB/s]Downloading:  82%|████████▏ | 360M/440M [00:29<00:06, 12.1MB/s]Downloading:  82%|████████▏ | 361M/440M [00:29<00:07, 11.1MB/s]Downloading:  83%|████████▎ | 363M/440M [00:29<00:06, 12.7MB/s]Downloading:  83%|████████▎ | 365M/440M [00:29<00:06, 12.5MB/s]Downloading:  83%|████████▎ | 366M/440M [00:29<00:06, 12.1MB/s]Downloading:  84%|████████▎ | 368M/440M [00:29<00:06, 10.5MB/s]Downloading:  84%|████████▍ | 371M/440M [00:29<00:05, 13.1MB/s]Downloading:  85%|████████▍ | 373M/440M [00:30<00:05, 12.8MB/s]Downloading:  85%|████████▍ | 374M/440M [00:30<00:05, 11.8MB/s]Downloading:  85%|████████▌ | 376M/440M [00:30<00:04, 13.1MB/s]Downloading:  86%|████████▌ | 378M/440M [00:30<00:04, 12.9MB/s]Downloading:  86%|████████▌ | 379M/440M [00:30<00:07, 8.72MB/s]Downloading:  86%|████████▋ | 381M/440M [00:30<00:05, 10.1MB/s]Downloading:  87%|████████▋ | 383M/440M [00:30<00:05, 11.3MB/s]Downloading:  87%|████████▋ | 384M/440M [00:31<00:05, 11.0MB/s]Downloading:  88%|████████▊ | 385M/440M [00:31<00:08, 6.53MB/s]Downloading:  88%|████████▊ | 388M/440M [00:31<00:06, 8.52MB/s]Downloading:  89%|████████▊ | 390M/440M [00:31<00:05, 9.56MB/s]Downloading:  89%|████████▉ | 392M/440M [00:31<00:04, 11.2MB/s]Downloading:  89%|████████▉ | 394M/440M [00:31<00:03, 12.6MB/s]Downloading:  90%|████████▉ | 396M/440M [00:32<00:04, 10.0MB/s]Downloading:  90%|█████████ | 397M/440M [00:32<00:03, 11.6MB/s]Downloading:  91%|█████████ | 399M/440M [00:32<00:03, 13.5MB/s]Downloading:  91%|█████████ | 401M/440M [00:32<00:02, 13.6MB/s]Downloading:  91%|█████████▏| 403M/440M [00:32<00:02, 13.2MB/s]Downloading:  92%|█████████▏| 405M/440M [00:32<00:02, 15.3MB/s]Downloading:  92%|█████████▏| 407M/440M [00:32<00:02, 16.3MB/s]Downloading:  93%|█████████▎| 409M/440M [00:32<00:01, 17.6MB/s]Downloading:  93%|█████████▎| 411M/440M [00:33<00:01, 16.6MB/s]Downloading:  94%|█████████▍| 413M/440M [00:33<00:01, 16.0MB/s]Downloading:  94%|█████████▍| 415M/440M [00:33<00:01, 17.2MB/s]Downloading:  95%|█████████▍| 417M/440M [00:33<00:01, 17.4MB/s]Downloading:  95%|█████████▌| 419M/440M [00:33<00:01, 19.0MB/s]Downloading:  96%|█████████▌| 421M/440M [00:33<00:01, 18.8MB/s]Downloading:  96%|█████████▌| 424M/440M [00:33<00:00, 20.0MB/s]Downloading:  97%|█████████▋| 426M/440M [00:33<00:00, 19.7MB/s]Downloading:  97%|█████████▋| 428M/440M [00:33<00:00, 19.0MB/s]Downloading:  98%|█████████▊| 431M/440M [00:34<00:00, 20.6MB/s]Downloading:  98%|█████████▊| 433M/440M [00:34<00:00, 17.2MB/s]Downloading:  99%|█████████▊| 435M/440M [00:34<00:00, 17.2MB/s]Downloading:  99%|█████████▉| 436M/440M [00:34<00:00, 12.1MB/s]Downloading:  99%|█████████▉| 438M/440M [00:34<00:00, 11.5MB/s]Downloading: 100%|█████████▉| 439M/440M [00:34<00:00, 9.13MB/s]Downloading: 100%|██████████| 440M/440M [00:35<00:00, 12.6MB/s]
I0511 02:16:59.898553 140260015290112 file_utils.py:448] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:16:59.899258 140260015290112 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:16:59.899870 140260015290112 filelock.py:318] Lock 140258840781656 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:16:59.900048 140260015290112 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
05/11/2020 02:17:06 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=3, bias=True)
  )
)

05/11/2020 02:17:06 Total number of params: 109484547
05/11/2020 02:17:06 At epoch 0
05/11/2020 02:17:07 Task [ 0] updates[     1] train loss[1.26445] remaining[0:03:55]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
predicting 0
05/11/2020 02:18:05 Task assin-ptpt-rte -- epoch 0 -- Dev F1MAC: 54.290
05/11/2020 02:18:05 Task assin-ptpt-rte -- epoch 0 -- Dev ACC: 82.800
predicting 0
predicting 100
predicting 200
05/11/2020 02:18:11 [new test scores saved.]
/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
I0511 02:18:12.748780 140260015290112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-rte/bert_base/seed/2018/model_0.pt
05/11/2020 02:18:12 At epoch 1
05/11/2020 02:18:47 Task [ 0] updates[   500] train loss[0.56937] remaining[0:00:23]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
predicting 0
05/11/2020 02:19:12 Task assin-ptpt-rte -- epoch 1 -- Dev F1MAC: 55.468
05/11/2020 02:19:12 Task assin-ptpt-rte -- epoch 1 -- Dev ACC: 84.200
predicting 0
predicting 100
predicting 200
05/11/2020 02:19:19 [new test scores saved.]
I0511 02:19:20.401744 140260015290112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-rte/bert_base/seed/2018/model_1.pt
05/11/2020 02:19:20 At epoch 2
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
predicting 0
05/11/2020 02:20:26 Task assin-ptpt-rte -- epoch 2 -- Dev F1MAC: 55.048
05/11/2020 02:20:26 Task assin-ptpt-rte -- epoch 2 -- Dev ACC: 83.800
predicting 0
predicting 100
predicting 200
05/11/2020 02:20:32 [new test scores saved.]
I0511 02:20:34.132239 140260015290112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-rte/bert_base/seed/2018/model_2.pt
05/11/2020 02:20:34 At epoch 3
05/11/2020 02:20:46 Task [ 0] updates[  1000] train loss[0.42473] remaining[0:00:51]
predicting 0
05/11/2020 02:21:39 Task assin-ptpt-rte -- epoch 3 -- Dev F1MAC: 55.264
05/11/2020 02:21:39 Task assin-ptpt-rte -- epoch 3 -- Dev ACC: 84.200
predicting 0
predicting 100
predicting 200
05/11/2020 02:21:46 [new test scores saved.]
I0511 02:21:47.961232 140260015290112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-rte/bert_base/seed/2018/model_3.pt
05/11/2020 02:21:47 At epoch 4
05/11/2020 02:22:38 Task [ 0] updates[  1500] train loss[0.34704] remaining[0:00:13]
predicting 0
05/11/2020 02:22:53 Task assin-ptpt-rte -- epoch 4 -- Dev F1MAC: 55.275
05/11/2020 02:22:53 Task assin-ptpt-rte -- epoch 4 -- Dev ACC: 83.800
predicting 0
predicting 100
predicting 200
05/11/2020 02:22:59 [new test scores saved.]
I0511 02:23:00.875106 140260015290112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-rte/bert_base/seed/2018/model_4.pt
Preparing train arguments
I0511 02:23:03.863723 140022990362368 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
I0511 02:23:04.806519 140022990362368 filelock.py:274] Lock 140022655915064 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:23:04.807173 140022990362368 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmppz_8lcst
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/232k [00:00<00:27, 8.43kB/s]Downloading:  15%|█▌        | 34.8k/232k [00:00<00:16, 11.9kB/s]Downloading:  38%|███▊      | 87.0k/232k [00:00<00:08, 16.8kB/s]Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 23.8kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 467kB/s] 
I0511 02:23:05.830629 140022990362368 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:23:05.830999 140022990362368 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:23:05.832103 140022990362368 filelock.py:318] Lock 140022655915064 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:23:05.832303 140022990362368 tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
05/11/2020 02:23:05 Task assin2-rte
05/11/2020 02:23:05 ../data/input/en/bert_base_uncased_lower/assin2-rte_train.json
05/11/2020 02:23:11 ../data/input/en/bert_base_uncased_lower/assin2-rte_dev.json
05/11/2020 02:23:11 ../data/input/en/bert_base_uncased_lower/assin2-rte_test.json
I0511 02:23:15.454627 139729515079424 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, bert_model_type='bert-base-uncased', cuda=True, data_dir='../data/input/en/bert_base_uncased_lower', data_sort_on=False, do_lower_case=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, embedding_opt=0, encode_mode=False, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=True, fp16_opt_level='O2', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='bert-base-uncased', init_ratio=1, learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, masked_lm_prob=0.15, max_answer_len=5, max_predictions_per_seq=128, max_seq_len=512, mem_cum_type='simple', mix_opt=0, mkd_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', num_hidden_layers=-1, optimizer='adamax', output_dir='../output/st-dnn/assin2-rte/bert_base/seed/2018', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, short_seq_prob=0.2, task_def='../data/task-def/assin2-rte.yaml', tensorboard=True, tensorboard_logdir='tensorboard_logdir', test_datasets=['assin2-rte'], train_datasets=['assin2-rte'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)
05/11/2020 02:23:16 0
05/11/2020 02:23:16 Launching the MT-DNN training
05/11/2020 02:23:16 Loading ../data/input/en/bert_base_uncased_lower/assin2-rte_train.json as task 0
Loaded 6500 samples out of 6500
Loaded 500 samples out of 500
Loaded 2448 samples out of 2448
05/11/2020 02:23:16 ####################
05/11/2020 02:23:16 {'log_file': 'mt-dnn-train.log', 'tensorboard': True, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': '../data/input/en/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../data/task-def/assin2-rte.yaml', 'train_datasets': ['assin2-rte'], 'test_datasets': ['assin2-rte'], 'glue_format_on': False, 'mkd_opt': 0, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 5, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 0, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': '../output/st-dnn/assin2-rte/bert_base/seed/2018', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': True, 'fp16_opt_level': 'O2', 'encode_mode': False, 'task_def_list': [{'kd_loss': '<LossCriterion.MseCriterion: 1>', 'loss': '<LossCriterion.CeCriterion: 0>', 'dropout_p': 'None', 'enable_san': 'True', 'split_names': "['train', 'dev', 'test']", 'metric_meta': '(<Metric.F1MAC: 9>, <Metric.ACC: 0>)', 'task_type': '<TaskType.Classification: 1>', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'n_class': '2', 'label_vocab': '<data_utils.vocab.Vocabulary object at 0x7f1514a90e10>', 'self': '{}', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
05/11/2020 02:23:16 ####################
05/11/2020 02:23:16 ############# Gradient Accumulation Info #############
05/11/2020 02:23:16 number of step: 4065
05/11/2020 02:23:16 number of grad grad_accumulation step: 1
05/11/2020 02:23:16 adjusted number of step: 4065
05/11/2020 02:23:16 ############# Gradient Accumulation Info #############
I0511 02:23:17.141240 139729515079424 filelock.py:274] Lock 139729263074384 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:23:17.141894 139729515079424 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpiarxpclf
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 303kB/s]
I0511 02:23:17.727480 139729515079424 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:23:17.727699 139729515079424 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:23:17.728497 139729515079424 filelock.py:318] Lock 139729263074384 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:23:17.728726 139729515079424 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:23:17.729323 139729515079424 configuration_utils.py:321] Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

I0511 02:23:17.730965 139729515079424 configuration_utils.py:321] Model config BertConfig {
  "adam_eps": 1e-06,
  "answer_att_hidden_size": 128,
  "answer_att_type": "bilinear",
  "answer_dropout_p": 0.1,
  "answer_mem_drop_p": 0.1,
  "answer_mem_type": 1,
  "answer_merge_opt": 1,
  "answer_num_turn": 5,
  "answer_opt": 0,
  "answer_rnn_type": "gru",
  "answer_sum_att_type": "bilinear",
  "answer_weight_norm_on": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "batch_size": 8,
  "batch_size_eval": 8,
  "bert_dropout_p": 0.1,
  "bert_l2norm": 0.0,
  "bert_model_type": "bert-base-uncased",
  "cuda": true,
  "data_dir": "../data/input/en/bert_base_uncased_lower",
  "data_sort_on": false,
  "do_lower_case": false,
  "dropout_p": 0.1,
  "dropout_w": 0.0,
  "dump_state_on": false,
  "embedding_opt": 0,
  "encode_mode": false,
  "encoder_type": 1,
  "epochs": 5,
  "fp16": true,
  "fp16_opt_level": "O2",
  "freeze_layers": -1,
  "global_grad_clipping": 1.0,
  "glue_format_on": false,
  "grad_accumulation_step": 1,
  "grad_clipping": 0,
  "have_lr_scheduler": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "init_checkpoint": "bert-base-uncased",
  "init_ratio": 1,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 5e-05,
  "log_file": "mt-dnn-train.log",
  "log_per_updates": 500,
  "lr_gamma": 0.5,
  "masked_lm_prob": 0.15,
  "max_answer_len": 5,
  "max_position_embeddings": 512,
  "max_predictions_per_seq": 128,
  "max_seq_len": 512,
  "mem_cum_type": "simple",
  "mix_opt": 0,
  "mkd_opt": 0,
  "model_ckpt": "checkpoints/model_0.pt",
  "model_type": "bert",
  "momentum": 0,
  "mtl_opt": 0,
  "multi_gpu_on": false,
  "multi_step_lr": "10,20,30",
  "name": "farmer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "optimizer": "adamax",
  "output_dir": "../output/st-dnn/assin2-rte/bert_base/seed/2018",
  "pad_token_id": 0,
  "ratio": 0,
  "resume": false,
  "save_per_updates": 10000,
  "save_per_updates_on": false,
  "scheduler_type": "ms",
  "seed": 2018,
  "short_seq_prob": 0.2,
  "task_def": "../data/task-def/assin2-rte.yaml",
  "task_def_list": [
    {
      "__class__": "<class 'experiments.exp_def.TaskDef'>",
      "data_type": "<DataFormat.PremiseAndOneHypothesis: 2>",
      "dropout_p": "None",
      "enable_san": "True",
      "kd_loss": "<LossCriterion.MseCriterion: 1>",
      "label_vocab": "<data_utils.vocab.Vocabulary object at 0x7f1514a90e10>",
      "loss": "<LossCriterion.CeCriterion: 0>",
      "metric_meta": "(<Metric.F1MAC: 9>, <Metric.ACC: 0>)",
      "n_class": "2",
      "self": "{}",
      "split_names": "['train', 'dev', 'test']",
      "task_type": "<TaskType.Classification: 1>"
    }
  ],
  "tensorboard": true,
  "tensorboard_logdir": "tensorboard_logdir",
  "test_datasets": [
    "assin2-rte"
  ],
  "train_datasets": [
    "assin2-rte"
  ],
  "type_vocab_size": 2,
  "update_bert_opt": 0,
  "vb_dropout": true,
  "vocab_size": 30522,
  "warmup": 0.1,
  "warmup_schedule": "warmup_linear",
  "weight_decay": 0
}

I0511 02:23:22.097743 139729515079424 filelock.py:274] Lock 139728388167496 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:23:22.098430 139729515079424 file_utils.py:444] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_4g5vtpc
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 33.8k/440M [00:00<28:37, 256kB/s]Downloading:   0%|          | 98.3k/440M [00:00<23:36, 311kB/s]Downloading:   0%|          | 229k/440M [00:00<18:19, 401kB/s] Downloading:   0%|          | 487k/440M [00:00<13:40, 536kB/s]Downloading:   0%|          | 983k/440M [00:00<10:00, 732kB/s]Downloading:   0%|          | 2.02M/440M [00:00<07:11, 1.02MB/s]Downloading:   1%|          | 3.47M/440M [00:00<05:10, 1.41MB/s]Downloading:   1%|          | 4.22M/440M [00:00<03:59, 1.82MB/s]Downloading:   1%|          | 5.43M/440M [00:00<02:57, 2.44MB/s]Downloading:   1%|▏         | 6.55M/440M [00:01<02:16, 3.18MB/s]Downloading:   2%|▏         | 7.46M/440M [00:01<01:55, 3.76MB/s]Downloading:   2%|▏         | 9.03M/440M [00:01<01:28, 4.87MB/s]Downloading:   2%|▏         | 10.6M/440M [00:01<01:10, 6.12MB/s]Downloading:   3%|▎         | 12.8M/440M [00:01<00:56, 7.59MB/s]Downloading:   3%|▎         | 14.3M/440M [00:01<00:48, 8.87MB/s]Downloading:   4%|▎         | 16.2M/440M [00:01<00:40, 10.5MB/s]Downloading:   4%|▍         | 18.1M/440M [00:01<00:34, 12.1MB/s]Downloading:   5%|▍         | 20.1M/440M [00:01<00:30, 13.8MB/s]Downloading:   5%|▌         | 22.2M/440M [00:02<00:27, 15.2MB/s]Downloading:   5%|▌         | 24.0M/440M [00:02<00:26, 15.8MB/s]Downloading:   6%|▌         | 26.3M/440M [00:02<00:23, 17.5MB/s]Downloading:   6%|▋         | 28.3M/440M [00:02<00:22, 17.9MB/s]Downloading:   7%|▋         | 30.2M/440M [00:02<00:22, 18.3MB/s]Downloading:   7%|▋         | 32.7M/440M [00:02<00:20, 19.8MB/s]Downloading:   8%|▊         | 34.8M/440M [00:02<00:20, 20.1MB/s]Downloading:   8%|▊         | 36.9M/440M [00:02<00:19, 20.2MB/s]Downloading:   9%|▉         | 38.9M/440M [00:02<00:21, 18.8MB/s]Downloading:   9%|▉         | 41.3M/440M [00:02<00:19, 20.1MB/s]Downloading:  10%|▉         | 43.4M/440M [00:03<00:20, 19.2MB/s]Downloading:  10%|█         | 45.4M/440M [00:03<00:24, 16.0MB/s]Downloading:  11%|█         | 47.1M/440M [00:03<00:25, 15.3MB/s]Downloading:  11%|█         | 48.7M/440M [00:03<00:26, 14.9MB/s]Downloading:  11%|█▏        | 50.3M/440M [00:03<00:27, 14.4MB/s]Downloading:  12%|█▏        | 51.8M/440M [00:03<00:36, 10.6MB/s]Downloading:  12%|█▏        | 53.7M/440M [00:03<00:31, 12.2MB/s]Downloading:  13%|█▎        | 55.9M/440M [00:04<00:27, 14.1MB/s]Downloading:  13%|█▎        | 57.6M/440M [00:04<00:25, 14.8MB/s]Downloading:  14%|█▎        | 59.6M/440M [00:04<00:24, 15.8MB/s]Downloading:  14%|█▍        | 61.7M/440M [00:04<00:22, 17.1MB/s]Downloading:  15%|█▍        | 63.9M/440M [00:04<00:20, 18.2MB/s]Downloading:  15%|█▍        | 65.8M/440M [00:04<00:23, 15.8MB/s]Downloading:  16%|█▌        | 68.9M/440M [00:04<00:20, 18.5MB/s]Downloading:  16%|█▌        | 71.1M/440M [00:04<00:20, 17.6MB/s]Downloading:  17%|█▋        | 73.0M/440M [00:04<00:20, 17.7MB/s]Downloading:  17%|█▋        | 75.0M/440M [00:05<00:20, 18.2MB/s]Downloading:  17%|█▋        | 76.9M/440M [00:05<00:20, 17.7MB/s]Downloading:  18%|█▊        | 79.5M/440M [00:05<00:18, 19.5MB/s]Downloading:  19%|█▊        | 81.6M/440M [00:05<00:19, 18.5MB/s]Downloading:  19%|█▉        | 83.5M/440M [00:05<00:19, 18.7MB/s]Downloading:  19%|█▉        | 85.5M/440M [00:05<00:19, 18.3MB/s]Downloading:  20%|█▉        | 87.5M/440M [00:05<00:19, 18.1MB/s]Downloading:  20%|██        | 89.6M/440M [00:05<00:18, 18.9MB/s]Downloading:  21%|██        | 91.9M/440M [00:05<00:17, 19.7MB/s]Downloading:  21%|██▏       | 93.9M/440M [00:06<00:17, 19.7MB/s]Downloading:  22%|██▏       | 95.9M/440M [00:06<00:17, 19.5MB/s]Downloading:  22%|██▏       | 98.2M/440M [00:06<00:16, 20.4MB/s]Downloading:  23%|██▎       | 100M/440M [00:06<00:17, 20.0MB/s] Downloading:  23%|██▎       | 102M/440M [00:06<00:23, 14.6MB/s]Downloading:  24%|██▍       | 105M/440M [00:06<00:19, 17.0MB/s]Downloading:  24%|██▍       | 107M/440M [00:06<00:23, 14.0MB/s]Downloading:  25%|██▍       | 109M/440M [00:07<00:24, 13.4MB/s]Downloading:  25%|██▌       | 110M/440M [00:07<00:31, 10.5MB/s]Downloading:  25%|██▌       | 112M/440M [00:07<00:37, 8.77MB/s]Downloading:  26%|██▌       | 113M/440M [00:07<00:35, 9.27MB/s]Downloading:  26%|██▌       | 115M/440M [00:07<00:29, 11.2MB/s]Downloading:  26%|██▋       | 117M/440M [00:07<00:25, 12.5MB/s]Downloading:  27%|██▋       | 118M/440M [00:07<00:23, 13.4MB/s]Downloading:  27%|██▋       | 120M/440M [00:08<00:27, 11.7MB/s]Downloading:  28%|██▊       | 121M/440M [00:08<00:31, 10.1MB/s]Downloading:  28%|██▊       | 123M/440M [00:08<00:27, 11.7MB/s]Downloading:  28%|██▊       | 125M/440M [00:08<00:23, 13.2MB/s]Downloading:  29%|██▉       | 127M/440M [00:08<00:21, 14.9MB/s]Downloading:  29%|██▉       | 129M/440M [00:08<00:20, 15.3MB/s]Downloading:  30%|██▉       | 131M/440M [00:08<00:19, 16.2MB/s]Downloading:  30%|███       | 133M/440M [00:08<00:18, 17.1MB/s]Downloading:  31%|███       | 135M/440M [00:09<00:24, 12.5MB/s]Downloading:  31%|███▏      | 138M/440M [00:09<00:19, 15.5MB/s]Downloading:  32%|███▏      | 141M/440M [00:09<00:16, 18.0MB/s]Downloading:  33%|███▎      | 144M/440M [00:09<00:15, 19.6MB/s]Downloading:  33%|███▎      | 146M/440M [00:09<00:14, 20.0MB/s]Downloading:  34%|███▎      | 148M/440M [00:09<00:13, 21.2MB/s]Downloading:  34%|███▍      | 151M/440M [00:09<00:14, 20.0MB/s]Downloading:  35%|███▍      | 153M/440M [00:10<00:26, 10.8MB/s]Downloading:  35%|███▌      | 156M/440M [00:10<00:21, 13.4MB/s]Downloading:  36%|███▌      | 158M/440M [00:10<00:18, 15.4MB/s]Downloading:  37%|███▋      | 161M/440M [00:10<00:16, 17.1MB/s]Downloading:  37%|███▋      | 163M/440M [00:10<00:16, 17.2MB/s]Downloading:  38%|███▊      | 165M/440M [00:10<00:14, 18.5MB/s]Downloading:  38%|███▊      | 167M/440M [00:10<00:21, 13.0MB/s]Downloading:  39%|███▉      | 171M/440M [00:11<00:16, 16.0MB/s]Downloading:  39%|███▉      | 173M/440M [00:11<00:16, 16.1MB/s]Downloading:  40%|███▉      | 175M/440M [00:11<00:19, 13.9MB/s]Downloading:  40%|████      | 177M/440M [00:11<00:16, 15.6MB/s]Downloading:  41%|████      | 180M/440M [00:11<00:15, 17.1MB/s]Downloading:  41%|████▏     | 182M/440M [00:11<00:14, 18.0MB/s]Downloading:  42%|████▏     | 184M/440M [00:11<00:14, 17.4MB/s]Downloading:  42%|████▏     | 186M/440M [00:11<00:13, 18.7MB/s]Downloading:  43%|████▎     | 188M/440M [00:12<00:13, 18.5MB/s]Downloading:  43%|████▎     | 190M/440M [00:12<00:12, 19.8MB/s]Downloading:  44%|████▎     | 192M/440M [00:12<00:12, 19.8MB/s]Downloading:  44%|████▍     | 194M/440M [00:12<00:12, 19.4MB/s]Downloading:  45%|████▍     | 197M/440M [00:12<00:12, 20.0MB/s]Downloading:  45%|████▌     | 199M/440M [00:12<00:12, 19.7MB/s]Downloading:  46%|████▌     | 201M/440M [00:12<00:17, 13.5MB/s]Downloading:  46%|████▋     | 204M/440M [00:12<00:14, 16.2MB/s]Downloading:  47%|████▋     | 206M/440M [00:13<00:14, 16.4MB/s]Downloading:  47%|████▋     | 208M/440M [00:13<00:14, 15.9MB/s]Downloading:  48%|████▊     | 210M/440M [00:13<00:15, 15.2MB/s]Downloading:  48%|████▊     | 212M/440M [00:13<00:15, 14.9MB/s]Downloading:  48%|████▊     | 213M/440M [00:13<00:14, 15.3MB/s]Downloading:  49%|████▉     | 215M/440M [00:13<00:18, 12.0MB/s]Downloading:  49%|████▉     | 216M/440M [00:13<00:18, 11.9MB/s]Downloading:  49%|████▉     | 218M/440M [00:13<00:18, 12.0MB/s]Downloading:  50%|████▉     | 219M/440M [00:14<00:18, 11.7MB/s]Downloading:  50%|█████     | 220M/440M [00:14<00:17, 12.3MB/s]Downloading:  50%|█████     | 222M/440M [00:14<00:17, 12.5MB/s]Downloading:  51%|█████     | 223M/440M [00:14<00:18, 12.1MB/s]Downloading:  51%|█████     | 224M/440M [00:14<00:17, 12.3MB/s]Downloading:  51%|█████     | 226M/440M [00:14<00:17, 12.0MB/s]Downloading:  52%|█████▏    | 227M/440M [00:14<00:16, 13.2MB/s]Downloading:  52%|█████▏    | 229M/440M [00:14<00:20, 10.5MB/s]Downloading:  52%|█████▏    | 230M/440M [00:15<00:18, 11.4MB/s]Downloading:  53%|█████▎    | 232M/440M [00:15<00:21, 9.89MB/s]Downloading:  53%|█████▎    | 233M/440M [00:15<00:20, 10.0MB/s]Downloading:  53%|█████▎    | 234M/440M [00:15<00:20, 10.1MB/s]Downloading:  53%|█████▎    | 235M/440M [00:15<00:24, 8.23MB/s]Downloading:  54%|█████▍    | 238M/440M [00:15<00:19, 10.6MB/s]Downloading:  54%|█████▍    | 240M/440M [00:15<00:19, 10.5MB/s]Downloading:  55%|█████▍    | 241M/440M [00:15<00:17, 11.4MB/s]Downloading:  55%|█████▌    | 243M/440M [00:16<00:18, 10.9MB/s]Downloading:  55%|█████▌    | 244M/440M [00:16<00:16, 12.0MB/s]Downloading:  56%|█████▌    | 246M/440M [00:16<00:14, 13.0MB/s]Downloading:  56%|█████▌    | 248M/440M [00:16<00:13, 14.4MB/s]Downloading:  57%|█████▋    | 250M/440M [00:16<00:11, 16.6MB/s]Downloading:  57%|█████▋    | 252M/440M [00:16<00:11, 16.8MB/s]Downloading:  58%|█████▊    | 254M/440M [00:16<00:12, 15.5MB/s]Downloading:  58%|█████▊    | 256M/440M [00:16<00:12, 14.4MB/s]Downloading:  58%|█████▊    | 257M/440M [00:17<00:13, 13.3MB/s]Downloading:  59%|█████▉    | 259M/440M [00:17<00:11, 15.2MB/s]Downloading:  59%|█████▉    | 261M/440M [00:17<00:15, 11.9MB/s]Downloading:  60%|█████▉    | 263M/440M [00:17<00:14, 12.6MB/s]Downloading:  60%|█████▉    | 264M/440M [00:17<00:19, 9.17MB/s]Downloading:  60%|██████    | 266M/440M [00:17<00:16, 10.5MB/s]Downloading:  61%|██████    | 268M/440M [00:17<00:13, 12.4MB/s]Downloading:  61%|██████    | 269M/440M [00:18<00:16, 10.6MB/s]Downloading:  62%|██████▏   | 272M/440M [00:18<00:12, 13.0MB/s]Downloading:  62%|██████▏   | 275M/440M [00:18<00:10, 15.6MB/s]Downloading:  63%|██████▎   | 277M/440M [00:18<00:09, 17.0MB/s]Downloading:  64%|██████▎   | 280M/440M [00:18<00:08, 19.2MB/s]Downloading:  64%|██████▍   | 283M/440M [00:18<00:08, 19.4MB/s]Downloading:  65%|██████▍   | 285M/440M [00:18<00:07, 19.9MB/s]Downloading:  65%|██████▌   | 287M/440M [00:18<00:07, 19.4MB/s]Downloading:  66%|██████▌   | 289M/440M [00:18<00:07, 19.3MB/s]Downloading:  66%|██████▌   | 291M/440M [00:19<00:07, 19.3MB/s]Downloading:  67%|██████▋   | 293M/440M [00:19<00:07, 19.1MB/s]Downloading:  67%|██████▋   | 295M/440M [00:19<00:08, 18.1MB/s]Downloading:  67%|██████▋   | 297M/440M [00:19<00:07, 19.1MB/s]Downloading:  68%|██████▊   | 300M/440M [00:19<00:07, 20.0MB/s]Downloading:  68%|██████▊   | 302M/440M [00:19<00:09, 14.1MB/s]Downloading:  69%|██████▉   | 305M/440M [00:19<00:07, 17.3MB/s]Downloading:  70%|██████▉   | 308M/440M [00:19<00:06, 19.2MB/s]Downloading:  70%|███████   | 310M/440M [00:20<00:06, 19.1MB/s]Downloading:  71%|███████   | 313M/440M [00:20<00:06, 21.1MB/s]Downloading:  72%|███████▏  | 316M/440M [00:20<00:05, 22.6MB/s]Downloading:  72%|███████▏  | 318M/440M [00:20<00:06, 19.5MB/s]Downloading:  73%|███████▎  | 320M/440M [00:20<00:07, 16.9MB/s]Downloading:  73%|███████▎  | 323M/440M [00:20<00:06, 18.3MB/s]Downloading:  74%|███████▎  | 325M/440M [00:20<00:08, 13.9MB/s]Downloading:  74%|███████▍  | 326M/440M [00:21<00:07, 14.8MB/s]Downloading:  75%|███████▍  | 328M/440M [00:21<00:06, 16.1MB/s]Downloading:  75%|███████▍  | 330M/440M [00:21<00:09, 11.7MB/s]Downloading:  75%|███████▌  | 332M/440M [00:21<00:10, 10.2MB/s]Downloading:  76%|███████▌  | 333M/440M [00:21<00:09, 10.9MB/s]Downloading:  76%|███████▌  | 334M/440M [00:21<00:09, 11.5MB/s]Downloading:  76%|███████▌  | 336M/440M [00:22<00:12, 8.43MB/s]Downloading:  77%|███████▋  | 338M/440M [00:22<00:10, 10.2MB/s]Downloading:  77%|███████▋  | 339M/440M [00:22<00:09, 10.6MB/s]Downloading:  77%|███████▋  | 340M/440M [00:22<00:09, 11.1MB/s]Downloading:  78%|███████▊  | 342M/440M [00:22<00:09, 10.8MB/s]Downloading:  78%|███████▊  | 343M/440M [00:22<00:08, 11.3MB/s]Downloading:  78%|███████▊  | 345M/440M [00:22<00:07, 12.6MB/s]Downloading:  79%|███████▊  | 347M/440M [00:22<00:06, 14.1MB/s]Downloading:  79%|███████▉  | 348M/440M [00:22<00:08, 11.3MB/s]Downloading:  79%|███████▉  | 350M/440M [00:23<00:08, 11.1MB/s]Downloading:  80%|███████▉  | 351M/440M [00:23<00:07, 12.0MB/s]Downloading:  80%|███████▉  | 352M/440M [00:23<00:07, 11.9MB/s]Downloading:  80%|████████  | 354M/440M [00:23<00:07, 11.5MB/s]Downloading:  81%|████████  | 355M/440M [00:23<00:07, 11.4MB/s]Downloading:  81%|████████  | 356M/440M [00:23<00:08, 10.3MB/s]Downloading:  81%|████████  | 357M/440M [00:23<00:08, 9.66MB/s]Downloading:  82%|████████▏ | 359M/440M [00:23<00:07, 11.5MB/s]Downloading:  82%|████████▏ | 361M/440M [00:24<00:05, 13.3MB/s]Downloading:  82%|████████▏ | 363M/440M [00:24<00:05, 14.5MB/s]Downloading:  83%|████████▎ | 365M/440M [00:24<00:04, 15.8MB/s]Downloading:  83%|████████▎ | 367M/440M [00:24<00:04, 16.7MB/s]Downloading:  84%|████████▍ | 369M/440M [00:24<00:05, 12.7MB/s]Downloading:  85%|████████▍ | 372M/440M [00:24<00:04, 15.1MB/s]Downloading:  85%|████████▌ | 375M/440M [00:25<00:05, 11.8MB/s]Downloading:  86%|████████▌ | 377M/440M [00:25<00:05, 11.3MB/s]Downloading:  86%|████████▌ | 380M/440M [00:25<00:04, 13.8MB/s]Downloading:  87%|████████▋ | 383M/440M [00:25<00:03, 16.5MB/s]Downloading:  88%|████████▊ | 386M/440M [00:25<00:02, 19.2MB/s]Downloading:  88%|████████▊ | 389M/440M [00:25<00:02, 21.7MB/s]Downloading:  89%|████████▉ | 392M/440M [00:25<00:02, 23.9MB/s]Downloading:  90%|████████▉ | 395M/440M [00:25<00:01, 25.6MB/s]Downloading:  90%|█████████ | 398M/440M [00:25<00:01, 23.8MB/s]Downloading:  91%|█████████ | 401M/440M [00:26<00:01, 22.7MB/s]Downloading:  92%|█████████▏| 403M/440M [00:26<00:02, 16.4MB/s]Downloading:  92%|█████████▏| 406M/440M [00:26<00:01, 18.8MB/s]Downloading:  93%|█████████▎| 408M/440M [00:26<00:01, 18.0MB/s]Downloading:  93%|█████████▎| 411M/440M [00:26<00:01, 19.0MB/s]Downloading:  94%|█████████▎| 413M/440M [00:26<00:01, 15.5MB/s]Downloading:  94%|█████████▍| 414M/440M [00:27<00:01, 13.3MB/s]Downloading:  94%|█████████▍| 416M/440M [00:27<00:01, 13.6MB/s]Downloading:  95%|█████████▍| 418M/440M [00:27<00:01, 14.3MB/s]Downloading:  95%|█████████▌| 419M/440M [00:27<00:01, 15.0MB/s]Downloading:  96%|█████████▌| 421M/440M [00:27<00:01, 15.5MB/s]Downloading:  96%|█████████▌| 424M/440M [00:27<00:01, 16.5MB/s]Downloading:  97%|█████████▋| 426M/440M [00:27<00:00, 17.5MB/s]Downloading:  97%|█████████▋| 428M/440M [00:27<00:00, 18.3MB/s]Downloading:  98%|█████████▊| 430M/440M [00:27<00:00, 18.9MB/s]Downloading:  98%|█████████▊| 432M/440M [00:27<00:00, 20.0MB/s]Downloading:  99%|█████████▊| 434M/440M [00:28<00:00, 20.0MB/s]Downloading:  99%|█████████▉| 436M/440M [00:28<00:00, 14.9MB/s]Downloading: 100%|█████████▉| 439M/440M [00:28<00:00, 17.9MB/s]Downloading: 100%|██████████| 440M/440M [00:28<00:00, 15.5MB/s]
I0511 02:23:50.943637 139729515079424 file_utils.py:448] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:23:50.944424 139729515079424 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:23:50.944953 139729515079424 filelock.py:318] Lock 139728388167496 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:23:50.945118 139729515079424 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
05/11/2020 02:23:58 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=2, bias=True)
  )
)

05/11/2020 02:23:58 Total number of params: 109483778
05/11/2020 02:23:58 At epoch 0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
05/11/2020 02:23:58 Task [ 0] updates[     1] train loss[2.56775] remaining[0:09:04]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
05/11/2020 02:25:47 Task [ 0] updates[   500] train loss[0.81314] remaining[0:01:08]
predicting 0
05/11/2020 02:26:57 Task assin2-rte -- epoch 0 -- Dev F1MAC: 91.799
05/11/2020 02:26:57 Task assin2-rte -- epoch 0 -- Dev ACC: 91.800
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 02:27:04 [new test scores saved.]
I0511 02:27:06.322939 139729515079424 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-rte/bert_base/seed/2018/model_0.pt
05/11/2020 02:27:06 At epoch 1
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
05/11/2020 02:27:46 Task [ 0] updates[  1000] train loss[0.61122] remaining[0:02:14]
05/11/2020 02:29:34 Task [ 0] updates[  1500] train loss[0.50176] remaining[0:00:27]
predicting 0
05/11/2020 02:30:03 Task assin2-rte -- epoch 1 -- Dev F1MAC: 94.400
05/11/2020 02:30:03 Task assin2-rte -- epoch 1 -- Dev ACC: 94.400
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 02:30:11 [new test scores saved.]
I0511 02:30:12.647152 139729515079424 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-rte/bert_base/seed/2018/model_1.pt
05/11/2020 02:30:12 At epoch 2
05/11/2020 02:31:34 Task [ 0] updates[  2000] train loss[0.43237] remaining[0:01:36]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
predicting 0
05/11/2020 02:33:09 Task assin2-rte -- epoch 2 -- Dev F1MAC: 94.600
05/11/2020 02:33:09 Task assin2-rte -- epoch 2 -- Dev ACC: 94.600
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 02:33:17 [new test scores saved.]
I0511 02:33:18.583431 139729515079424 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-rte/bert_base/seed/2018/model_2.pt
05/11/2020 02:33:18 At epoch 3
05/11/2020 02:33:31 Task [ 0] updates[  2500] train loss[0.37779] remaining[0:02:42]
05/11/2020 02:35:18 Task [ 0] updates[  3000] train loss[0.33902] remaining[0:00:53]
predicting 0
05/11/2020 02:36:14 Task assin2-rte -- epoch 3 -- Dev F1MAC: 95.200
05/11/2020 02:36:14 Task assin2-rte -- epoch 3 -- Dev ACC: 95.200
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 02:36:22 [new test scores saved.]
I0511 02:36:24.226881 139729515079424 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-rte/bert_base/seed/2018/model_3.pt
05/11/2020 02:36:24 At epoch 4
05/11/2020 02:37:17 Task [ 0] updates[  3500] train loss[0.30739] remaining[0:02:01]
05/11/2020 02:39:02 Task [ 0] updates[  4000] train loss[0.28377] remaining[0:00:13]
predicting 0
05/11/2020 02:39:18 Task assin2-rte -- epoch 4 -- Dev F1MAC: 95.800
05/11/2020 02:39:18 Task assin2-rte -- epoch 4 -- Dev ACC: 95.800
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 02:39:26 [new test scores saved.]
I0511 02:39:27.572544 139729515079424 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-rte/bert_base/seed/2018/model_4.pt
Preparing train arguments
I0511 02:39:30.623270 139927869323008 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
I0511 02:39:31.524044 139927869323008 filelock.py:274] Lock 139927500387272 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:39:31.524656 139927869323008 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmprsx0ght1
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/232k [00:00<00:26, 8.54kB/s]Downloading:  19%|█▉        | 44.0k/232k [00:00<00:15, 12.1kB/s]Downloading:  49%|████▊     | 113k/232k [00:00<00:06, 17.1kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 566kB/s] 
I0511 02:39:32.460445 139927869323008 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:39:32.460659 139927869323008 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:39:32.461500 139927869323008 filelock.py:318] Lock 139927500387272 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:39:32.461638 139927869323008 tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
05/11/2020 02:39:32 Task assin-ptbr-sts
05/11/2020 02:39:32 ../data/input/en/bert_base_uncased_lower/assin-ptbr-sts_train.json
05/11/2020 02:39:35 ../data/input/en/bert_base_uncased_lower/assin-ptbr-sts_dev.json
05/11/2020 02:39:36 ../data/input/en/bert_base_uncased_lower/assin-ptbr-sts_test.json
I0511 02:39:40.346677 139645896890112 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, bert_model_type='bert-base-uncased', cuda=True, data_dir='../data/input/en/bert_base_uncased_lower', data_sort_on=False, do_lower_case=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, embedding_opt=0, encode_mode=False, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=True, fp16_opt_level='O2', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='bert-base-uncased', init_ratio=1, learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, masked_lm_prob=0.15, max_answer_len=5, max_predictions_per_seq=128, max_seq_len=512, mem_cum_type='simple', mix_opt=0, mkd_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', num_hidden_layers=-1, optimizer='adamax', output_dir='../output/st-dnn/assin-ptbr-sts/bert_base/seed/2018', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, short_seq_prob=0.2, task_def='../data/task-def/assin-ptbr-sts.yaml', tensorboard=True, tensorboard_logdir='tensorboard_logdir', test_datasets=['assin-ptbr-sts'], train_datasets=['assin-ptbr-sts'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)
05/11/2020 02:39:41 0
05/11/2020 02:39:41 Launching the MT-DNN training
05/11/2020 02:39:41 Loading ../data/input/en/bert_base_uncased_lower/assin-ptbr-sts_train.json as task 0
Loaded 2500 samples out of 2500
Loaded 500 samples out of 500
Loaded 2000 samples out of 2000
05/11/2020 02:39:41 ####################
05/11/2020 02:39:41 {'log_file': 'mt-dnn-train.log', 'tensorboard': True, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': '../data/input/en/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../data/task-def/assin-ptbr-sts.yaml', 'train_datasets': ['assin-ptbr-sts'], 'test_datasets': ['assin-ptbr-sts'], 'glue_format_on': False, 'mkd_opt': 0, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 5, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 0, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': '../output/st-dnn/assin-ptbr-sts/bert_base/seed/2018', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': True, 'fp16_opt_level': 'O2', 'encode_mode': False, 'task_def_list': [{'kd_loss': '<LossCriterion.MseCriterion: 1>', 'loss': '<LossCriterion.MseCriterion: 1>', 'dropout_p': 'None', 'enable_san': 'False', 'split_names': "['train', 'dev', 'test']", 'metric_meta': '(<Metric.Pearson: 3>, <Metric.MSE: 11>)', 'task_type': '<TaskType.Regression: 2>', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'n_class': '1', 'label_vocab': 'None', 'self': '{}', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
05/11/2020 02:39:41 ####################
05/11/2020 02:39:41 ############# Gradient Accumulation Info #############
05/11/2020 02:39:41 number of step: 1565
05/11/2020 02:39:41 number of grad grad_accumulation step: 1
05/11/2020 02:39:41 adjusted number of step: 1565
05/11/2020 02:39:41 ############# Gradient Accumulation Info #############
I0511 02:39:42.029287 139645896890112 filelock.py:274] Lock 139644970444504 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:39:42.029881 139645896890112 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpl2vmfy40
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 276kB/s]
I0511 02:39:42.559714 139645896890112 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:39:42.559926 139645896890112 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:39:42.560822 139645896890112 filelock.py:318] Lock 139644970444504 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:39:42.561072 139645896890112 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:39:42.561670 139645896890112 configuration_utils.py:321] Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

I0511 02:39:42.563369 139645896890112 configuration_utils.py:321] Model config BertConfig {
  "adam_eps": 1e-06,
  "answer_att_hidden_size": 128,
  "answer_att_type": "bilinear",
  "answer_dropout_p": 0.1,
  "answer_mem_drop_p": 0.1,
  "answer_mem_type": 1,
  "answer_merge_opt": 1,
  "answer_num_turn": 5,
  "answer_opt": 0,
  "answer_rnn_type": "gru",
  "answer_sum_att_type": "bilinear",
  "answer_weight_norm_on": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "batch_size": 8,
  "batch_size_eval": 8,
  "bert_dropout_p": 0.1,
  "bert_l2norm": 0.0,
  "bert_model_type": "bert-base-uncased",
  "cuda": true,
  "data_dir": "../data/input/en/bert_base_uncased_lower",
  "data_sort_on": false,
  "do_lower_case": false,
  "dropout_p": 0.1,
  "dropout_w": 0.0,
  "dump_state_on": false,
  "embedding_opt": 0,
  "encode_mode": false,
  "encoder_type": 1,
  "epochs": 5,
  "fp16": true,
  "fp16_opt_level": "O2",
  "freeze_layers": -1,
  "global_grad_clipping": 1.0,
  "glue_format_on": false,
  "grad_accumulation_step": 1,
  "grad_clipping": 0,
  "have_lr_scheduler": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "init_checkpoint": "bert-base-uncased",
  "init_ratio": 1,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 5e-05,
  "log_file": "mt-dnn-train.log",
  "log_per_updates": 500,
  "lr_gamma": 0.5,
  "masked_lm_prob": 0.15,
  "max_answer_len": 5,
  "max_position_embeddings": 512,
  "max_predictions_per_seq": 128,
  "max_seq_len": 512,
  "mem_cum_type": "simple",
  "mix_opt": 0,
  "mkd_opt": 0,
  "model_ckpt": "checkpoints/model_0.pt",
  "model_type": "bert",
  "momentum": 0,
  "mtl_opt": 0,
  "multi_gpu_on": false,
  "multi_step_lr": "10,20,30",
  "name": "farmer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "optimizer": "adamax",
  "output_dir": "../output/st-dnn/assin-ptbr-sts/bert_base/seed/2018",
  "pad_token_id": 0,
  "ratio": 0,
  "resume": false,
  "save_per_updates": 10000,
  "save_per_updates_on": false,
  "scheduler_type": "ms",
  "seed": 2018,
  "short_seq_prob": 0.2,
  "task_def": "../data/task-def/assin-ptbr-sts.yaml",
  "task_def_list": [
    {
      "__class__": "<class 'experiments.exp_def.TaskDef'>",
      "data_type": "<DataFormat.PremiseAndOneHypothesis: 2>",
      "dropout_p": "None",
      "enable_san": "False",
      "kd_loss": "<LossCriterion.MseCriterion: 1>",
      "label_vocab": "None",
      "loss": "<LossCriterion.MseCriterion: 1>",
      "metric_meta": "(<Metric.Pearson: 3>, <Metric.MSE: 11>)",
      "n_class": "1",
      "self": "{}",
      "split_names": "['train', 'dev', 'test']",
      "task_type": "<TaskType.Regression: 2>"
    }
  ],
  "tensorboard": true,
  "tensorboard_logdir": "tensorboard_logdir",
  "test_datasets": [
    "assin-ptbr-sts"
  ],
  "train_datasets": [
    "assin-ptbr-sts"
  ],
  "type_vocab_size": 2,
  "update_bert_opt": 0,
  "vb_dropout": true,
  "vocab_size": 30522,
  "warmup": 0.1,
  "warmup_schedule": "warmup_linear",
  "weight_decay": 0
}

I0511 02:39:46.544498 139645896890112 filelock.py:274] Lock 139644336607920 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:39:46.545147 139645896890112 file_utils.py:444] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmplmg2rp8v
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 33.8k/440M [00:00<29:18, 250kB/s]Downloading:   0%|          | 98.3k/440M [00:00<24:18, 302kB/s]Downloading:   0%|          | 229k/440M [00:00<18:47, 390kB/s] Downloading:   0%|          | 492k/440M [00:00<13:59, 524kB/s]Downloading:   0%|          | 999k/440M [00:00<10:13, 717kB/s]Downloading:   0%|          | 2.06M/440M [00:00<07:20, 995kB/s]Downloading:   1%|          | 4.24M/440M [00:00<05:12, 1.39MB/s]Downloading:   2%|▏         | 7.37M/440M [00:00<03:41, 1.95MB/s]Downloading:   2%|▏         | 10.5M/440M [00:00<02:38, 2.72MB/s]Downloading:   3%|▎         | 13.1M/440M [00:01<01:55, 3.72MB/s]Downloading:   3%|▎         | 15.3M/440M [00:01<01:26, 4.89MB/s]Downloading:   4%|▍         | 17.4M/440M [00:01<01:07, 6.28MB/s]Downloading:   4%|▍         | 19.6M/440M [00:01<00:53, 7.94MB/s]Downloading:   5%|▍         | 21.6M/440M [00:01<00:44, 9.42MB/s]Downloading:   5%|▌         | 23.6M/440M [00:01<00:37, 11.1MB/s]Downloading:   6%|▌         | 25.9M/440M [00:01<00:32, 12.6MB/s]Downloading:   6%|▋         | 27.9M/440M [00:01<00:29, 14.1MB/s]Downloading:   7%|▋         | 30.0M/440M [00:01<00:26, 15.8MB/s]Downloading:   7%|▋         | 32.0M/440M [00:02<00:25, 16.3MB/s]Downloading:   8%|▊         | 34.2M/440M [00:02<00:23, 17.3MB/s]Downloading:   8%|▊         | 36.2M/440M [00:02<00:22, 18.1MB/s]Downloading:   9%|▊         | 38.2M/440M [00:02<00:22, 17.8MB/s]Downloading:   9%|▉         | 40.1M/440M [00:02<00:25, 15.8MB/s]Downloading:   9%|▉         | 41.8M/440M [00:02<00:34, 11.4MB/s]Downloading:  10%|▉         | 43.2M/440M [00:02<00:37, 10.5MB/s]Downloading:  10%|█         | 44.4M/440M [00:03<00:36, 10.8MB/s]Downloading:  10%|█         | 45.6M/440M [00:03<00:42, 9.35MB/s]Downloading:  11%|█         | 46.7M/440M [00:03<00:41, 9.54MB/s]Downloading:  11%|█         | 48.2M/440M [00:03<00:37, 10.5MB/s]Downloading:  11%|█         | 49.3M/440M [00:03<00:36, 10.7MB/s]Downloading:  12%|█▏        | 51.1M/440M [00:03<00:32, 11.9MB/s]Downloading:  12%|█▏        | 53.2M/440M [00:03<00:29, 13.0MB/s]Downloading:  13%|█▎        | 55.1M/440M [00:03<00:27, 14.1MB/s]Downloading:  13%|█▎        | 56.6M/440M [00:03<00:26, 14.3MB/s]Downloading:  13%|█▎        | 58.9M/440M [00:04<00:23, 16.1MB/s]Downloading:  14%|█▍        | 60.6M/440M [00:04<00:23, 15.9MB/s]Downloading:  14%|█▍        | 62.7M/440M [00:04<00:22, 16.7MB/s]Downloading:  15%|█▍        | 65.0M/440M [00:04<00:20, 18.1MB/s]Downloading:  15%|█▌        | 66.9M/440M [00:04<00:20, 18.0MB/s]Downloading:  16%|█▌        | 69.2M/440M [00:04<00:19, 19.1MB/s]Downloading:  16%|█▌        | 71.1M/440M [00:04<00:20, 18.1MB/s]Downloading:  17%|█▋        | 73.2M/440M [00:04<00:19, 18.5MB/s]Downloading:  17%|█▋        | 75.6M/440M [00:04<00:18, 19.8MB/s]Downloading:  18%|█▊        | 77.8M/440M [00:05<00:17, 20.3MB/s]Downloading:  18%|█▊        | 79.8M/440M [00:05<00:18, 19.7MB/s]Downloading:  19%|█▊        | 81.8M/440M [00:05<00:18, 19.3MB/s]Downloading:  19%|█▉        | 84.0M/440M [00:05<00:17, 19.9MB/s]Downloading:  20%|█▉        | 86.0M/440M [00:05<00:18, 19.4MB/s]Downloading:  20%|█▉        | 87.9M/440M [00:05<00:21, 16.1MB/s]Downloading:  21%|██        | 90.3M/440M [00:05<00:19, 17.9MB/s]Downloading:  21%|██        | 92.3M/440M [00:05<00:23, 14.8MB/s]Downloading:  21%|██▏       | 93.9M/440M [00:06<00:26, 13.3MB/s]Downloading:  22%|██▏       | 95.8M/440M [00:06<00:23, 14.5MB/s]Downloading:  22%|██▏       | 97.4M/440M [00:06<00:25, 13.7MB/s]Downloading:  22%|██▏       | 98.9M/440M [00:06<00:24, 14.0MB/s]Downloading:  23%|██▎       | 100M/440M [00:06<00:27, 12.4MB/s] Downloading:  23%|██▎       | 102M/440M [00:06<00:33, 10.0MB/s]Downloading:  24%|██▍       | 105M/440M [00:06<00:26, 12.5MB/s]Downloading:  24%|██▍       | 108M/440M [00:06<00:21, 15.2MB/s]Downloading:  25%|██▌       | 111M/440M [00:07<00:18, 17.5MB/s]Downloading:  26%|██▌       | 113M/440M [00:07<00:18, 18.2MB/s]Downloading:  26%|██▌       | 115M/440M [00:07<00:17, 19.0MB/s]Downloading:  27%|██▋       | 117M/440M [00:07<00:17, 18.6MB/s]Downloading:  27%|██▋       | 119M/440M [00:07<00:17, 18.5MB/s]Downloading:  28%|██▊       | 121M/440M [00:07<00:16, 18.8MB/s]Downloading:  28%|██▊       | 123M/440M [00:07<00:16, 18.7MB/s]Downloading:  28%|██▊       | 125M/440M [00:07<00:16, 19.3MB/s]Downloading:  29%|██▉       | 127M/440M [00:07<00:19, 16.3MB/s]Downloading:  29%|██▉       | 129M/440M [00:08<00:18, 16.5MB/s]Downloading:  30%|██▉       | 131M/440M [00:08<00:17, 17.6MB/s]Downloading:  30%|███       | 133M/440M [00:08<00:17, 17.1MB/s]Downloading:  31%|███       | 135M/440M [00:08<00:23, 12.8MB/s]Downloading:  31%|███       | 136M/440M [00:08<00:22, 13.4MB/s]Downloading:  31%|███▏      | 138M/440M [00:08<00:21, 14.3MB/s]Downloading:  32%|███▏      | 140M/440M [00:08<00:18, 16.1MB/s]Downloading:  32%|███▏      | 142M/440M [00:08<00:20, 14.8MB/s]Downloading:  33%|███▎      | 144M/440M [00:09<00:19, 15.1MB/s]Downloading:  33%|███▎      | 146M/440M [00:09<00:18, 16.2MB/s]Downloading:  34%|███▎      | 148M/440M [00:09<00:17, 16.9MB/s]Downloading:  34%|███▍      | 150M/440M [00:09<00:17, 17.1MB/s]Downloading:  34%|███▍      | 151M/440M [00:09<00:22, 12.7MB/s]Downloading:  35%|███▍      | 153M/440M [00:09<00:23, 12.1MB/s]Downloading:  35%|███▌      | 154M/440M [00:09<00:23, 12.3MB/s]Downloading:  35%|███▌      | 156M/440M [00:09<00:23, 12.2MB/s]Downloading:  36%|███▌      | 157M/440M [00:10<00:26, 10.9MB/s]Downloading:  36%|███▌      | 158M/440M [00:10<00:24, 11.3MB/s]Downloading:  36%|███▌      | 159M/440M [00:10<00:25, 10.9MB/s]Downloading:  37%|███▋      | 161M/440M [00:10<00:23, 12.1MB/s]Downloading:  37%|███▋      | 162M/440M [00:10<00:24, 11.5MB/s]Downloading:  37%|███▋      | 164M/440M [00:10<00:20, 13.3MB/s]Downloading:  38%|███▊      | 166M/440M [00:10<00:23, 11.8MB/s]Downloading:  38%|███▊      | 167M/440M [00:10<00:23, 11.6MB/s]Downloading:  38%|███▊      | 168M/440M [00:11<00:28, 9.42MB/s]Downloading:  39%|███▉      | 172M/440M [00:11<00:22, 12.1MB/s]Downloading:  40%|███▉      | 174M/440M [00:11<00:18, 14.2MB/s]Downloading:  40%|████      | 176M/440M [00:11<00:17, 15.1MB/s]Downloading:  41%|████      | 179M/440M [00:11<00:15, 16.5MB/s]Downloading:  41%|████      | 181M/440M [00:11<00:14, 18.1MB/s]Downloading:  42%|████▏     | 183M/440M [00:11<00:13, 19.0MB/s]Downloading:  42%|████▏     | 185M/440M [00:11<00:13, 19.6MB/s]Downloading:  43%|████▎     | 187M/440M [00:12<00:14, 17.2MB/s]Downloading:  43%|████▎     | 189M/440M [00:12<00:17, 14.3MB/s]Downloading:  43%|████▎     | 191M/440M [00:12<00:16, 15.1MB/s]Downloading:  44%|████▍     | 193M/440M [00:12<00:16, 14.7MB/s]Downloading:  44%|████▍     | 195M/440M [00:12<00:15, 15.4MB/s]Downloading:  45%|████▍     | 197M/440M [00:12<00:14, 16.8MB/s]Downloading:  45%|████▌     | 198M/440M [00:12<00:14, 17.2MB/s]Downloading:  45%|████▌     | 200M/440M [00:12<00:15, 16.0MB/s]Downloading:  46%|████▌     | 202M/440M [00:13<00:22, 10.5MB/s]Downloading:  47%|████▋     | 205M/440M [00:13<00:18, 13.0MB/s]Downloading:  47%|████▋     | 207M/440M [00:13<00:16, 14.4MB/s]Downloading:  47%|████▋     | 209M/440M [00:13<00:14, 15.9MB/s]Downloading:  48%|████▊     | 211M/440M [00:13<00:13, 17.3MB/s]Downloading:  48%|████▊     | 213M/440M [00:13<00:13, 16.3MB/s]Downloading:  49%|████▉     | 215M/440M [00:13<00:13, 17.3MB/s]Downloading:  49%|████▉     | 217M/440M [00:13<00:12, 18.5MB/s]Downloading:  50%|████▉     | 219M/440M [00:13<00:11, 19.3MB/s]Downloading:  50%|█████     | 221M/440M [00:14<00:11, 19.7MB/s]Downloading:  51%|█████     | 224M/440M [00:14<00:11, 19.7MB/s]Downloading:  51%|█████     | 226M/440M [00:14<00:12, 17.8MB/s]Downloading:  52%|█████▏    | 227M/440M [00:14<00:13, 16.2MB/s]Downloading:  52%|█████▏    | 229M/440M [00:14<00:13, 15.6MB/s]Downloading:  52%|█████▏    | 231M/440M [00:14<00:13, 15.1MB/s]Downloading:  53%|█████▎    | 232M/440M [00:14<00:13, 15.4MB/s]Downloading:  53%|█████▎    | 234M/440M [00:14<00:13, 14.8MB/s]Downloading:  54%|█████▎    | 236M/440M [00:15<00:13, 15.7MB/s]Downloading:  54%|█████▍    | 237M/440M [00:15<00:13, 15.5MB/s]Downloading:  54%|█████▍    | 240M/440M [00:15<00:11, 16.9MB/s]Downloading:  55%|█████▍    | 241M/440M [00:15<00:11, 17.4MB/s]Downloading:  55%|█████▌    | 244M/440M [00:15<00:10, 18.4MB/s]Downloading:  56%|█████▌    | 246M/440M [00:15<00:10, 19.3MB/s]Downloading:  56%|█████▌    | 248M/440M [00:15<00:10, 19.1MB/s]Downloading:  57%|█████▋    | 250M/440M [00:15<00:10, 18.5MB/s]Downloading:  57%|█████▋    | 252M/440M [00:15<00:11, 16.9MB/s]Downloading:  58%|█████▊    | 253M/440M [00:15<00:10, 17.1MB/s]Downloading:  58%|█████▊    | 255M/440M [00:16<00:11, 16.8MB/s]Downloading:  58%|█████▊    | 257M/440M [00:16<00:10, 17.1MB/s]Downloading:  59%|█████▉    | 259M/440M [00:16<00:10, 17.8MB/s]Downloading:  59%|█████▉    | 261M/440M [00:16<00:10, 16.5MB/s]Downloading:  60%|█████▉    | 263M/440M [00:16<00:10, 17.6MB/s]Downloading:  60%|██████    | 265M/440M [00:16<00:10, 16.3MB/s]Downloading:  60%|██████    | 266M/440M [00:16<00:14, 11.8MB/s]Downloading:  61%|██████    | 270M/440M [00:16<00:11, 14.7MB/s]Downloading:  62%|██████▏   | 272M/440M [00:17<00:10, 15.8MB/s]Downloading:  62%|██████▏   | 274M/440M [00:17<00:11, 15.1MB/s]Downloading:  63%|██████▎   | 275M/440M [00:17<00:10, 15.5MB/s]Downloading:  63%|██████▎   | 277M/440M [00:17<00:10, 15.7MB/s]Downloading:  63%|██████▎   | 279M/440M [00:17<00:10, 16.0MB/s]Downloading:  64%|██████▎   | 281M/440M [00:17<00:09, 16.1MB/s]Downloading:  64%|██████▍   | 282M/440M [00:17<00:09, 16.6MB/s]Downloading:  65%|██████▍   | 284M/440M [00:17<00:09, 17.1MB/s]Downloading:  65%|██████▍   | 286M/440M [00:17<00:08, 17.7MB/s]Downloading:  65%|██████▌   | 288M/440M [00:18<00:08, 17.6MB/s]Downloading:  66%|██████▌   | 290M/440M [00:18<00:08, 18.4MB/s]Downloading:  66%|██████▋   | 292M/440M [00:18<00:07, 19.0MB/s]Downloading:  67%|██████▋   | 294M/440M [00:18<00:07, 19.4MB/s]Downloading:  67%|██████▋   | 296M/440M [00:18<00:07, 19.9MB/s]Downloading:  68%|██████▊   | 299M/440M [00:18<00:06, 21.4MB/s]Downloading:  68%|██████▊   | 301M/440M [00:18<00:08, 16.0MB/s]Downloading:  69%|██████▉   | 304M/440M [00:18<00:07, 19.0MB/s]Downloading:  70%|██████▉   | 307M/440M [00:18<00:06, 21.1MB/s]Downloading:  70%|███████   | 310M/440M [00:19<00:05, 22.5MB/s]Downloading:  71%|███████   | 312M/440M [00:19<00:06, 18.6MB/s]Downloading:  71%|███████▏  | 315M/440M [00:19<00:06, 18.8MB/s]Downloading:  72%|███████▏  | 317M/440M [00:19<00:10, 12.1MB/s]Downloading:  72%|███████▏  | 318M/440M [00:20<00:19, 6.31MB/s]Downloading:  73%|███████▎  | 321M/440M [00:20<00:14, 8.31MB/s]Downloading:  74%|███████▎  | 324M/440M [00:20<00:10, 10.6MB/s]Downloading:  74%|███████▍  | 327M/440M [00:20<00:09, 11.5MB/s]Downloading:  75%|███████▍  | 329M/440M [00:20<00:08, 13.0MB/s]Downloading:  75%|███████▌  | 331M/440M [00:20<00:07, 14.5MB/s]Downloading:  76%|███████▌  | 333M/440M [00:21<00:12, 8.88MB/s]Downloading:  76%|███████▌  | 335M/440M [00:21<00:09, 10.7MB/s]Downloading:  76%|███████▋  | 337M/440M [00:21<00:10, 10.1MB/s]Downloading:  77%|███████▋  | 338M/440M [00:21<00:09, 11.3MB/s]Downloading:  77%|███████▋  | 340M/440M [00:21<00:08, 12.4MB/s]Downloading:  78%|███████▊  | 342M/440M [00:21<00:07, 14.0MB/s]Downloading:  78%|███████▊  | 344M/440M [00:21<00:06, 15.1MB/s]Downloading:  78%|███████▊  | 345M/440M [00:22<00:05, 15.9MB/s]Downloading:  79%|███████▉  | 347M/440M [00:22<00:05, 16.5MB/s]Downloading:  79%|███████▉  | 349M/440M [00:22<00:05, 17.7MB/s]Downloading:  80%|███████▉  | 351M/440M [00:22<00:05, 16.5MB/s]Downloading:  80%|████████  | 354M/440M [00:22<00:04, 18.1MB/s]Downloading:  81%|████████  | 355M/440M [00:22<00:04, 18.0MB/s]Downloading:  81%|████████  | 358M/440M [00:22<00:04, 19.4MB/s]Downloading:  82%|████████▏ | 360M/440M [00:22<00:04, 17.7MB/s]Downloading:  82%|████████▏ | 362M/440M [00:22<00:05, 15.6MB/s]Downloading:  83%|████████▎ | 364M/440M [00:23<00:04, 16.7MB/s]Downloading:  83%|████████▎ | 366M/440M [00:23<00:04, 15.3MB/s]Downloading:  83%|████████▎ | 367M/440M [00:23<00:13, 5.53MB/s]Downloading:  84%|████████▍ | 371M/440M [00:24<00:09, 7.41MB/s]Downloading:  85%|████████▍ | 373M/440M [00:24<00:08, 7.80MB/s]Downloading:  85%|████████▍ | 374M/440M [00:24<00:08, 8.17MB/s]Downloading:  85%|████████▌ | 376M/440M [00:24<00:07, 8.45MB/s]Downloading:  86%|████████▌ | 377M/440M [00:24<00:07, 8.64MB/s]Downloading:  86%|████████▌ | 378M/440M [00:24<00:06, 9.15MB/s]Downloading:  86%|████████▌ | 379M/440M [00:24<00:06, 9.83MB/s]Downloading:  86%|████████▋ | 381M/440M [00:25<00:05, 10.8MB/s]Downloading:  87%|████████▋ | 382M/440M [00:25<00:04, 12.0MB/s]Downloading:  87%|████████▋ | 384M/440M [00:25<00:04, 13.5MB/s]Downloading:  88%|████████▊ | 386M/440M [00:25<00:03, 15.2MB/s]Downloading:  88%|████████▊ | 389M/440M [00:25<00:02, 17.3MB/s]Downloading:  89%|████████▉ | 392M/440M [00:25<00:02, 19.6MB/s]Downloading:  90%|████████▉ | 395M/440M [00:25<00:02, 22.1MB/s]Downloading:  90%|█████████ | 397M/440M [00:25<00:02, 15.2MB/s]Downloading:  91%|█████████ | 399M/440M [00:26<00:02, 14.0MB/s]Downloading:  91%|█████████ | 401M/440M [00:26<00:03, 11.7MB/s]Downloading:  92%|█████████▏| 404M/440M [00:26<00:02, 14.3MB/s]Downloading:  92%|█████████▏| 407M/440M [00:26<00:02, 16.6MB/s]Downloading:  93%|█████████▎| 409M/440M [00:26<00:01, 16.6MB/s]Downloading:  93%|█████████▎| 411M/440M [00:26<00:01, 17.2MB/s]Downloading:  94%|█████████▍| 413M/440M [00:26<00:01, 17.7MB/s]Downloading:  94%|█████████▍| 415M/440M [00:26<00:01, 17.1MB/s]Downloading:  95%|█████████▍| 417M/440M [00:27<00:01, 17.5MB/s]Downloading:  95%|█████████▌| 419M/440M [00:27<00:01, 19.0MB/s]Downloading:  96%|█████████▌| 421M/440M [00:27<00:01, 18.3MB/s]Downloading:  96%|█████████▌| 423M/440M [00:27<00:00, 17.7MB/s]Downloading:  97%|█████████▋| 425M/440M [00:27<00:00, 18.4MB/s]Downloading:  97%|█████████▋| 427M/440M [00:27<00:00, 18.7MB/s]Downloading:  97%|█████████▋| 429M/440M [00:27<00:00, 18.6MB/s]Downloading:  98%|█████████▊| 432M/440M [00:27<00:00, 20.1MB/s]Downloading:  98%|█████████▊| 434M/440M [00:28<00:00, 13.7MB/s]Downloading:  99%|█████████▉| 437M/440M [00:28<00:00, 16.9MB/s]Downloading: 100%|█████████▉| 440M/440M [00:28<00:00, 16.1MB/s]Downloading: 100%|██████████| 440M/440M [00:28<00:00, 15.5MB/s]
I0511 02:40:15.312271 139645896890112 file_utils.py:448] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:40:15.312933 139645896890112 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:40:15.313806 139645896890112 filelock.py:318] Lock 139644336607920 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:40:15.313967 139645896890112 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
05/11/2020 02:40:22 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=1, bias=True)
  )
)

05/11/2020 02:40:22 Total number of params: 109483009
05/11/2020 02:40:22 At epoch 0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
05/11/2020 02:40:22 Task [ 0] updates[     1] train loss[5.04629] remaining[0:03:47]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
predicting 0
05/11/2020 02:41:29 Task assin-ptbr-sts -- epoch 0 -- Dev Pearson: 73.631
05/11/2020 02:41:29 Task assin-ptbr-sts -- epoch 0 -- Dev MSE: 10.038
predicting 0
predicting 100
predicting 200
05/11/2020 02:41:35 [new test scores saved.]
I0511 02:41:36.965388 139645896890112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-sts/bert_base/seed/2018/model_0.pt
05/11/2020 02:41:36 At epoch 1
05/11/2020 02:42:15 Task [ 0] updates[   500] train loss[1.16435] remaining[0:00:25]
predicting 0
05/11/2020 02:42:43 Task assin-ptbr-sts -- epoch 1 -- Dev Pearson: 76.496
05/11/2020 02:42:43 Task assin-ptbr-sts -- epoch 1 -- Dev MSE: 10.038
predicting 0
predicting 100
predicting 200
05/11/2020 02:42:49 [new test scores saved.]
I0511 02:42:51.022916 139645896890112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-sts/bert_base/seed/2018/model_1.pt
05/11/2020 02:42:51 At epoch 2
predicting 0
05/11/2020 02:43:58 Task assin-ptbr-sts -- epoch 2 -- Dev Pearson: 76.258
05/11/2020 02:43:58 Task assin-ptbr-sts -- epoch 2 -- Dev MSE: 10.038
predicting 0
predicting 100
predicting 200
05/11/2020 02:44:04 [new test scores saved.]
I0511 02:44:05.588857 139645896890112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-sts/bert_base/seed/2018/model_2.pt
05/11/2020 02:44:05 At epoch 3
05/11/2020 02:44:18 Task [ 0] updates[  1000] train loss[0.69700] remaining[0:00:53]
predicting 0
05/11/2020 02:45:13 Task assin-ptbr-sts -- epoch 3 -- Dev Pearson: 76.523
05/11/2020 02:45:13 Task assin-ptbr-sts -- epoch 3 -- Dev MSE: 10.038
predicting 0
predicting 100
predicting 200
05/11/2020 02:45:19 [new test scores saved.]
I0511 02:45:20.605355 139645896890112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-sts/bert_base/seed/2018/model_3.pt
05/11/2020 02:45:20 At epoch 4
05/11/2020 02:46:12 Task [ 0] updates[  1500] train loss[0.51413] remaining[0:00:13]
predicting 0
05/11/2020 02:46:27 Task assin-ptbr-sts -- epoch 4 -- Dev Pearson: 77.065
05/11/2020 02:46:27 Task assin-ptbr-sts -- epoch 4 -- Dev MSE: 10.038
predicting 0
predicting 100
predicting 200
05/11/2020 02:46:34 [new test scores saved.]
I0511 02:46:35.424785 139645896890112 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptbr-sts/bert_base/seed/2018/model_4.pt
Preparing train arguments
I0511 02:46:38.506289 140015501432576 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
I0511 02:46:39.427089 140015501432576 filelock.py:274] Lock 140014543763552 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:46:39.427783 140015501432576 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8ejgd7k2
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/232k [00:00<00:27, 8.49kB/s]Downloading:  15%|█▌        | 34.8k/232k [00:00<00:16, 12.0kB/s]Downloading:  38%|███▊      | 87.0k/232k [00:00<00:08, 16.9kB/s]Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 24.0kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 473kB/s] 
I0511 02:46:40.435408 140015501432576 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:46:40.435612 140015501432576 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:46:40.436397 140015501432576 filelock.py:318] Lock 140014543763552 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:46:40.436567 140015501432576 tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
05/11/2020 02:46:40 Task assin-ptpt-sts
05/11/2020 02:46:40 ../data/input/en/bert_base_uncased_lower/assin-ptpt-sts_train.json
05/11/2020 02:46:44 ../data/input/en/bert_base_uncased_lower/assin-ptpt-sts_dev.json
05/11/2020 02:46:45 ../data/input/en/bert_base_uncased_lower/assin-ptpt-sts_test.json
I0511 02:46:49.261608 140171865458432 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, bert_model_type='bert-base-uncased', cuda=True, data_dir='../data/input/en/bert_base_uncased_lower', data_sort_on=False, do_lower_case=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, embedding_opt=0, encode_mode=False, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=True, fp16_opt_level='O2', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='bert-base-uncased', init_ratio=1, learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, masked_lm_prob=0.15, max_answer_len=5, max_predictions_per_seq=128, max_seq_len=512, mem_cum_type='simple', mix_opt=0, mkd_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', num_hidden_layers=-1, optimizer='adamax', output_dir='../output/st-dnn/assin-ptpt-sts/bert_base/seed/2018', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, short_seq_prob=0.2, task_def='../data/task-def/assin-ptpt-sts.yaml', tensorboard=True, tensorboard_logdir='tensorboard_logdir', test_datasets=['assin-ptpt-sts'], train_datasets=['assin-ptpt-sts'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)
05/11/2020 02:46:50 0
05/11/2020 02:46:50 Launching the MT-DNN training
05/11/2020 02:46:50 Loading ../data/input/en/bert_base_uncased_lower/assin-ptpt-sts_train.json as task 0
Loaded 2500 samples out of 2500
Loaded 500 samples out of 500
Loaded 2000 samples out of 2000
05/11/2020 02:46:50 ####################
05/11/2020 02:46:50 {'log_file': 'mt-dnn-train.log', 'tensorboard': True, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': '../data/input/en/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../data/task-def/assin-ptpt-sts.yaml', 'train_datasets': ['assin-ptpt-sts'], 'test_datasets': ['assin-ptpt-sts'], 'glue_format_on': False, 'mkd_opt': 0, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 5, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 0, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': '../output/st-dnn/assin-ptpt-sts/bert_base/seed/2018', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': True, 'fp16_opt_level': 'O2', 'encode_mode': False, 'task_def_list': [{'kd_loss': '<LossCriterion.MseCriterion: 1>', 'loss': '<LossCriterion.MseCriterion: 1>', 'dropout_p': 'None', 'enable_san': 'False', 'split_names': "['train', 'dev', 'test']", 'metric_meta': '(<Metric.Pearson: 3>, <Metric.MSE: 11>)', 'task_type': '<TaskType.Regression: 2>', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'n_class': '1', 'label_vocab': 'None', 'self': '{}', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
05/11/2020 02:46:50 ####################
05/11/2020 02:46:50 ############# Gradient Accumulation Info #############
05/11/2020 02:46:50 number of step: 1565
05/11/2020 02:46:50 number of grad grad_accumulation step: 1
05/11/2020 02:46:50 adjusted number of step: 1565
05/11/2020 02:46:50 ############# Gradient Accumulation Info #############
I0511 02:46:50.940587 140171865458432 filelock.py:274] Lock 140170905265880 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:46:50.941413 140171865458432 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpoiyp5yya
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 294kB/s]
I0511 02:46:51.503498 140171865458432 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:46:51.503704 140171865458432 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:46:51.504536 140171865458432 filelock.py:318] Lock 140170905265880 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:46:51.504781 140171865458432 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:46:51.505334 140171865458432 configuration_utils.py:321] Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

I0511 02:46:51.506999 140171865458432 configuration_utils.py:321] Model config BertConfig {
  "adam_eps": 1e-06,
  "answer_att_hidden_size": 128,
  "answer_att_type": "bilinear",
  "answer_dropout_p": 0.1,
  "answer_mem_drop_p": 0.1,
  "answer_mem_type": 1,
  "answer_merge_opt": 1,
  "answer_num_turn": 5,
  "answer_opt": 0,
  "answer_rnn_type": "gru",
  "answer_sum_att_type": "bilinear",
  "answer_weight_norm_on": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "batch_size": 8,
  "batch_size_eval": 8,
  "bert_dropout_p": 0.1,
  "bert_l2norm": 0.0,
  "bert_model_type": "bert-base-uncased",
  "cuda": true,
  "data_dir": "../data/input/en/bert_base_uncased_lower",
  "data_sort_on": false,
  "do_lower_case": false,
  "dropout_p": 0.1,
  "dropout_w": 0.0,
  "dump_state_on": false,
  "embedding_opt": 0,
  "encode_mode": false,
  "encoder_type": 1,
  "epochs": 5,
  "fp16": true,
  "fp16_opt_level": "O2",
  "freeze_layers": -1,
  "global_grad_clipping": 1.0,
  "glue_format_on": false,
  "grad_accumulation_step": 1,
  "grad_clipping": 0,
  "have_lr_scheduler": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "init_checkpoint": "bert-base-uncased",
  "init_ratio": 1,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 5e-05,
  "log_file": "mt-dnn-train.log",
  "log_per_updates": 500,
  "lr_gamma": 0.5,
  "masked_lm_prob": 0.15,
  "max_answer_len": 5,
  "max_position_embeddings": 512,
  "max_predictions_per_seq": 128,
  "max_seq_len": 512,
  "mem_cum_type": "simple",
  "mix_opt": 0,
  "mkd_opt": 0,
  "model_ckpt": "checkpoints/model_0.pt",
  "model_type": "bert",
  "momentum": 0,
  "mtl_opt": 0,
  "multi_gpu_on": false,
  "multi_step_lr": "10,20,30",
  "name": "farmer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "optimizer": "adamax",
  "output_dir": "../output/st-dnn/assin-ptpt-sts/bert_base/seed/2018",
  "pad_token_id": 0,
  "ratio": 0,
  "resume": false,
  "save_per_updates": 10000,
  "save_per_updates_on": false,
  "scheduler_type": "ms",
  "seed": 2018,
  "short_seq_prob": 0.2,
  "task_def": "../data/task-def/assin-ptpt-sts.yaml",
  "task_def_list": [
    {
      "__class__": "<class 'experiments.exp_def.TaskDef'>",
      "data_type": "<DataFormat.PremiseAndOneHypothesis: 2>",
      "dropout_p": "None",
      "enable_san": "False",
      "kd_loss": "<LossCriterion.MseCriterion: 1>",
      "label_vocab": "None",
      "loss": "<LossCriterion.MseCriterion: 1>",
      "metric_meta": "(<Metric.Pearson: 3>, <Metric.MSE: 11>)",
      "n_class": "1",
      "self": "{}",
      "split_names": "['train', 'dev', 'test']",
      "task_type": "<TaskType.Regression: 2>"
    }
  ],
  "tensorboard": true,
  "tensorboard_logdir": "tensorboard_logdir",
  "test_datasets": [
    "assin-ptpt-sts"
  ],
  "train_datasets": [
    "assin-ptpt-sts"
  ],
  "type_vocab_size": 2,
  "update_bert_opt": 0,
  "vb_dropout": true,
  "vocab_size": 30522,
  "warmup": 0.1,
  "warmup_schedule": "warmup_linear",
  "weight_decay": 0
}

I0511 02:46:55.825886 140171865458432 filelock.py:274] Lock 140170734006960 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:46:55.826640 140171865458432 file_utils.py:444] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpa9nf91_2
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 33.8k/440M [00:00<25:20, 290kB/s]Downloading:   0%|          | 98.3k/440M [00:00<21:18, 344kB/s]Downloading:   0%|          | 229k/440M [00:00<16:39, 441kB/s] Downloading:   0%|          | 475k/440M [00:00<12:33, 584kB/s]Downloading:   0%|          | 983k/440M [00:00<09:13, 794kB/s]Downloading:   0%|          | 2.03M/440M [00:00<06:38, 1.10MB/s]Downloading:   1%|          | 4.16M/440M [00:00<04:44, 1.54MB/s]Downloading:   2%|▏         | 7.29M/440M [00:00<03:21, 2.15MB/s]Downloading:   2%|▏         | 10.4M/440M [00:00<02:24, 2.98MB/s]Downloading:   3%|▎         | 12.7M/440M [00:01<01:45, 4.04MB/s]Downloading:   3%|▎         | 14.8M/440M [00:01<01:20, 5.29MB/s]Downloading:   4%|▍         | 16.9M/440M [00:01<01:03, 6.66MB/s]Downloading:   4%|▍         | 18.9M/440M [00:01<00:51, 8.20MB/s]Downloading:   5%|▍         | 20.9M/440M [00:01<00:42, 9.90MB/s]Downloading:   5%|▌         | 23.0M/440M [00:01<00:35, 11.8MB/s]Downloading:   6%|▌         | 25.0M/440M [00:01<00:33, 12.3MB/s]Downloading:   6%|▌         | 26.8M/440M [00:01<00:33, 12.2MB/s]Downloading:   6%|▋         | 28.4M/440M [00:01<00:31, 13.1MB/s]Downloading:   7%|▋         | 30.0M/440M [00:02<00:34, 11.9MB/s]Downloading:   7%|▋         | 31.4M/440M [00:02<00:37, 10.9MB/s]Downloading:   8%|▊         | 33.5M/440M [00:02<00:32, 12.5MB/s]Downloading:   8%|▊         | 35.0M/440M [00:02<00:34, 11.8MB/s]Downloading:   8%|▊         | 36.8M/440M [00:02<00:30, 13.1MB/s]Downloading:   9%|▊         | 38.3M/440M [00:02<00:31, 12.8MB/s]Downloading:   9%|▉         | 39.7M/440M [00:02<00:39, 10.2MB/s]Downloading:   9%|▉         | 40.8M/440M [00:03<00:43, 9.24MB/s]Downloading:  10%|▉         | 41.9M/440M [00:03<00:49, 8.00MB/s]Downloading:  10%|▉         | 43.6M/440M [00:03<00:41, 9.52MB/s]Downloading:  10%|█         | 45.7M/440M [00:03<00:34, 11.3MB/s]Downloading:  11%|█         | 47.1M/440M [00:03<00:32, 12.0MB/s]Downloading:  11%|█         | 48.8M/440M [00:03<00:29, 13.2MB/s]Downloading:  12%|█▏        | 50.8M/440M [00:03<00:26, 14.8MB/s]Downloading:  12%|█▏        | 52.5M/440M [00:03<00:27, 14.3MB/s]Downloading:  12%|█▏        | 54.6M/440M [00:04<00:24, 15.9MB/s]Downloading:  13%|█▎        | 56.4M/440M [00:04<00:24, 15.9MB/s]Downloading:  13%|█▎        | 58.6M/440M [00:04<00:21, 17.4MB/s]Downloading:  14%|█▎        | 60.5M/440M [00:04<00:35, 10.7MB/s]Downloading:  14%|█▍        | 62.3M/440M [00:04<00:30, 12.2MB/s]Downloading:  15%|█▍        | 63.9M/440M [00:04<00:30, 12.5MB/s]Downloading:  15%|█▍        | 65.4M/440M [00:04<00:30, 12.5MB/s]Downloading:  15%|█▌        | 67.6M/440M [00:05<00:27, 13.8MB/s]Downloading:  16%|█▌        | 69.1M/440M [00:05<00:27, 13.3MB/s]Downloading:  16%|█▌        | 70.5M/440M [00:05<00:28, 12.9MB/s]Downloading:  16%|█▋        | 72.1M/440M [00:05<00:27, 13.5MB/s]Downloading:  17%|█▋        | 73.7M/440M [00:05<00:27, 13.5MB/s]Downloading:  17%|█▋        | 76.0M/440M [00:05<00:23, 15.4MB/s]Downloading:  18%|█▊        | 78.3M/440M [00:05<00:21, 17.0MB/s]Downloading:  18%|█▊        | 80.4M/440M [00:05<00:19, 18.0MB/s]Downloading:  19%|█▊        | 82.3M/440M [00:05<00:19, 17.9MB/s]Downloading:  19%|█▉        | 84.2M/440M [00:06<00:19, 18.3MB/s]Downloading:  20%|█▉        | 86.4M/440M [00:06<00:19, 18.6MB/s]Downloading:  20%|██        | 88.7M/440M [00:06<00:17, 19.7MB/s]Downloading:  21%|██        | 90.7M/440M [00:06<00:18, 18.9MB/s]Downloading:  21%|██        | 92.6M/440M [00:06<00:19, 17.4MB/s]Downloading:  21%|██▏       | 94.6M/440M [00:06<00:19, 17.9MB/s]Downloading:  22%|██▏       | 97.0M/440M [00:06<00:17, 19.3MB/s]Downloading:  22%|██▏       | 99.0M/440M [00:06<00:18, 18.8MB/s]Downloading:  23%|██▎       | 101M/440M [00:07<00:25, 13.2MB/s] Downloading:  24%|██▎       | 104M/440M [00:07<00:20, 16.1MB/s]Downloading:  24%|██▍       | 107M/440M [00:07<00:18, 18.4MB/s]Downloading:  25%|██▍       | 110M/440M [00:07<00:16, 20.5MB/s]Downloading:  26%|██▌       | 113M/440M [00:07<00:15, 21.8MB/s]Downloading:  26%|██▌       | 115M/440M [00:07<00:15, 20.8MB/s]Downloading:  27%|██▋       | 117M/440M [00:07<00:16, 20.0MB/s]Downloading:  27%|██▋       | 120M/440M [00:07<00:15, 20.6MB/s]Downloading:  28%|██▊       | 122M/440M [00:07<00:15, 20.9MB/s]Downloading:  28%|██▊       | 124M/440M [00:07<00:14, 21.6MB/s]Downloading:  29%|██▊       | 126M/440M [00:08<00:20, 15.4MB/s]Downloading:  29%|██▉       | 128M/440M [00:08<00:20, 15.4MB/s]Downloading:  29%|██▉       | 130M/440M [00:08<00:21, 14.7MB/s]Downloading:  30%|██▉       | 131M/440M [00:08<00:22, 13.8MB/s]Downloading:  30%|███       | 134M/440M [00:08<00:19, 15.4MB/s]Downloading:  31%|███       | 135M/440M [00:08<00:25, 12.1MB/s]Downloading:  31%|███       | 137M/440M [00:09<00:29, 10.4MB/s]Downloading:  31%|███▏      | 138M/440M [00:09<00:30, 10.0MB/s]Downloading:  32%|███▏      | 140M/440M [00:09<00:25, 11.6MB/s]Downloading:  32%|███▏      | 142M/440M [00:09<00:23, 12.8MB/s]Downloading:  32%|███▏      | 143M/440M [00:09<00:22, 12.9MB/s]Downloading:  33%|███▎      | 145M/440M [00:09<00:20, 14.2MB/s]Downloading:  33%|███▎      | 147M/440M [00:09<00:19, 15.4MB/s]Downloading:  34%|███▍      | 149M/440M [00:09<00:17, 16.4MB/s]Downloading:  34%|███▍      | 151M/440M [00:10<00:21, 13.3MB/s]Downloading:  35%|███▍      | 152M/440M [00:10<00:21, 13.3MB/s]Downloading:  35%|███▌      | 154M/440M [00:10<00:19, 15.0MB/s]Downloading:  36%|███▌      | 157M/440M [00:10<00:17, 16.5MB/s]Downloading:  36%|███▌      | 159M/440M [00:10<00:15, 17.6MB/s]Downloading:  37%|███▋      | 161M/440M [00:10<00:15, 18.4MB/s]Downloading:  37%|███▋      | 163M/440M [00:10<00:15, 18.4MB/s]Downloading:  37%|███▋      | 165M/440M [00:10<00:14, 18.8MB/s]Downloading:  38%|███▊      | 167M/440M [00:10<00:13, 20.1MB/s]Downloading:  38%|███▊      | 169M/440M [00:11<00:18, 14.6MB/s]Downloading:  39%|███▉      | 172M/440M [00:11<00:15, 16.8MB/s]Downloading:  39%|███▉      | 174M/440M [00:11<00:15, 17.6MB/s]Downloading:  40%|███▉      | 176M/440M [00:11<00:14, 18.5MB/s]Downloading:  40%|████      | 178M/440M [00:11<00:13, 19.6MB/s]Downloading:  41%|████      | 181M/440M [00:11<00:12, 20.5MB/s]Downloading:  42%|████▏     | 183M/440M [00:11<00:12, 21.2MB/s]Downloading:  42%|████▏     | 185M/440M [00:11<00:14, 17.2MB/s]Downloading:  42%|████▏     | 187M/440M [00:12<00:18, 13.4MB/s]Downloading:  43%|████▎     | 189M/440M [00:12<00:21, 11.5MB/s]Downloading:  43%|████▎     | 190M/440M [00:12<00:20, 12.4MB/s]Downloading:  44%|████▎     | 192M/440M [00:12<00:19, 12.5MB/s]Downloading:  44%|████▍     | 194M/440M [00:12<00:18, 13.7MB/s]Downloading:  44%|████▍     | 196M/440M [00:12<00:16, 15.2MB/s]Downloading:  45%|████▍     | 198M/440M [00:12<00:14, 16.3MB/s]Downloading:  45%|████▌     | 200M/440M [00:12<00:13, 17.6MB/s]Downloading:  46%|████▌     | 202M/440M [00:13<00:19, 12.4MB/s]Downloading:  47%|████▋     | 205M/440M [00:13<00:15, 15.3MB/s]Downloading:  47%|████▋     | 207M/440M [00:13<00:14, 16.5MB/s]Downloading:  48%|████▊     | 209M/440M [00:13<00:12, 18.5MB/s]Downloading:  48%|████▊     | 212M/440M [00:13<00:11, 20.3MB/s]Downloading:  49%|████▉     | 215M/440M [00:13<00:10, 21.8MB/s]Downloading:  49%|████▉     | 217M/440M [00:13<00:09, 22.4MB/s]Downloading:  50%|████▉     | 220M/440M [00:13<00:09, 22.8MB/s]Downloading:  50%|█████     | 222M/440M [00:14<00:10, 20.0MB/s]Downloading:  51%|█████     | 224M/440M [00:14<00:10, 21.1MB/s]Downloading:  51%|█████▏    | 227M/440M [00:14<00:09, 21.4MB/s]Downloading:  52%|█████▏    | 229M/440M [00:14<00:10, 21.1MB/s]Downloading:  52%|█████▏    | 231M/440M [00:14<00:09, 21.6MB/s]Downloading:  53%|█████▎    | 233M/440M [00:14<00:11, 18.2MB/s]Downloading:  53%|█████▎    | 235M/440M [00:14<00:14, 14.3MB/s]Downloading:  54%|█████▍    | 239M/440M [00:14<00:11, 17.0MB/s]Downloading:  55%|█████▍    | 241M/440M [00:15<00:12, 16.5MB/s]Downloading:  55%|█████▌    | 243M/440M [00:15<00:11, 17.4MB/s]Downloading:  56%|█████▌    | 245M/440M [00:15<00:10, 19.2MB/s]Downloading:  56%|█████▋    | 248M/440M [00:15<00:09, 21.3MB/s]Downloading:  57%|█████▋    | 251M/440M [00:15<00:08, 23.5MB/s]Downloading:  58%|█████▊    | 254M/440M [00:15<00:07, 24.9MB/s]Downloading:  58%|█████▊    | 257M/440M [00:15<00:07, 23.3MB/s]Downloading:  59%|█████▉    | 259M/440M [00:15<00:07, 22.8MB/s]Downloading:  59%|█████▉    | 262M/440M [00:16<00:10, 16.6MB/s]Downloading:  60%|█████▉    | 264M/440M [00:16<00:13, 13.4MB/s]Downloading:  60%|██████    | 265M/440M [00:16<00:12, 13.8MB/s]Downloading:  61%|██████    | 267M/440M [00:16<00:12, 13.4MB/s]Downloading:  61%|██████    | 268M/440M [00:16<00:18, 9.46MB/s]Downloading:  61%|██████▏   | 270M/440M [00:16<00:15, 11.0MB/s]Downloading:  62%|██████▏   | 272M/440M [00:17<00:16, 10.0MB/s]Downloading:  62%|██████▏   | 273M/440M [00:17<00:14, 11.3MB/s]Downloading:  62%|██████▏   | 275M/440M [00:17<00:13, 12.3MB/s]Downloading:  63%|██████▎   | 276M/440M [00:17<00:12, 13.3MB/s]Downloading:  63%|██████▎   | 278M/440M [00:17<00:15, 10.5MB/s]Downloading:  63%|██████▎   | 279M/440M [00:17<00:14, 11.5MB/s]Downloading:  64%|██████▎   | 281M/440M [00:17<00:14, 11.1MB/s]Downloading:  64%|██████▍   | 282M/440M [00:17<00:16, 9.34MB/s]Downloading:  64%|██████▍   | 283M/440M [00:18<00:15, 9.93MB/s]Downloading:  65%|██████▍   | 284M/440M [00:18<00:14, 10.5MB/s]Downloading:  65%|██████▍   | 286M/440M [00:18<00:14, 10.9MB/s]Downloading:  65%|██████▌   | 288M/440M [00:18<00:11, 13.0MB/s]Downloading:  66%|██████▌   | 290M/440M [00:18<00:11, 12.9MB/s]Downloading:  66%|██████▌   | 291M/440M [00:18<00:10, 14.1MB/s]Downloading:  67%|██████▋   | 293M/440M [00:18<00:09, 15.4MB/s]Downloading:  67%|██████▋   | 296M/440M [00:18<00:08, 17.2MB/s]Downloading:  68%|██████▊   | 298M/440M [00:18<00:07, 18.2MB/s]Downloading:  68%|██████▊   | 300M/440M [00:18<00:07, 18.4MB/s]Downloading:  68%|██████▊   | 302M/440M [00:19<00:10, 13.3MB/s]Downloading:  69%|██████▉   | 305M/440M [00:19<00:08, 16.2MB/s]Downloading:  70%|██████▉   | 307M/440M [00:19<00:08, 14.9MB/s]Downloading:  70%|███████   | 309M/440M [00:19<00:09, 13.9MB/s]Downloading:  71%|███████   | 311M/440M [00:19<00:10, 12.7MB/s]Downloading:  71%|███████   | 312M/440M [00:19<00:10, 12.3MB/s]Downloading:  71%|███████   | 313M/440M [00:20<00:10, 11.7MB/s]Downloading:  72%|███████▏  | 315M/440M [00:20<00:09, 12.9MB/s]Downloading:  72%|███████▏  | 317M/440M [00:20<00:09, 13.4MB/s]Downloading:  72%|███████▏  | 319M/440M [00:20<00:08, 14.8MB/s]Downloading:  73%|███████▎  | 321M/440M [00:20<00:07, 15.7MB/s]Downloading:  73%|███████▎  | 322M/440M [00:20<00:07, 15.7MB/s]Downloading:  74%|███████▎  | 324M/440M [00:20<00:10, 11.6MB/s]Downloading:  74%|███████▍  | 326M/440M [00:20<00:09, 12.3MB/s]Downloading:  74%|███████▍  | 327M/440M [00:21<00:09, 12.0MB/s]Downloading:  75%|███████▍  | 329M/440M [00:21<00:08, 13.3MB/s]Downloading:  75%|███████▌  | 331M/440M [00:21<00:08, 13.1MB/s]Downloading:  75%|███████▌  | 332M/440M [00:21<00:10, 10.3MB/s]Downloading:  76%|███████▌  | 333M/440M [00:21<00:12, 8.87MB/s]Downloading:  76%|███████▌  | 334M/440M [00:21<00:12, 8.50MB/s]Downloading:  76%|███████▌  | 335M/440M [00:22<00:17, 6.02MB/s]Downloading:  77%|███████▋  | 338M/440M [00:22<00:12, 7.99MB/s]Downloading:  77%|███████▋  | 340M/440M [00:22<00:10, 9.44MB/s]Downloading:  78%|███████▊  | 342M/440M [00:22<00:09, 10.7MB/s]Downloading:  78%|███████▊  | 344M/440M [00:22<00:07, 12.5MB/s]Downloading:  79%|███████▊  | 346M/440M [00:22<00:08, 11.4MB/s]Downloading:  79%|███████▉  | 347M/440M [00:23<00:14, 6.35MB/s]Downloading:  79%|███████▉  | 350M/440M [00:23<00:11, 8.19MB/s]Downloading:  80%|███████▉  | 352M/440M [00:23<00:08, 10.3MB/s]Downloading:  80%|████████  | 354M/440M [00:23<00:07, 11.8MB/s]Downloading:  81%|████████  | 356M/440M [00:23<00:07, 11.1MB/s]Downloading:  81%|████████  | 358M/440M [00:23<00:07, 11.7MB/s]Downloading:  82%|████████▏ | 360M/440M [00:23<00:05, 13.4MB/s]Downloading:  82%|████████▏ | 362M/440M [00:24<00:05, 13.3MB/s]Downloading:  82%|████████▏ | 363M/440M [00:24<00:06, 12.4MB/s]Downloading:  83%|████████▎ | 365M/440M [00:24<00:08, 9.16MB/s]Downloading:  83%|████████▎ | 366M/440M [00:24<00:07, 9.74MB/s]Downloading:  83%|████████▎ | 367M/440M [00:24<00:07, 10.2MB/s]Downloading:  84%|████████▎ | 368M/440M [00:24<00:08, 8.14MB/s]Downloading:  84%|████████▍ | 372M/440M [00:24<00:06, 10.6MB/s]Downloading:  85%|████████▍ | 373M/440M [00:25<00:05, 11.4MB/s]Downloading:  85%|████████▌ | 375M/440M [00:25<00:05, 12.0MB/s]Downloading:  86%|████████▌ | 377M/440M [00:25<00:05, 12.1MB/s]Downloading:  86%|████████▌ | 378M/440M [00:25<00:06, 9.40MB/s]Downloading:  86%|████████▋ | 380M/440M [00:25<00:05, 11.0MB/s]Downloading:  87%|████████▋ | 382M/440M [00:25<00:04, 12.4MB/s]Downloading:  87%|████████▋ | 383M/440M [00:25<00:04, 12.2MB/s]Downloading:  87%|████████▋ | 385M/440M [00:26<00:06, 8.47MB/s]Downloading:  88%|████████▊ | 386M/440M [00:26<00:06, 8.67MB/s]Downloading:  88%|████████▊ | 388M/440M [00:26<00:04, 10.6MB/s]Downloading:  88%|████████▊ | 390M/440M [00:26<00:04, 11.2MB/s]Downloading:  89%|████████▊ | 391M/440M [00:26<00:04, 11.1MB/s]Downloading:  89%|████████▉ | 392M/440M [00:26<00:05, 8.83MB/s]Downloading:  89%|████████▉ | 393M/440M [00:26<00:05, 8.71MB/s]Downloading:  90%|████████▉ | 394M/440M [00:27<00:05, 7.71MB/s]Downloading:  90%|████████▉ | 395M/440M [00:27<00:06, 7.49MB/s]Downloading:  90%|█████████ | 397M/440M [00:27<00:04, 9.04MB/s]Downloading:  90%|█████████ | 398M/440M [00:27<00:04, 10.3MB/s]Downloading:  91%|█████████ | 400M/440M [00:27<00:04, 9.01MB/s]Downloading:  91%|█████████ | 401M/440M [00:27<00:04, 8.03MB/s]Downloading:  91%|█████████ | 402M/440M [00:28<00:06, 5.71MB/s]Downloading:  92%|█████████▏| 405M/440M [00:28<00:04, 7.63MB/s]Downloading:  92%|█████████▏| 407M/440M [00:28<00:03, 9.39MB/s]Downloading:  93%|█████████▎| 410M/440M [00:28<00:02, 11.5MB/s]Downloading:  93%|█████████▎| 412M/440M [00:28<00:02, 11.5MB/s]Downloading:  94%|█████████▍| 413M/440M [00:28<00:02, 9.56MB/s]Downloading:  94%|█████████▍| 415M/440M [00:28<00:02, 10.4MB/s]Downloading:  94%|█████████▍| 416M/440M [00:29<00:02, 10.7MB/s]Downloading:  95%|█████████▍| 417M/440M [00:29<00:02, 8.64MB/s]Downloading:  95%|█████████▌| 419M/440M [00:29<00:03, 7.26MB/s]Downloading:  95%|█████████▌| 420M/440M [00:29<00:02, 8.86MB/s]Downloading:  96%|█████████▌| 422M/440M [00:29<00:02, 9.45MB/s]Downloading:  96%|█████████▌| 424M/440M [00:29<00:01, 11.2MB/s]Downloading:  96%|█████████▋| 425M/440M [00:29<00:01, 10.5MB/s]Downloading:  97%|█████████▋| 427M/440M [00:29<00:01, 11.7MB/s]Downloading:  97%|█████████▋| 428M/440M [00:30<00:00, 12.9MB/s]Downloading:  98%|█████████▊| 430M/440M [00:30<00:00, 10.8MB/s]Downloading:  98%|█████████▊| 431M/440M [00:30<00:00, 11.9MB/s]Downloading:  98%|█████████▊| 433M/440M [00:30<00:00, 11.3MB/s]Downloading:  99%|█████████▊| 434M/440M [00:30<00:00, 9.33MB/s]Downloading:  99%|█████████▉| 435M/440M [00:30<00:00, 6.78MB/s]Downloading:  99%|█████████▉| 437M/440M [00:31<00:00, 8.05MB/s]Downloading:  99%|█████████▉| 438M/440M [00:31<00:00, 8.76MB/s]Downloading: 100%|█████████▉| 439M/440M [00:31<00:00, 6.83MB/s]Downloading: 100%|█████████▉| 440M/440M [00:31<00:00, 7.72MB/s]Downloading: 100%|██████████| 440M/440M [00:31<00:00, 13.9MB/s]
I0511 02:47:27.818022 140171865458432 file_utils.py:448] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:47:27.818820 140171865458432 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:47:27.819377 140171865458432 filelock.py:318] Lock 140170734006960 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:47:27.819562 140171865458432 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
05/11/2020 02:47:34 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=1, bias=True)
  )
)

05/11/2020 02:47:34 Total number of params: 109483009
05/11/2020 02:47:34 At epoch 0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
05/11/2020 02:47:35 Task [ 0] updates[     1] train loss[7.20858] remaining[0:03:30]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
predicting 0
05/11/2020 02:48:45 Task assin-ptpt-sts -- epoch 0 -- Dev Pearson: 79.301
05/11/2020 02:48:45 Task assin-ptpt-sts -- epoch 0 -- Dev MSE: 10.606
predicting 0
predicting 100
predicting 200
05/11/2020 02:48:51 [new test scores saved.]
I0511 02:48:52.763420 140171865458432 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-sts/bert_base/seed/2018/model_0.pt
05/11/2020 02:48:52 At epoch 1
05/11/2020 02:49:32 Task [ 0] updates[   500] train loss[1.21227] remaining[0:00:26]
predicting 0
05/11/2020 02:50:00 Task assin-ptpt-sts -- epoch 1 -- Dev Pearson: 81.836
05/11/2020 02:50:00 Task assin-ptpt-sts -- epoch 1 -- Dev MSE: 10.606
predicting 0
predicting 100
predicting 200
05/11/2020 02:50:06 [new test scores saved.]
I0511 02:50:08.186539 140171865458432 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-sts/bert_base/seed/2018/model_1.pt
05/11/2020 02:50:08 At epoch 2
predicting 0
05/11/2020 02:51:16 Task assin-ptpt-sts -- epoch 2 -- Dev Pearson: 81.878
05/11/2020 02:51:16 Task assin-ptpt-sts -- epoch 2 -- Dev MSE: 10.606
predicting 0
predicting 100
predicting 200
05/11/2020 02:51:22 [new test scores saved.]
I0511 02:51:24.182249 140171865458432 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-sts/bert_base/seed/2018/model_2.pt
05/11/2020 02:51:24 At epoch 3
05/11/2020 02:51:37 Task [ 0] updates[  1000] train loss[0.69459] remaining[0:00:53]
predicting 0
05/11/2020 02:52:28 Task assin-ptpt-sts -- epoch 3 -- Dev Pearson: 82.766
05/11/2020 02:52:28 Task assin-ptpt-sts -- epoch 3 -- Dev MSE: 10.606
predicting 0
predicting 100
predicting 200
05/11/2020 02:52:34 [new test scores saved.]
I0511 02:52:36.098777 140171865458432 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-sts/bert_base/seed/2018/model_3.pt
05/11/2020 02:52:36 At epoch 4
05/11/2020 02:53:22 Task [ 0] updates[  1500] train loss[0.49972] remaining[0:00:12]
predicting 0
05/11/2020 02:53:35 Task assin-ptpt-sts -- epoch 4 -- Dev Pearson: 83.015
05/11/2020 02:53:35 Task assin-ptpt-sts -- epoch 4 -- Dev MSE: 10.606
predicting 0
predicting 100
predicting 200
05/11/2020 02:53:42 [new test scores saved.]
I0511 02:53:43.596672 140171865458432 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin-ptpt-sts/bert_base/seed/2018/model_4.pt
Preparing train arguments
I0511 02:53:46.642281 139949035525888 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
I0511 02:53:47.576202 139949035525888 filelock.py:274] Lock 139948740154312 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:53:47.576931 139949035525888 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpth5f66k3
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/232k [00:00<00:27, 8.47kB/s]Downloading:  15%|█▌        | 34.8k/232k [00:00<00:16, 11.9kB/s]Downloading:  38%|███▊      | 87.0k/232k [00:00<00:08, 16.8kB/s]Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 23.9kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 472kB/s] 
I0511 02:53:48.588455 139949035525888 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:53:48.588681 139949035525888 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 02:53:48.589471 139949035525888 filelock.py:318] Lock 139948740154312 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 02:53:48.589586 139949035525888 tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
05/11/2020 02:53:48 Task assin2-sts
05/11/2020 02:53:48 ../data/input/en/bert_base_uncased_lower/assin2-sts_train.json
05/11/2020 02:53:54 ../data/input/en/bert_base_uncased_lower/assin2-sts_dev.json
05/11/2020 02:53:54 ../data/input/en/bert_base_uncased_lower/assin2-sts_test.json
I0511 02:53:58.190482 139894713902848 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, bert_model_type='bert-base-uncased', cuda=True, data_dir='../data/input/en/bert_base_uncased_lower', data_sort_on=False, do_lower_case=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, embedding_opt=0, encode_mode=False, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=True, fp16_opt_level='O2', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='bert-base-uncased', init_ratio=1, learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, masked_lm_prob=0.15, max_answer_len=5, max_predictions_per_seq=128, max_seq_len=512, mem_cum_type='simple', mix_opt=0, mkd_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', num_hidden_layers=-1, optimizer='adamax', output_dir='../output/st-dnn/assin2-sts/bert_base/seed/2018', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, short_seq_prob=0.2, task_def='../data/task-def/assin2-sts.yaml', tensorboard=True, tensorboard_logdir='tensorboard_logdir', test_datasets=['assin2-sts'], train_datasets=['assin2-sts'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)
05/11/2020 02:53:59 0
05/11/2020 02:53:59 Launching the MT-DNN training
05/11/2020 02:53:59 Loading ../data/input/en/bert_base_uncased_lower/assin2-sts_train.json as task 0
Loaded 6500 samples out of 6500
Loaded 500 samples out of 500
Loaded 2448 samples out of 2448
05/11/2020 02:53:59 ####################
05/11/2020 02:53:59 {'log_file': 'mt-dnn-train.log', 'tensorboard': True, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': '../data/input/en/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../data/task-def/assin2-sts.yaml', 'train_datasets': ['assin2-sts'], 'test_datasets': ['assin2-sts'], 'glue_format_on': False, 'mkd_opt': 0, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 5, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 0, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': '../output/st-dnn/assin2-sts/bert_base/seed/2018', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': True, 'fp16_opt_level': 'O2', 'encode_mode': False, 'task_def_list': [{'kd_loss': '<LossCriterion.MseCriterion: 1>', 'loss': '<LossCriterion.MseCriterion: 1>', 'dropout_p': 'None', 'enable_san': 'False', 'split_names': "['train', 'dev', 'test']", 'metric_meta': '(<Metric.Pearson: 3>, <Metric.MSE: 11>)', 'task_type': '<TaskType.Regression: 2>', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'n_class': '1', 'label_vocab': 'None', 'self': '{}', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
05/11/2020 02:53:59 ####################
05/11/2020 02:53:59 ############# Gradient Accumulation Info #############
05/11/2020 02:53:59 number of step: 4065
05/11/2020 02:53:59 number of grad grad_accumulation step: 1
05/11/2020 02:53:59 adjusted number of step: 4065
05/11/2020 02:53:59 ############# Gradient Accumulation Info #############
I0511 02:53:59.944738 139894713902848 filelock.py:274] Lock 139894380596304 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:53:59.945386 139894713902848 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpaq9dyxzv
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 272kB/s]
I0511 02:54:00.530550 139894713902848 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:54:00.530755 139894713902848 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:54:00.531559 139894713902848 filelock.py:318] Lock 139894380596304 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 02:54:00.531826 139894713902848 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 02:54:00.532417 139894713902848 configuration_utils.py:321] Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

I0511 02:54:00.533976 139894713902848 configuration_utils.py:321] Model config BertConfig {
  "adam_eps": 1e-06,
  "answer_att_hidden_size": 128,
  "answer_att_type": "bilinear",
  "answer_dropout_p": 0.1,
  "answer_mem_drop_p": 0.1,
  "answer_mem_type": 1,
  "answer_merge_opt": 1,
  "answer_num_turn": 5,
  "answer_opt": 0,
  "answer_rnn_type": "gru",
  "answer_sum_att_type": "bilinear",
  "answer_weight_norm_on": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "batch_size": 8,
  "batch_size_eval": 8,
  "bert_dropout_p": 0.1,
  "bert_l2norm": 0.0,
  "bert_model_type": "bert-base-uncased",
  "cuda": true,
  "data_dir": "../data/input/en/bert_base_uncased_lower",
  "data_sort_on": false,
  "do_lower_case": false,
  "dropout_p": 0.1,
  "dropout_w": 0.0,
  "dump_state_on": false,
  "embedding_opt": 0,
  "encode_mode": false,
  "encoder_type": 1,
  "epochs": 5,
  "fp16": true,
  "fp16_opt_level": "O2",
  "freeze_layers": -1,
  "global_grad_clipping": 1.0,
  "glue_format_on": false,
  "grad_accumulation_step": 1,
  "grad_clipping": 0,
  "have_lr_scheduler": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "init_checkpoint": "bert-base-uncased",
  "init_ratio": 1,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 5e-05,
  "log_file": "mt-dnn-train.log",
  "log_per_updates": 500,
  "lr_gamma": 0.5,
  "masked_lm_prob": 0.15,
  "max_answer_len": 5,
  "max_position_embeddings": 512,
  "max_predictions_per_seq": 128,
  "max_seq_len": 512,
  "mem_cum_type": "simple",
  "mix_opt": 0,
  "mkd_opt": 0,
  "model_ckpt": "checkpoints/model_0.pt",
  "model_type": "bert",
  "momentum": 0,
  "mtl_opt": 0,
  "multi_gpu_on": false,
  "multi_step_lr": "10,20,30",
  "name": "farmer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "optimizer": "adamax",
  "output_dir": "../output/st-dnn/assin2-sts/bert_base/seed/2018",
  "pad_token_id": 0,
  "ratio": 0,
  "resume": false,
  "save_per_updates": 10000,
  "save_per_updates_on": false,
  "scheduler_type": "ms",
  "seed": 2018,
  "short_seq_prob": 0.2,
  "task_def": "../data/task-def/assin2-sts.yaml",
  "task_def_list": [
    {
      "__class__": "<class 'experiments.exp_def.TaskDef'>",
      "data_type": "<DataFormat.PremiseAndOneHypothesis: 2>",
      "dropout_p": "None",
      "enable_san": "False",
      "kd_loss": "<LossCriterion.MseCriterion: 1>",
      "label_vocab": "None",
      "loss": "<LossCriterion.MseCriterion: 1>",
      "metric_meta": "(<Metric.Pearson: 3>, <Metric.MSE: 11>)",
      "n_class": "1",
      "self": "{}",
      "split_names": "['train', 'dev', 'test']",
      "task_type": "<TaskType.Regression: 2>"
    }
  ],
  "tensorboard": true,
  "tensorboard_logdir": "tensorboard_logdir",
  "test_datasets": [
    "assin2-sts"
  ],
  "train_datasets": [
    "assin2-sts"
  ],
  "type_vocab_size": 2,
  "update_bert_opt": 0,
  "vb_dropout": true,
  "vocab_size": 30522,
  "warmup": 0.1,
  "warmup_schedule": "warmup_linear",
  "weight_decay": 0
}

I0511 02:54:04.827622 139894713902848 filelock.py:274] Lock 139893531412296 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:54:04.828381 139894713902848 file_utils.py:444] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp02cyjie4
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 33.8k/440M [00:00<25:15, 291kB/s]Downloading:   0%|          | 98.3k/440M [00:00<21:14, 345kB/s]Downloading:   0%|          | 242k/440M [00:00<16:31, 444kB/s] Downloading:   0%|          | 524k/440M [00:00<12:21, 593kB/s]Downloading:   0%|          | 1.06M/440M [00:00<09:02, 809kB/s]Downloading:   0%|          | 2.19M/440M [00:00<06:30, 1.12MB/s]Downloading:   1%|          | 4.45M/440M [00:00<04:37, 1.57MB/s]Downloading:   2%|▏         | 7.58M/440M [00:00<03:17, 2.19MB/s]Downloading:   2%|▏         | 10.7M/440M [00:00<02:21, 3.04MB/s]Downloading:   3%|▎         | 13.6M/440M [00:01<01:42, 4.16MB/s]Downloading:   4%|▎         | 15.9M/440M [00:01<01:17, 5.48MB/s]Downloading:   4%|▍         | 18.2M/440M [00:01<01:00, 7.02MB/s]Downloading:   5%|▍         | 20.4M/440M [00:01<00:47, 8.79MB/s]Downloading:   5%|▌         | 22.5M/440M [00:01<00:39, 10.5MB/s]Downloading:   6%|▌         | 24.7M/440M [00:01<00:33, 12.4MB/s]Downloading:   6%|▌         | 26.8M/440M [00:01<00:29, 14.0MB/s]Downloading:   7%|▋         | 28.9M/440M [00:01<00:27, 15.1MB/s]Downloading:   7%|▋         | 31.0M/440M [00:01<00:24, 16.6MB/s]Downloading:   8%|▊         | 33.1M/440M [00:01<00:23, 17.6MB/s]Downloading:   8%|▊         | 35.2M/440M [00:02<00:22, 18.0MB/s]Downloading:   9%|▊         | 37.5M/440M [00:02<00:20, 19.2MB/s]Downloading:   9%|▉         | 39.6M/440M [00:02<00:20, 19.7MB/s]Downloading:   9%|▉         | 41.6M/440M [00:02<00:21, 18.8MB/s]Downloading:  10%|▉         | 43.8M/440M [00:02<00:20, 19.4MB/s]Downloading:  10%|█         | 45.8M/440M [00:02<00:20, 19.3MB/s]Downloading:  11%|█         | 47.8M/440M [00:02<00:20, 19.6MB/s]Downloading:  11%|█▏        | 49.9M/440M [00:02<00:19, 20.0MB/s]Downloading:  12%|█▏        | 51.9M/440M [00:02<00:23, 16.7MB/s]Downloading:  12%|█▏        | 53.7M/440M [00:03<00:27, 14.3MB/s]Downloading:  13%|█▎        | 55.3M/440M [00:03<00:27, 14.0MB/s]Downloading:  13%|█▎        | 56.8M/440M [00:03<00:29, 13.1MB/s]Downloading:  13%|█▎        | 59.2M/440M [00:03<00:25, 15.2MB/s]Downloading:  14%|█▍        | 61.6M/440M [00:03<00:22, 17.0MB/s]Downloading:  14%|█▍        | 63.5M/440M [00:03<00:22, 16.4MB/s]Downloading:  15%|█▍        | 65.3M/440M [00:03<00:25, 14.9MB/s]Downloading:  15%|█▌        | 66.9M/440M [00:04<00:26, 14.3MB/s]Downloading:  16%|█▌        | 68.4M/440M [00:04<00:27, 13.6MB/s]Downloading:  16%|█▌        | 70.8M/440M [00:04<00:23, 15.6MB/s]Downloading:  16%|█▋        | 72.6M/440M [00:04<00:22, 16.2MB/s]Downloading:  17%|█▋        | 74.3M/440M [00:04<00:23, 15.5MB/s]Downloading:  17%|█▋        | 76.1M/440M [00:04<00:22, 15.9MB/s]Downloading:  18%|█▊        | 77.8M/440M [00:04<00:24, 14.6MB/s]Downloading:  18%|█▊        | 79.3M/440M [00:04<00:26, 13.7MB/s]Downloading:  19%|█▊        | 81.5M/440M [00:04<00:23, 15.1MB/s]Downloading:  19%|█▉        | 83.1M/440M [00:05<00:24, 14.4MB/s]Downloading:  19%|█▉        | 84.6M/440M [00:05<00:25, 14.1MB/s]Downloading:  20%|█▉        | 86.1M/440M [00:05<00:24, 14.2MB/s]Downloading:  20%|█▉        | 87.6M/440M [00:05<00:25, 14.0MB/s]Downloading:  20%|██        | 89.7M/440M [00:05<00:22, 15.5MB/s]Downloading:  21%|██        | 91.3M/440M [00:05<00:23, 14.9MB/s]Downloading:  21%|██        | 93.3M/440M [00:05<00:21, 16.1MB/s]Downloading:  22%|██▏       | 95.1M/440M [00:05<00:20, 16.7MB/s]Downloading:  22%|██▏       | 96.8M/440M [00:05<00:20, 16.9MB/s]Downloading:  22%|██▏       | 98.9M/440M [00:06<00:19, 17.9MB/s]Downloading:  23%|██▎       | 101M/440M [00:06<00:25, 13.1MB/s] Downloading:  24%|██▎       | 104M/440M [00:06<00:20, 16.2MB/s]Downloading:  24%|██▍       | 107M/440M [00:06<00:18, 18.3MB/s]Downloading:  25%|██▍       | 109M/440M [00:06<00:18, 18.1MB/s]Downloading:  25%|██▌       | 112M/440M [00:06<00:16, 20.3MB/s]Downloading:  26%|██▌       | 115M/440M [00:06<00:14, 22.2MB/s]Downloading:  27%|██▋       | 117M/440M [00:06<00:14, 21.8MB/s]Downloading:  27%|██▋       | 120M/440M [00:06<00:15, 21.1MB/s]Downloading:  28%|██▊       | 122M/440M [00:07<00:14, 21.3MB/s]Downloading:  28%|██▊       | 124M/440M [00:07<00:15, 20.1MB/s]Downloading:  29%|██▊       | 126M/440M [00:07<00:15, 20.2MB/s]Downloading:  29%|██▉       | 128M/440M [00:07<00:15, 19.8MB/s]Downloading:  30%|██▉       | 130M/440M [00:07<00:18, 17.2MB/s]Downloading:  30%|███       | 132M/440M [00:07<00:21, 14.2MB/s]Downloading:  30%|███       | 134M/440M [00:07<00:24, 12.3MB/s]Downloading:  31%|███       | 135M/440M [00:08<00:30, 10.1MB/s]Downloading:  31%|███▏      | 138M/440M [00:08<00:24, 12.6MB/s]Downloading:  32%|███▏      | 140M/440M [00:08<00:21, 14.1MB/s]Downloading:  32%|███▏      | 142M/440M [00:08<00:19, 15.7MB/s]Downloading:  33%|███▎      | 144M/440M [00:08<00:17, 16.7MB/s]Downloading:  33%|███▎      | 146M/440M [00:08<00:16, 17.8MB/s]Downloading:  34%|███▎      | 148M/440M [00:08<00:15, 18.9MB/s]Downloading:  34%|███▍      | 150M/440M [00:08<00:15, 19.2MB/s]Downloading:  35%|███▍      | 153M/440M [00:08<00:15, 18.1MB/s]Downloading:  35%|███▌      | 155M/440M [00:09<00:15, 18.6MB/s]Downloading:  36%|███▌      | 157M/440M [00:09<00:15, 18.5MB/s]Downloading:  36%|███▌      | 159M/440M [00:09<00:14, 19.6MB/s]Downloading:  37%|███▋      | 161M/440M [00:09<00:13, 20.6MB/s]Downloading:  37%|███▋      | 164M/440M [00:09<00:12, 21.4MB/s]Downloading:  38%|███▊      | 166M/440M [00:09<00:13, 20.9MB/s]Downloading:  38%|███▊      | 168M/440M [00:09<00:19, 13.9MB/s]Downloading:  39%|███▉      | 171M/440M [00:09<00:15, 16.9MB/s]Downloading:  39%|███▉      | 173M/440M [00:10<00:15, 16.9MB/s]Downloading:  40%|███▉      | 176M/440M [00:10<00:15, 16.7MB/s]Downloading:  40%|████      | 178M/440M [00:10<00:14, 17.6MB/s]Downloading:  41%|████      | 180M/440M [00:10<00:14, 18.3MB/s]Downloading:  41%|████      | 182M/440M [00:10<00:13, 18.8MB/s]Downloading:  42%|████▏     | 184M/440M [00:10<00:13, 18.6MB/s]Downloading:  42%|████▏     | 186M/440M [00:10<00:12, 20.0MB/s]Downloading:  43%|████▎     | 189M/440M [00:10<00:11, 21.3MB/s]Downloading:  43%|████▎     | 192M/440M [00:10<00:10, 23.4MB/s]Downloading:  44%|████▍     | 194M/440M [00:11<00:09, 24.8MB/s]Downloading:  45%|████▍     | 197M/440M [00:11<00:09, 26.1MB/s]Downloading:  45%|████▌     | 200M/440M [00:11<00:09, 25.7MB/s]Downloading:  46%|████▌     | 203M/440M [00:11<00:13, 17.7MB/s]Downloading:  47%|████▋     | 205M/440M [00:11<00:13, 17.6MB/s]Downloading:  47%|████▋     | 207M/440M [00:11<00:15, 14.9MB/s]Downloading:  47%|████▋     | 209M/440M [00:11<00:14, 16.0MB/s]Downloading:  48%|████▊     | 211M/440M [00:12<00:15, 15.0MB/s]Downloading:  48%|████▊     | 212M/440M [00:12<00:17, 13.1MB/s]Downloading:  49%|████▊     | 214M/440M [00:12<00:16, 13.8MB/s]Downloading:  49%|████▉     | 215M/440M [00:12<00:16, 14.0MB/s]Downloading:  49%|████▉     | 218M/440M [00:12<00:14, 15.7MB/s]Downloading:  50%|████▉     | 219M/440M [00:12<00:14, 15.5MB/s]Downloading:  50%|█████     | 221M/440M [00:12<00:13, 16.7MB/s]Downloading:  51%|█████     | 224M/440M [00:12<00:11, 18.5MB/s]Downloading:  51%|█████▏    | 226M/440M [00:12<00:11, 18.7MB/s]Downloading:  52%|█████▏    | 228M/440M [00:13<00:10, 19.4MB/s]Downloading:  52%|█████▏    | 230M/440M [00:13<00:10, 19.6MB/s]Downloading:  53%|█████▎    | 232M/440M [00:13<00:11, 18.5MB/s]Downloading:  53%|█████▎    | 234M/440M [00:13<00:11, 17.9MB/s]Downloading:  54%|█████▎    | 236M/440M [00:13<00:15, 13.1MB/s]Downloading:  54%|█████▍    | 239M/440M [00:13<00:12, 15.8MB/s]Downloading:  55%|█████▍    | 241M/440M [00:13<00:12, 16.0MB/s]Downloading:  55%|█████▌    | 243M/440M [00:13<00:11, 17.0MB/s]Downloading:  56%|█████▌    | 245M/440M [00:14<00:10, 18.1MB/s]Downloading:  56%|█████▌    | 247M/440M [00:14<00:10, 18.9MB/s]Downloading:  57%|█████▋    | 249M/440M [00:14<00:09, 19.5MB/s]Downloading:  57%|█████▋    | 251M/440M [00:14<00:09, 20.0MB/s]Downloading:  58%|█████▊    | 253M/440M [00:14<00:11, 17.0MB/s]Downloading:  58%|█████▊    | 255M/440M [00:14<00:17, 10.7MB/s]Downloading:  58%|█████▊    | 257M/440M [00:14<00:15, 11.8MB/s]Downloading:  59%|█████▊    | 258M/440M [00:15<00:14, 12.4MB/s]Downloading:  59%|█████▉    | 260M/440M [00:15<00:13, 13.8MB/s]Downloading:  60%|█████▉    | 262M/440M [00:15<00:12, 14.7MB/s]Downloading:  60%|██████    | 264M/440M [00:15<00:10, 16.5MB/s]Downloading:  60%|██████    | 266M/440M [00:15<00:12, 14.0MB/s]Downloading:  61%|██████    | 268M/440M [00:15<00:15, 11.4MB/s]Downloading:  61%|██████▏   | 271M/440M [00:15<00:12, 13.8MB/s]Downloading:  62%|██████▏   | 272M/440M [00:16<00:21, 7.68MB/s]Downloading:  62%|██████▏   | 275M/440M [00:16<00:17, 9.66MB/s]Downloading:  63%|██████▎   | 278M/440M [00:16<00:13, 12.0MB/s]Downloading:  64%|██████▎   | 281M/440M [00:16<00:10, 14.8MB/s]Downloading:  64%|██████▍   | 284M/440M [00:16<00:09, 17.3MB/s]Downloading:  65%|██████▌   | 286M/440M [00:16<00:07, 19.5MB/s]Downloading:  66%|██████▌   | 289M/440M [00:16<00:07, 19.2MB/s]Downloading:  66%|██████▌   | 291M/440M [00:17<00:07, 20.1MB/s]Downloading:  67%|██████▋   | 294M/440M [00:17<00:07, 20.5MB/s]Downloading:  67%|██████▋   | 296M/440M [00:17<00:06, 21.2MB/s]Downloading:  68%|██████▊   | 298M/440M [00:17<00:06, 21.5MB/s]Downloading:  68%|██████▊   | 301M/440M [00:17<00:06, 21.9MB/s]Downloading:  69%|██████▉   | 303M/440M [00:17<00:08, 16.0MB/s]Downloading:  70%|██████▉   | 306M/440M [00:17<00:07, 18.9MB/s]Downloading:  70%|███████   | 309M/440M [00:17<00:06, 21.1MB/s]Downloading:  71%|███████   | 312M/440M [00:17<00:06, 20.7MB/s]Downloading:  71%|███████▏  | 314M/440M [00:18<00:06, 20.1MB/s]Downloading:  72%|███████▏  | 316M/440M [00:18<00:06, 19.9MB/s]Downloading:  72%|███████▏  | 318M/440M [00:18<00:06, 19.9MB/s]Downloading:  73%|███████▎  | 320M/440M [00:18<00:06, 19.3MB/s]Downloading:  73%|███████▎  | 322M/440M [00:18<00:07, 16.4MB/s]Downloading:  74%|███████▎  | 324M/440M [00:18<00:08, 13.7MB/s]Downloading:  74%|███████▍  | 326M/440M [00:18<00:07, 14.5MB/s]Downloading:  74%|███████▍  | 327M/440M [00:19<00:08, 13.7MB/s]Downloading:  75%|███████▍  | 329M/440M [00:19<00:07, 14.6MB/s]Downloading:  75%|███████▌  | 331M/440M [00:19<00:11, 9.64MB/s]Downloading:  76%|███████▌  | 333M/440M [00:19<00:09, 11.3MB/s]Downloading:  76%|███████▌  | 335M/440M [00:19<00:10, 9.73MB/s]Downloading:  77%|███████▋  | 338M/440M [00:19<00:08, 12.5MB/s]Downloading:  77%|███████▋  | 341M/440M [00:19<00:06, 14.8MB/s]Downloading:  78%|███████▊  | 343M/440M [00:20<00:06, 15.5MB/s]Downloading:  78%|███████▊  | 345M/440M [00:20<00:05, 16.7MB/s]Downloading:  79%|███████▉  | 347M/440M [00:20<00:05, 18.0MB/s]Downloading:  79%|███████▉  | 350M/440M [00:20<00:04, 19.0MB/s]Downloading:  80%|███████▉  | 352M/440M [00:20<00:04, 18.8MB/s]Downloading:  80%|████████  | 354M/440M [00:20<00:04, 18.1MB/s]Downloading:  81%|████████  | 356M/440M [00:20<00:04, 18.9MB/s]Downloading:  81%|████████▏ | 358M/440M [00:20<00:04, 20.4MB/s]Downloading:  82%|████████▏ | 360M/440M [00:20<00:03, 20.2MB/s]Downloading:  82%|████████▏ | 363M/440M [00:21<00:04, 18.2MB/s]Downloading:  83%|████████▎ | 365M/440M [00:21<00:04, 18.8MB/s]Downloading:  83%|████████▎ | 367M/440M [00:21<00:03, 20.1MB/s]Downloading:  84%|████████▍ | 369M/440M [00:21<00:03, 20.2MB/s]Downloading:  84%|████████▍ | 371M/440M [00:21<00:03, 19.9MB/s]Downloading:  85%|████████▍ | 374M/440M [00:21<00:03, 20.6MB/s]Downloading:  85%|████████▌ | 376M/440M [00:21<00:03, 21.3MB/s]Downloading:  86%|████████▌ | 378M/440M [00:21<00:02, 21.8MB/s]Downloading:  86%|████████▋ | 381M/440M [00:21<00:02, 22.2MB/s]Downloading:  87%|████████▋ | 383M/440M [00:22<00:02, 20.6MB/s]Downloading:  87%|████████▋ | 385M/440M [00:22<00:03, 18.2MB/s]Downloading:  88%|████████▊ | 387M/440M [00:22<00:03, 16.4MB/s]Downloading:  88%|████████▊ | 389M/440M [00:22<00:03, 15.6MB/s]Downloading:  89%|████████▊ | 390M/440M [00:22<00:03, 16.1MB/s]Downloading:  89%|████████▉ | 393M/440M [00:22<00:02, 17.9MB/s]Downloading:  90%|████████▉ | 395M/440M [00:22<00:02, 16.8MB/s]Downloading:  90%|████████▉ | 396M/440M [00:22<00:02, 16.3MB/s]Downloading:  90%|█████████ | 398M/440M [00:23<00:02, 15.1MB/s]Downloading:  91%|█████████ | 400M/440M [00:23<00:03, 13.5MB/s]Downloading:  91%|█████████ | 402M/440M [00:23<00:02, 15.4MB/s]Downloading:  92%|█████████▏| 404M/440M [00:23<00:02, 12.5MB/s]Downloading:  92%|█████████▏| 405M/440M [00:23<00:03, 11.5MB/s]Downloading:  92%|█████████▏| 406M/440M [00:23<00:02, 11.6MB/s]Downloading:  93%|█████████▎| 408M/440M [00:23<00:03, 9.69MB/s]Downloading:  93%|█████████▎| 409M/440M [00:23<00:02, 10.8MB/s]Downloading:  93%|█████████▎| 411M/440M [00:24<00:02, 12.3MB/s]Downloading:  94%|█████████▎| 412M/440M [00:24<00:02, 11.7MB/s]Downloading:  94%|█████████▍| 414M/440M [00:24<00:01, 13.3MB/s]Downloading:  95%|█████████▍| 416M/440M [00:24<00:01, 14.9MB/s]Downloading:  95%|█████████▍| 418M/440M [00:24<00:01, 16.4MB/s]Downloading:  95%|█████████▌| 420M/440M [00:24<00:01, 16.6MB/s]Downloading:  96%|█████████▌| 422M/440M [00:24<00:01, 17.6MB/s]Downloading:  96%|█████████▋| 424M/440M [00:24<00:00, 17.7MB/s]Downloading:  97%|█████████▋| 426M/440M [00:24<00:00, 18.1MB/s]Downloading:  97%|█████████▋| 428M/440M [00:25<00:00, 19.3MB/s]Downloading:  98%|█████████▊| 430M/440M [00:25<00:00, 18.7MB/s]Downloading:  98%|█████████▊| 432M/440M [00:25<00:00, 19.1MB/s]Downloading:  99%|█████████▊| 434M/440M [00:25<00:00, 15.0MB/s]Downloading:  99%|█████████▉| 436M/440M [00:25<00:00, 11.8MB/s]Downloading: 100%|█████████▉| 439M/440M [00:25<00:00, 14.7MB/s]Downloading: 100%|██████████| 440M/440M [00:25<00:00, 17.1MB/s]
I0511 02:54:31.049505 139894713902848 file_utils.py:448] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:54:31.050370 139894713902848 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 02:54:31.050866 139894713902848 filelock.py:318] Lock 139893531412296 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 02:54:31.051041 139894713902848 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
05/11/2020 02:54:38 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=1, bias=True)
  )
)

05/11/2020 02:54:38 Total number of params: 109483009
05/11/2020 02:54:38 At epoch 0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
05/11/2020 02:54:38 Task [ 0] updates[     1] train loss[9.20563] remaining[0:08:19]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
05/11/2020 02:56:18 Task [ 0] updates[   500] train loss[2.59899] remaining[0:01:03]
predicting 0
05/11/2020 02:57:29 Task assin2-sts -- epoch 0 -- Dev Pearson: 85.510
05/11/2020 02:57:29 Task assin2-sts -- epoch 0 -- Dev MSE: 16.447
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 02:57:37 [new test scores saved.]
I0511 02:57:38.993118 139894713902848 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-sts/bert_base/seed/2018/model_0.pt
05/11/2020 02:57:39 At epoch 1
05/11/2020 02:58:18 Task [ 0] updates[  1000] train loss[1.47123] remaining[0:02:11]
05/11/2020 03:00:04 Task [ 0] updates[  1500] train loss[1.05831] remaining[0:00:26]
predicting 0
05/11/2020 03:00:32 Task assin2-sts -- epoch 1 -- Dev Pearson: 88.821
05/11/2020 03:00:32 Task assin2-sts -- epoch 1 -- Dev MSE: 16.447
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 03:00:39 [new test scores saved.]
I0511 03:00:41.091087 139894713902848 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-sts/bert_base/seed/2018/model_1.pt
05/11/2020 03:00:41 At epoch 2
05/11/2020 03:02:00 Task [ 0] updates[  2000] train loss[0.84486] remaining[0:01:33]
predicting 0
05/11/2020 03:03:37 Task assin2-sts -- epoch 2 -- Dev Pearson: 90.550
05/11/2020 03:03:37 Task assin2-sts -- epoch 2 -- Dev MSE: 16.447
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 03:03:45 [new test scores saved.]
I0511 03:03:46.551518 139894713902848 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-sts/bert_base/seed/2018/model_2.pt
05/11/2020 03:03:46 At epoch 3
05/11/2020 03:04:00 Task [ 0] updates[  2500] train loss[0.70776] remaining[0:02:47]
05/11/2020 03:05:46 Task [ 0] updates[  3000] train loss[0.61276] remaining[0:00:54]
predicting 0
05/11/2020 03:06:42 Task assin2-sts -- epoch 3 -- Dev Pearson: 91.516
05/11/2020 03:06:42 Task assin2-sts -- epoch 3 -- Dev MSE: 16.447
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 03:06:49 [new test scores saved.]
I0511 03:06:51.001087 139894713902848 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-sts/bert_base/seed/2018/model_3.pt
05/11/2020 03:06:51 At epoch 4
05/11/2020 03:07:44 Task [ 0] updates[  3500] train loss[0.54116] remaining[0:02:01]
05/11/2020 03:09:30 Task [ 0] updates[  4000] train loss[0.48712] remaining[0:00:13]
predicting 0
05/11/2020 03:09:47 Task assin2-sts -- epoch 4 -- Dev Pearson: 91.647
05/11/2020 03:09:47 Task assin2-sts -- epoch 4 -- Dev MSE: 16.447
predicting 0
predicting 100
predicting 200
predicting 300
05/11/2020 03:09:54 [new test scores saved.]
I0511 03:09:56.226977 139894713902848 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/assin2-sts/bert_base/seed/2018/model_4.pt
Preparing train arguments
I0511 03:09:59.242110 140428068222720 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
I0511 03:10:00.142081 140428068222720 filelock.py:274] Lock 140427733775416 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 03:10:00.142807 140428068222720 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpk5qii09g
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:   0%|          | 1.02k/232k [00:00<00:27, 8.42kB/s]Downloading:  15%|█▌        | 34.8k/232k [00:00<00:16, 11.9kB/s]Downloading:  38%|███▊      | 87.0k/232k [00:00<00:08, 16.8kB/s]Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 23.8kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 470kB/s] 
I0511 03:10:01.160568 140428068222720 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 03:10:01.160747 140428068222720 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
I0511 03:10:01.161395 140428068222720 filelock.py:318] Lock 140427733775416 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
I0511 03:10:01.161506 140428068222720 tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
05/11/2020 03:10:01 Task tweetsent
05/11/2020 03:10:01 ../data/input/en/bert_base_uncased_lower/tweetsent_train.json
05/11/2020 03:10:08 ../data/input/en/bert_base_uncased_lower/tweetsent_test.json
I0511 03:10:10.807492 139841247500032 file_utils.py:38] PyTorch version 1.1.0a0+be364ac available.
Namespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, bert_model_type='bert-base-uncased', cuda=True, data_dir='../data/input/en/bert_base_uncased_lower', data_sort_on=False, do_lower_case=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, embedding_opt=0, encode_mode=False, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=True, fp16_opt_level='O2', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='bert-base-uncased', init_ratio=1, learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, masked_lm_prob=0.15, max_answer_len=5, max_predictions_per_seq=128, max_seq_len=512, mem_cum_type='simple', mix_opt=0, mkd_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', num_hidden_layers=-1, optimizer='adamax', output_dir='../output/st-dnn/tweetsent/bert_base/seed/2018', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, short_seq_prob=0.2, task_def='../data/task-def/tweetsent.yaml', tensorboard=True, tensorboard_logdir='tensorboard_logdir', test_datasets=['tweetsent'], train_datasets=['tweetsent'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)
05/11/2020 03:10:11 0
05/11/2020 03:10:11 Launching the MT-DNN training
05/11/2020 03:10:11 Loading ../data/input/en/bert_base_uncased_lower/tweetsent_train.json as task 0
Loaded 10980 samples out of 10980
Loaded 2010 samples out of 2010
05/11/2020 03:10:12 ####################
05/11/2020 03:10:12 {'log_file': 'mt-dnn-train.log', 'tensorboard': True, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': '../data/input/en/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../data/task-def/tweetsent.yaml', 'train_datasets': ['tweetsent'], 'test_datasets': ['tweetsent'], 'glue_format_on': False, 'mkd_opt': 0, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 5, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 0, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': '../output/st-dnn/tweetsent/bert_base/seed/2018', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': True, 'fp16_opt_level': 'O2', 'encode_mode': False, 'task_def_list': [{'kd_loss': '<LossCriterion.MseCriterion: 1>', 'loss': '<LossCriterion.CeCriterion: 0>', 'dropout_p': 'None', 'enable_san': 'False', 'split_names': "['train', 'test']", 'metric_meta': '(<Metric.ACC: 0>, <Metric.NoMeanF1: 12>)', 'task_type': '<TaskType.Classification: 1>', 'data_type': '<DataFormat.PremiseOnly: 1>', 'n_class': '3', 'label_vocab': '<data_utils.vocab.Vocabulary object at 0x7f2f1a6c2dd8>', 'self': '{}', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
05/11/2020 03:10:12 ####################
05/11/2020 03:10:12 ############# Gradient Accumulation Info #############
05/11/2020 03:10:12 number of step: 6865
05/11/2020 03:10:12 number of grad grad_accumulation step: 1
05/11/2020 03:10:12 adjusted number of step: 6865
05/11/2020 03:10:12 ############# Gradient Accumulation Info #############
I0511 03:10:12.662555 139841247500032 filelock.py:274] Lock 139840287303384 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 03:10:12.663274 139841247500032 file_utils.py:444] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmppay5zyyg
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 260kB/s]
I0511 03:10:13.182303 139841247500032 file_utils.py:448] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 03:10:13.182518 139841247500032 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 03:10:13.183348 139841247500032 filelock.py:318] Lock 139840287303384 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
I0511 03:10:13.183572 139841247500032 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
I0511 03:10:13.184251 139841247500032 configuration_utils.py:321] Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

I0511 03:10:13.186084 139841247500032 configuration_utils.py:321] Model config BertConfig {
  "adam_eps": 1e-06,
  "answer_att_hidden_size": 128,
  "answer_att_type": "bilinear",
  "answer_dropout_p": 0.1,
  "answer_mem_drop_p": 0.1,
  "answer_mem_type": 1,
  "answer_merge_opt": 1,
  "answer_num_turn": 5,
  "answer_opt": 0,
  "answer_rnn_type": "gru",
  "answer_sum_att_type": "bilinear",
  "answer_weight_norm_on": false,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "batch_size": 8,
  "batch_size_eval": 8,
  "bert_dropout_p": 0.1,
  "bert_l2norm": 0.0,
  "bert_model_type": "bert-base-uncased",
  "cuda": true,
  "data_dir": "../data/input/en/bert_base_uncased_lower",
  "data_sort_on": false,
  "do_lower_case": false,
  "dropout_p": 0.1,
  "dropout_w": 0.0,
  "dump_state_on": false,
  "embedding_opt": 0,
  "encode_mode": false,
  "encoder_type": 1,
  "epochs": 5,
  "fp16": true,
  "fp16_opt_level": "O2",
  "freeze_layers": -1,
  "global_grad_clipping": 1.0,
  "glue_format_on": false,
  "grad_accumulation_step": 1,
  "grad_clipping": 0,
  "have_lr_scheduler": true,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "init_checkpoint": "bert-base-uncased",
  "init_ratio": 1,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "learning_rate": 5e-05,
  "log_file": "mt-dnn-train.log",
  "log_per_updates": 500,
  "lr_gamma": 0.5,
  "masked_lm_prob": 0.15,
  "max_answer_len": 5,
  "max_position_embeddings": 512,
  "max_predictions_per_seq": 128,
  "max_seq_len": 512,
  "mem_cum_type": "simple",
  "mix_opt": 0,
  "mkd_opt": 0,
  "model_ckpt": "checkpoints/model_0.pt",
  "model_type": "bert",
  "momentum": 0,
  "mtl_opt": 0,
  "multi_gpu_on": false,
  "multi_step_lr": "10,20,30",
  "name": "farmer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "optimizer": "adamax",
  "output_dir": "../output/st-dnn/tweetsent/bert_base/seed/2018",
  "pad_token_id": 0,
  "ratio": 0,
  "resume": false,
  "save_per_updates": 10000,
  "save_per_updates_on": false,
  "scheduler_type": "ms",
  "seed": 2018,
  "short_seq_prob": 0.2,
  "task_def": "../data/task-def/tweetsent.yaml",
  "task_def_list": [
    {
      "__class__": "<class 'experiments.exp_def.TaskDef'>",
      "data_type": "<DataFormat.PremiseOnly: 1>",
      "dropout_p": "None",
      "enable_san": "False",
      "kd_loss": "<LossCriterion.MseCriterion: 1>",
      "label_vocab": "<data_utils.vocab.Vocabulary object at 0x7f2f1a6c2dd8>",
      "loss": "<LossCriterion.CeCriterion: 0>",
      "metric_meta": "(<Metric.ACC: 0>, <Metric.NoMeanF1: 12>)",
      "n_class": "3",
      "self": "{}",
      "split_names": "['train', 'test']",
      "task_type": "<TaskType.Classification: 1>"
    }
  ],
  "tensorboard": true,
  "tensorboard_logdir": "tensorboard_logdir",
  "test_datasets": [
    "tweetsent"
  ],
  "train_datasets": [
    "tweetsent"
  ],
  "type_vocab_size": 2,
  "update_bert_opt": 0,
  "vb_dropout": true,
  "vocab_size": 30522,
  "warmup": 0.1,
  "warmup_schedule": "warmup_linear",
  "weight_decay": 0
}

I0511 03:10:17.597049 139841247500032 filelock.py:274] Lock 139839896989536 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 03:10:17.597891 139841247500032 file_utils.py:444] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp1erpolyy
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 31.7k/440M [00:00<23:40, 310kB/s]Downloading:   0%|          | 94.2k/440M [00:00<20:30, 358kB/s]Downloading:   0%|          | 213k/440M [00:00<16:16, 451kB/s] Downloading:   0%|          | 438k/440M [00:00<12:21, 593kB/s]Downloading:   0%|          | 901k/440M [00:00<09:07, 803kB/s]Downloading:   0%|          | 1.87M/440M [00:00<06:35, 1.11MB/s]Downloading:   1%|          | 3.75M/440M [00:00<04:42, 1.54MB/s]Downloading:   2%|▏         | 6.86M/440M [00:00<03:20, 2.16MB/s]Downloading:   2%|▏         | 8.40M/440M [00:00<02:33, 2.81MB/s]Downloading:   2%|▏         | 9.76M/440M [00:01<02:08, 3.35MB/s]Downloading:   3%|▎         | 11.4M/440M [00:01<01:38, 4.34MB/s]Downloading:   3%|▎         | 12.6M/440M [00:01<01:33, 4.58MB/s]Downloading:   3%|▎         | 14.5M/440M [00:01<01:11, 5.93MB/s]Downloading:   4%|▎         | 15.9M/440M [00:01<00:59, 7.17MB/s]Downloading:   4%|▍         | 17.7M/440M [00:01<00:49, 8.53MB/s]Downloading:   5%|▍         | 19.9M/440M [00:01<00:40, 10.4MB/s]Downloading:   5%|▌         | 22.2M/440M [00:02<00:33, 12.5MB/s]Downloading:   5%|▌         | 24.1M/440M [00:02<00:40, 10.2MB/s]Downloading:   6%|▌         | 26.2M/440M [00:02<00:34, 12.1MB/s]Downloading:   6%|▋         | 27.9M/440M [00:02<00:34, 12.1MB/s]Downloading:   7%|▋         | 29.4M/440M [00:02<00:34, 12.0MB/s]Downloading:   7%|▋         | 30.8M/440M [00:02<00:38, 10.7MB/s]Downloading:   7%|▋         | 33.0M/440M [00:02<00:32, 12.6MB/s]Downloading:   8%|▊         | 34.9M/440M [00:03<00:28, 14.0MB/s]Downloading:   8%|▊         | 36.5M/440M [00:03<00:30, 13.4MB/s]Downloading:   9%|▊         | 38.0M/440M [00:03<00:33, 11.9MB/s]Downloading:   9%|▉         | 39.4M/440M [00:03<00:32, 12.5MB/s]Downloading:  10%|▉         | 41.9M/440M [00:03<00:27, 14.6MB/s]Downloading:  10%|▉         | 43.7M/440M [00:03<00:26, 15.1MB/s]Downloading:  10%|█         | 45.4M/440M [00:03<00:25, 15.4MB/s]Downloading:  11%|█         | 47.0M/440M [00:03<00:26, 14.9MB/s]Downloading:  11%|█         | 49.1M/440M [00:03<00:23, 16.3MB/s]Downloading:  12%|█▏        | 50.9M/440M [00:04<00:27, 14.1MB/s]Downloading:  12%|█▏        | 52.4M/440M [00:04<00:34, 11.3MB/s]Downloading:  12%|█▏        | 53.9M/440M [00:04<00:31, 12.3MB/s]Downloading:  13%|█▎        | 56.3M/440M [00:04<00:26, 14.3MB/s]Downloading:  13%|█▎        | 58.5M/440M [00:04<00:23, 16.0MB/s]Downloading:  14%|█▎        | 60.3M/440M [00:04<00:25, 15.2MB/s]Downloading:  14%|█▍        | 62.0M/440M [00:04<00:30, 12.6MB/s]Downloading:  14%|█▍        | 63.4M/440M [00:05<00:28, 13.1MB/s]Downloading:  15%|█▍        | 64.9M/440M [00:05<00:30, 12.5MB/s]Downloading:  15%|█▌        | 66.6M/440M [00:05<00:28, 13.1MB/s]Downloading:  15%|█▌        | 68.0M/440M [00:05<00:30, 12.2MB/s]Downloading:  16%|█▌        | 69.4M/440M [00:05<00:30, 12.1MB/s]Downloading:  16%|█▌        | 70.7M/440M [00:05<00:29, 12.4MB/s]Downloading:  16%|█▋        | 72.2M/440M [00:05<00:27, 13.2MB/s]Downloading:  17%|█▋        | 73.6M/440M [00:05<00:35, 10.3MB/s]Downloading:  17%|█▋        | 74.7M/440M [00:06<00:35, 10.2MB/s]Downloading:  17%|█▋        | 75.9M/440M [00:06<00:34, 10.5MB/s]Downloading:  17%|█▋        | 77.0M/440M [00:06<00:34, 10.6MB/s]Downloading:  18%|█▊        | 78.1M/440M [00:06<00:33, 10.7MB/s]Downloading:  18%|█▊        | 79.3M/440M [00:06<00:41, 8.67MB/s]Downloading:  18%|█▊        | 80.9M/440M [00:06<00:36, 9.98MB/s]Downloading:  19%|█▊        | 82.0M/440M [00:06<00:37, 9.63MB/s]Downloading:  19%|█▉        | 83.1M/440M [00:06<00:37, 9.61MB/s]Downloading:  19%|█▉        | 84.1M/440M [00:07<00:47, 7.48MB/s]Downloading:  20%|█▉        | 86.3M/440M [00:07<00:39, 8.95MB/s]Downloading:  20%|█▉        | 87.6M/440M [00:07<00:35, 9.92MB/s]Downloading:  20%|██        | 88.8M/440M [00:07<00:34, 10.3MB/s]Downloading:  21%|██        | 90.6M/440M [00:07<00:29, 11.8MB/s]Downloading:  21%|██        | 91.9M/440M [00:07<00:29, 11.7MB/s]Downloading:  21%|██        | 93.2M/440M [00:07<00:31, 11.0MB/s]Downloading:  21%|██▏       | 94.4M/440M [00:08<00:39, 8.84MB/s]Downloading:  22%|██▏       | 95.9M/440M [00:08<00:34, 9.89MB/s]Downloading:  22%|██▏       | 97.7M/440M [00:08<00:29, 11.4MB/s]Downloading:  23%|██▎       | 99.6M/440M [00:08<00:26, 13.0MB/s]Downloading:  23%|██▎       | 101M/440M [00:10<02:43, 2.07MB/s] Downloading:  24%|██▎       | 105M/440M [00:10<01:56, 2.89MB/s]Downloading:  24%|██▍       | 107M/440M [00:10<01:24, 3.93MB/s]Downloading:  25%|██▍       | 110M/440M [00:10<01:02, 5.28MB/s]Downloading:  26%|██▌       | 113M/440M [00:10<00:46, 6.99MB/s]Downloading:  26%|██▌       | 115M/440M [00:10<00:36, 8.94MB/s]Downloading:  27%|██▋       | 118M/440M [00:11<00:28, 11.3MB/s]Downloading:  27%|██▋       | 121M/440M [00:11<00:23, 13.9MB/s]Downloading:  28%|██▊       | 124M/440M [00:11<00:19, 16.4MB/s]Downloading:  29%|██▉       | 127M/440M [00:11<00:16, 19.0MB/s]Downloading:  29%|██▉       | 130M/440M [00:11<00:14, 21.5MB/s]Downloading:  30%|███       | 133M/440M [00:11<00:13, 23.6MB/s]Downloading:  31%|███       | 136M/440M [00:11<00:22, 13.3MB/s]Downloading:  32%|███▏      | 139M/440M [00:12<00:18, 16.0MB/s]Downloading:  32%|███▏      | 142M/440M [00:12<00:15, 18.7MB/s]Downloading:  33%|███▎      | 145M/440M [00:12<00:20, 14.4MB/s]Downloading:  33%|███▎      | 147M/440M [00:12<00:20, 14.5MB/s]Downloading:  34%|███▍      | 149M/440M [00:12<00:18, 15.5MB/s]Downloading:  34%|███▍      | 151M/440M [00:12<00:16, 17.1MB/s]Downloading:  35%|███▍      | 153M/440M [00:12<00:16, 17.8MB/s]Downloading:  35%|███▌      | 155M/440M [00:13<00:15, 17.9MB/s]Downloading:  36%|███▌      | 157M/440M [00:13<00:15, 18.7MB/s]Downloading:  36%|███▌      | 159M/440M [00:13<00:14, 19.3MB/s]Downloading:  37%|███▋      | 161M/440M [00:13<00:14, 19.9MB/s]Downloading:  37%|███▋      | 164M/440M [00:13<00:13, 20.6MB/s]Downloading:  38%|███▊      | 166M/440M [00:13<00:13, 20.1MB/s]Downloading:  38%|███▊      | 168M/440M [00:13<00:19, 14.2MB/s]Downloading:  39%|███▉      | 171M/440M [00:13<00:15, 17.3MB/s]Downloading:  39%|███▉      | 174M/440M [00:14<00:15, 17.5MB/s]Downloading:  40%|███▉      | 176M/440M [00:14<00:14, 18.0MB/s]Downloading:  40%|████      | 178M/440M [00:14<00:13, 19.1MB/s]Downloading:  41%|████      | 180M/440M [00:14<00:14, 18.1MB/s]Downloading:  41%|████▏     | 182M/440M [00:14<00:27, 9.38MB/s]Downloading:  42%|████▏     | 184M/440M [00:14<00:26, 9.84MB/s]Downloading:  42%|████▏     | 186M/440M [00:15<00:21, 11.8MB/s]Downloading:  43%|████▎     | 188M/440M [00:15<00:18, 13.3MB/s]Downloading:  43%|████▎     | 189M/440M [00:15<00:20, 12.3MB/s]Downloading:  44%|████▎     | 192M/440M [00:15<00:17, 14.1MB/s]Downloading:  44%|████▍     | 194M/440M [00:15<00:16, 15.2MB/s]Downloading:  44%|████▍     | 196M/440M [00:15<00:14, 16.8MB/s]Downloading:  45%|████▍     | 198M/440M [00:15<00:14, 17.2MB/s]Downloading:  45%|████▌     | 200M/440M [00:15<00:19, 12.4MB/s]Downloading:  46%|████▌     | 201M/440M [00:16<00:25, 9.29MB/s]Downloading:  46%|████▋     | 204M/440M [00:16<00:20, 11.6MB/s]Downloading:  47%|████▋     | 206M/440M [00:16<00:18, 12.8MB/s]Downloading:  47%|████▋     | 207M/440M [00:16<00:17, 13.0MB/s]Downloading:  47%|████▋     | 209M/440M [00:16<00:16, 13.7MB/s]Downloading:  48%|████▊     | 211M/440M [00:16<00:16, 13.9MB/s]Downloading:  48%|████▊     | 213M/440M [00:16<00:15, 15.0MB/s]Downloading:  49%|████▊     | 214M/440M [00:16<00:15, 15.0MB/s]Downloading:  49%|████▉     | 216M/440M [00:17<00:14, 15.1MB/s]Downloading:  49%|████▉     | 217M/440M [00:17<00:14, 15.0MB/s]Downloading:  50%|████▉     | 219M/440M [00:17<00:15, 14.7MB/s]Downloading:  50%|█████     | 221M/440M [00:17<00:14, 15.2MB/s]Downloading:  50%|█████     | 222M/440M [00:17<00:14, 15.2MB/s]Downloading:  51%|█████     | 224M/440M [00:17<00:13, 15.9MB/s]Downloading:  51%|█████     | 225M/440M [00:17<00:13, 15.9MB/s]Downloading:  52%|█████▏    | 227M/440M [00:17<00:13, 15.6MB/s]Downloading:  52%|█████▏    | 229M/440M [00:17<00:14, 14.4MB/s]Downloading:  52%|█████▏    | 230M/440M [00:18<00:15, 14.0MB/s]Downloading:  53%|█████▎    | 232M/440M [00:18<00:14, 14.0MB/s]Downloading:  53%|█████▎    | 234M/440M [00:18<00:13, 15.0MB/s]Downloading:  53%|█████▎    | 235M/440M [00:18<00:18, 11.0MB/s]Downloading:  54%|█████▍    | 238M/440M [00:18<00:14, 13.7MB/s]Downloading:  55%|█████▍    | 240M/440M [00:18<00:14, 13.6MB/s]Downloading:  55%|█████▍    | 242M/440M [00:18<00:14, 14.0MB/s]Downloading:  55%|█████▌    | 244M/440M [00:18<00:13, 14.8MB/s]Downloading:  56%|█████▌    | 245M/440M [00:19<00:12, 15.7MB/s]Downloading:  56%|█████▌    | 247M/440M [00:19<00:13, 14.0MB/s]Downloading:  56%|█████▋    | 249M/440M [00:19<00:15, 12.4MB/s]Downloading:  57%|█████▋    | 250M/440M [00:19<00:21, 8.72MB/s]Downloading:  57%|█████▋    | 251M/440M [00:19<00:23, 8.17MB/s]Downloading:  57%|█████▋    | 253M/440M [00:19<00:19, 9.69MB/s]Downloading:  58%|█████▊    | 254M/440M [00:20<00:21, 8.62MB/s]Downloading:  58%|█████▊    | 255M/440M [00:20<00:21, 8.78MB/s]Downloading:  58%|█████▊    | 257M/440M [00:20<00:17, 10.2MB/s]Downloading:  59%|█████▊    | 258M/440M [00:20<00:17, 10.2MB/s]Downloading:  59%|█████▉    | 259M/440M [00:20<00:18, 10.0MB/s]Downloading:  59%|█████▉    | 261M/440M [00:20<00:17, 10.5MB/s]Downloading:  60%|█████▉    | 263M/440M [00:20<00:14, 12.6MB/s]Downloading:  60%|██████    | 264M/440M [00:20<00:13, 12.9MB/s]Downloading:  60%|██████    | 266M/440M [00:20<00:12, 14.0MB/s]Downloading:  61%|██████    | 268M/440M [00:21<00:12, 14.1MB/s]Downloading:  61%|██████    | 269M/440M [00:21<00:15, 11.2MB/s]Downloading:  62%|██████▏   | 272M/440M [00:21<00:12, 13.9MB/s]Downloading:  62%|██████▏   | 274M/440M [00:21<00:10, 15.3MB/s]Downloading:  63%|██████▎   | 277M/440M [00:21<00:09, 16.9MB/s]Downloading:  63%|██████▎   | 279M/440M [00:21<00:08, 18.1MB/s]Downloading:  64%|██████▍   | 281M/440M [00:21<00:08, 18.5MB/s]Downloading:  64%|██████▍   | 283M/440M [00:21<00:07, 19.7MB/s]Downloading:  65%|██████▍   | 285M/440M [00:21<00:07, 19.5MB/s]Downloading:  65%|██████▌   | 287M/440M [00:22<00:08, 18.4MB/s]Downloading:  66%|██████▌   | 289M/440M [00:22<00:08, 18.2MB/s]Downloading:  66%|██████▌   | 291M/440M [00:22<00:08, 17.0MB/s]Downloading:  67%|██████▋   | 293M/440M [00:22<00:09, 16.2MB/s]Downloading:  67%|██████▋   | 295M/440M [00:22<00:09, 15.8MB/s]Downloading:  67%|██████▋   | 296M/440M [00:22<00:09, 14.7MB/s]Downloading:  68%|██████▊   | 299M/440M [00:22<00:08, 16.4MB/s]Downloading:  68%|██████▊   | 300M/440M [00:22<00:08, 16.2MB/s]Downloading:  69%|██████▊   | 302M/440M [00:23<00:20, 6.71MB/s]Downloading:  69%|██████▉   | 306M/440M [00:23<00:15, 8.89MB/s]Downloading:  70%|██████▉   | 308M/440M [00:23<00:12, 10.9MB/s]Downloading:  71%|███████   | 311M/440M [00:23<00:09, 13.5MB/s]Downloading:  71%|███████▏  | 314M/440M [00:23<00:07, 16.3MB/s]Downloading:  72%|███████▏  | 317M/440M [00:24<00:06, 19.0MB/s]Downloading:  73%|███████▎  | 320M/440M [00:24<00:05, 21.5MB/s]Downloading:  73%|███████▎  | 323M/440M [00:24<00:05, 22.7MB/s]Downloading:  74%|███████▍  | 326M/440M [00:24<00:05, 22.4MB/s]Downloading:  75%|███████▍  | 329M/440M [00:24<00:05, 21.8MB/s]Downloading:  75%|███████▌  | 331M/440M [00:24<00:05, 20.7MB/s]Downloading:  76%|███████▌  | 333M/440M [00:24<00:05, 20.6MB/s]Downloading:  76%|███████▌  | 336M/440M [00:24<00:07, 14.4MB/s]Downloading:  77%|███████▋  | 339M/440M [00:25<00:05, 17.7MB/s]Downloading:  78%|███████▊  | 342M/440M [00:25<00:05, 19.4MB/s]Downloading:  78%|███████▊  | 344M/440M [00:25<00:05, 18.2MB/s]Downloading:  79%|███████▊  | 346M/440M [00:25<00:04, 19.0MB/s]Downloading:  79%|███████▉  | 349M/440M [00:25<00:04, 19.9MB/s]Downloading:  80%|███████▉  | 351M/440M [00:25<00:04, 21.2MB/s]Downloading:  80%|████████  | 353M/440M [00:25<00:04, 20.5MB/s]Downloading:  81%|████████  | 356M/440M [00:25<00:03, 21.3MB/s]Downloading:  81%|████████▏ | 358M/440M [00:25<00:03, 21.4MB/s]Downloading:  82%|████████▏ | 360M/440M [00:26<00:04, 20.0MB/s]Downloading:  82%|████████▏ | 362M/440M [00:26<00:04, 19.1MB/s]Downloading:  83%|████████▎ | 364M/440M [00:26<00:03, 19.5MB/s]Downloading:  83%|████████▎ | 366M/440M [00:26<00:03, 19.9MB/s]Downloading:  84%|████████▎ | 368M/440M [00:26<00:05, 14.1MB/s]Downloading:  84%|████████▍ | 372M/440M [00:26<00:03, 17.3MB/s]Downloading:  85%|████████▌ | 375M/440M [00:26<00:03, 19.1MB/s]Downloading:  86%|████████▌ | 377M/440M [00:27<00:03, 16.3MB/s]Downloading:  86%|████████▌ | 379M/440M [00:27<00:04, 13.7MB/s]Downloading:  87%|████████▋ | 381M/440M [00:27<00:03, 15.3MB/s]Downloading:  87%|████████▋ | 383M/440M [00:27<00:04, 14.3MB/s]Downloading:  87%|████████▋ | 385M/440M [00:27<00:03, 15.6MB/s]Downloading:  88%|████████▊ | 387M/440M [00:27<00:03, 14.6MB/s]Downloading:  88%|████████▊ | 388M/440M [00:27<00:03, 13.1MB/s]Downloading:  88%|████████▊ | 390M/440M [00:27<00:03, 13.0MB/s]Downloading:  89%|████████▉ | 391M/440M [00:28<00:03, 13.0MB/s]Downloading:  89%|████████▉ | 393M/440M [00:28<00:04, 11.8MB/s]Downloading:  89%|████████▉ | 394M/440M [00:28<00:04, 11.5MB/s]Downloading:  90%|████████▉ | 396M/440M [00:28<00:03, 12.2MB/s]Downloading:  90%|█████████ | 397M/440M [00:28<00:03, 11.0MB/s]Downloading:  90%|█████████ | 398M/440M [00:28<00:03, 11.9MB/s]Downloading:  91%|█████████ | 400M/440M [00:28<00:03, 12.4MB/s]Downloading:  91%|█████████ | 402M/440M [00:29<00:03, 10.9MB/s]Downloading:  92%|█████████▏| 404M/440M [00:29<00:02, 13.3MB/s]Downloading:  92%|█████████▏| 407M/440M [00:29<00:02, 14.8MB/s]Downloading:  93%|█████████▎| 408M/440M [00:29<00:02, 14.9MB/s]Downloading:  93%|█████████▎| 410M/440M [00:29<00:02, 13.0MB/s]Downloading:  94%|█████████▎| 412M/440M [00:29<00:01, 14.9MB/s]Downloading:  94%|█████████▍| 414M/440M [00:29<00:02, 13.1MB/s]Downloading:  94%|█████████▍| 416M/440M [00:29<00:01, 14.1MB/s]Downloading:  95%|█████████▍| 417M/440M [00:30<00:01, 15.0MB/s]Downloading:  95%|█████████▌| 419M/440M [00:30<00:01, 13.6MB/s]Downloading:  96%|█████████▌| 421M/440M [00:30<00:01, 14.3MB/s]Downloading:  96%|█████████▌| 423M/440M [00:30<00:01, 15.9MB/s]Downloading:  97%|█████████▋| 425M/440M [00:30<00:00, 17.3MB/s]Downloading:  97%|█████████▋| 427M/440M [00:30<00:00, 18.5MB/s]Downloading:  97%|█████████▋| 429M/440M [00:30<00:00, 18.3MB/s]Downloading:  98%|█████████▊| 432M/440M [00:30<00:00, 19.4MB/s]Downloading:  99%|█████████▊| 434M/440M [00:30<00:00, 20.4MB/s]Downloading:  99%|█████████▉| 436M/440M [00:30<00:00, 19.1MB/s]Downloading:  99%|█████████▉| 438M/440M [00:31<00:00, 19.7MB/s]Downloading: 100%|█████████▉| 440M/440M [00:31<00:00, 18.7MB/s]Downloading: 100%|██████████| 440M/440M [00:31<00:00, 14.1MB/s]
I0511 03:10:49.252154 139841247500032 file_utils.py:448] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 03:10:49.252990 139841247500032 file_utils.py:451] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
I0511 03:10:49.253664 139841247500032 filelock.py:318] Lock 139839896989536 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
I0511 03:10:49.253945 139841247500032 modeling_utils.py:617] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
05/11/2020 03:10:56 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=3, bias=True)
  )
)

05/11/2020 03:10:56 Total number of params: 109484547
05/11/2020 03:10:56 At epoch 0
05/11/2020 03:10:57 Task [ 0] updates[     1] train loss[1.11769] remaining[0:18:39]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
05/11/2020 03:12:42 Task [ 0] updates[   500] train loss[1.01764] remaining[0:03:04]
05/11/2020 03:14:30 Task [ 0] updates[  1000] train loss[0.92640] remaining[0:01:19]
predicting 0
predicting 100
predicting 200
05/11/2020 03:15:54 [new test scores saved.]
I0511 03:15:56.118234 139841247500032 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/tweetsent/bert_base/seed/2018/model_0.pt
05/11/2020 03:15:56 At epoch 1
05/11/2020 03:16:22 Task [ 0] updates[  1500] train loss[0.87816] remaining[0:04:23]
05/11/2020 03:18:09 Task [ 0] updates[  2000] train loss[0.84287] remaining[0:02:38]
05/11/2020 03:19:54 Task [ 0] updates[  2500] train loss[0.80227] remaining[0:00:52]
predicting 0
predicting 100
predicting 200
05/11/2020 03:20:51 [new test scores saved.]
I0511 03:20:52.390866 139841247500032 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/tweetsent/bert_base/seed/2018/model_1.pt
05/11/2020 03:20:52 At epoch 2
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
05/11/2020 03:21:47 Task [ 0] updates[  3000] train loss[0.77195] remaining[0:04:03]
05/11/2020 03:23:32 Task [ 0] updates[  3500] train loss[0.74126] remaining[0:02:11]
05/11/2020 03:25:18 Task [ 0] updates[  4000] train loss[0.71120] remaining[0:00:25]
predicting 0
predicting 100
predicting 200
05/11/2020 03:25:48 [new test scores saved.]
I0511 03:25:50.078529 139841247500032 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/tweetsent/bert_base/seed/2018/model_2.pt
05/11/2020 03:25:50 At epoch 3
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
05/11/2020 03:27:11 Task [ 0] updates[  4500] train loss[0.68623] remaining[0:03:31]
05/11/2020 03:28:54 Task [ 0] updates[  5000] train loss[0.65821] remaining[0:01:42]
predicting 0
predicting 100
predicting 200
05/11/2020 03:30:29 [new test scores saved.]
I0511 03:30:30.927282 139841247500032 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/tweetsent/bert_base/seed/2018/model_3.pt
05/11/2020 03:30:30 At epoch 4
05/11/2020 03:30:32 Task [ 0] updates[  5500] train loss[0.63420] remaining[0:04:13]
05/11/2020 03:32:09 Task [ 0] updates[  6000] train loss[0.61447] remaining[0:02:48]
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
05/11/2020 03:33:57 Task [ 0] updates[  6500] train loss[0.59368] remaining[0:01:14]
predicting 0
predicting 100
predicting 200
05/11/2020 03:35:20 [new test scores saved.]
I0511 03:35:21.753345 139841247500032 model.py:278] model saved to /container/mt-dnn_port/output/st-dnn/tweetsent/bert_base/seed/2018/model_4.pt

real	86m25.363s
user	76m22.665s
sys	9m24.979s
