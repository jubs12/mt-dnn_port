{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assin_tsv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qTMo3bsyQJea"
      },
      "source": [
        "Pegar dataset e requisitos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "colab_type": "code",
        "id": "ZOa1JXb1PAFO",
        "outputId": "f8206d5f-61aa-49a0-9712-1eedb721dfff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/I.C. Nádia/Datasets/Train/assin\n",
            "Collecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q9j1a83CuKzsHCGaNulSkNxBm7Dkn7Ln' -O assin2-train-only.xml\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1J3FpQaHxpM-FDfBUyooh-sZF-B-bM_lU' -O assin2-test.xml\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kb7xq6Mb3eaqe9cOAo70BaG9ypwkIqEU' -O assin2-dev.xml\n",
        "!wget http://nilc.icmc.usp.br/assin/assin.tar.gz\n",
        "!tar -xzf assin.tar.gz\n",
        "%mkdir input\n",
        "%mv *.xml input\n",
        "%rm -rf assin.tar.gz\n",
        "%cd input\n",
        "!pip install xmltodict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pGk_NBOcQPu8"
      },
      "source": [
        "Gerar dicionário para assin1 e assin2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "colab_type": "code",
        "id": "SisZe9vQQzC3",
        "outputId": "4ccace2f-2a96-4986-b0ce-e551c21c795e"
      },
      "outputs": [
        {
          "ename": "ExpatError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExpatError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-4f898a6ffe01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mxml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmltodict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(.*).xml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr'\\1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mxmls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xmltodict.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(xml_input, encoding, expat, process_namespaces, namespace_separator, disable_entities, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mExpatError\u001b[0m: no element found: line 1, column 0"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import xmltodict\n",
        "import os\n",
        "import re\n",
        "\n",
        "files_xml = [f for f in os.listdir(os.curdir) if 'xml' in f]\n",
        "names = list()\n",
        "xmls = list()\n",
        "\n",
        "for filename in files_xml:\n",
        "    with open(filename) as f:\n",
        "        xml = xmltodict.parse(f.read())\n",
        "        name = re.sub(r'(.*).xml', r'\\1', filename)\n",
        "        xmls.append(xml)\n",
        "        names.append(name)\n",
        "\n",
        "xml_names = dict(zip(names, xmls))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dnj_eNGDJHdP"
      },
      "source": [
        "Gerar formatação para o mt-dnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VduSnluFx4tG"
      },
      "outputs": [],
      "source": [
        "formatting = ['id', 'label','premise', 'hipothesis']\n",
        "output_names = []\n",
        "output_files = []\n",
        "for name in names:\n",
        "    place = xml_names[name]['entailment-corpus']['pair'] \n",
        "\n",
        "    if 'only' in name:\n",
        "        name = name.replace('-only', '')\n",
        "\n",
        "    output_names.append(re.sub(r'(.+)-(.+)', r'\\1-rte_\\2', name))\n",
        "    output_names.append(re.sub(r'(.+)-(.+)', r'\\1-sts_\\2', name))\n",
        "    rte = list()\n",
        "    sts = list()\n",
        "\n",
        "    for idx, item in enumerate(place):\n",
        "        rte.append((item['@id'],item['@entailment'],item['t'],item['h']))\n",
        "        sts.append((item['@id'],item['@similarity'],item['t'],item['h']))\n",
        "\n",
        "    rte_df = pd.DataFrame(rte, index = None, columns = formatting)\n",
        "    sts_df = pd.DataFrame(sts, index = None, columns = formatting)\n",
        "\n",
        "    output_files.append(rte_df)\n",
        "    output_files.append(sts_df)\n",
        "\n",
        "for idx, output in enumerate(output_files):\n",
        "    output_name = '{}.tmp'.format(output_names[idx])\n",
        "    output.to_csv(output_name, sep = '\\t', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RUCN3jYoJPPm"
      },
      "source": [
        "Importar avanço"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XRb-bSIhJSm0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "table_names = [f for f in os.listdir(os.curdir) if 'tmp' in f]\n",
        "tables = list()\n",
        "\n",
        "for name in table_names:\n",
        "    table = pd.read_csv(name, sep = '\\t', names = formatting)\n",
        "    tables.append(table)\n",
        "\n",
        "table_dict = dict(zip(table_names, tables))\n",
        "!rm -f *.xml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gFkdn1_bOu1h"
      },
      "source": [
        "Substituir texto original por traduzido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YMTo6gH8Obk_"
      },
      "outputs": [],
      "source": [
        "!cp ../../../Translation/assin-dic.json assin-dic.json\n",
        "import json\n",
        "\n",
        "with open('assin-dic.json') as json_file:\n",
        "    dic = json.load(json_file)\n",
        "\n",
        "for key in table_dict.keys():\n",
        "    for col in ['premise', 'hipothesis']:\n",
        "        table_dict[key][col] = table_dict[key][col].map(dic)\n",
        "\n",
        "    output_name = re.sub(r'(.*).tmp', r'\\1.tsv', key)\n",
        "    table_dict[key].to_csv(output_name, sep = '\\t', index = False, header = None)\n",
        "!rm -f *.tmp\n",
        "!rm -f *.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remover marcas html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [

        "!pip install ftfy\n",
        "!cp ../ftfy_assin.sh .\n",
        "!bash ftfy_assin.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFzmLjfQe1c",
        "colab_type": "text"
      },
      "source": [
        "Remover aspas que está em excesso na linha 1711 do *assin-ptbr-train*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHJVJPHQQeOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filepath in ['assin-ptbr-rte_train.tsv', 'assin-ptbr-sts_train.tsv']:\n",
        "    with open(filepath, 'r') as f:\n",
        "        corpus = f.read()\n",
        "        corpus = corpus.replace('\"As long as','As long as')\n",
        "\n",
        "    with open(filepath, 'w') as f:   \n",
        "        f.write(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
